<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="WeNet代码, 梦园">
    <meta name="description" content="1.  数据预处理

compute_cmvn_stats：生成global_cmvn

mean_stat：训练集上各维特征的总和
var_stat：训练集上各维特征的平方和
frame_num：训练集总帧数



2.  Iterabl">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>WeNet代码 | 梦园</title>
    <link rel="icon" type="image/png" href="/blog/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/blog/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/blog/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/blog/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/blog/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/blog/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/blog/css/matery.css">
<link rel="stylesheet" type="text/css" href="/blog/css/my.css">
<link rel="stylesheet" type="text/css" href="/blog/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/blog/css/post.css">



    



    <script src="/blog/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.1.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">梦园</div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/" class="waves-effect waves-light">
      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/blog/categories" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/blog/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>我</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/words">
          
          <i class="fas fa-pen" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>文字</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/books">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>书</span>
        </a>
      </li>
      
      <li>
        <a href="/blog/videos">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>视频</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/blog/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">梦园</div>
        <div class="logo-desc">
            
            愿，彼此相伴，走过流年
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/blog/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-list"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/blog/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			关于
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/blog/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>我</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/words " style="margin-left:75px">
				  
				   <i class="fa fas fa-pen" style="position: absolute;left:50px" ></i>
			      
		          <span>文字</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/books " style="margin-left:75px">
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>书</span>
                  </a>
                </li>
              
                <li>

                  <a href="/blog/videos " style="margin-left:75px">
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>视频</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    
<script src="/blog/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/blog/';
            }
        }
    })();
</script>


<div class="bg-cover pd-header about-cover">
    <div class="container">
    <div class="row">
    <div class="col s10 offset-s1 m8 offset-m2 l8 offset-l1">
        <div class="brand">
            <div class="title">
                
                愿，彼此相伴，走过流年
                
            </div>
        </div>
    </div>
</div>

    </div>
</div>



<main class="container content">

    
    <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="pd-header post-cover">
                <div class="container" style="right: 0px;left: 0px;">
                    <div class="row">
                        <div class="col s12 m12 l12">
                            <div class="brand">
                                <h1 class="description center-align post-title">WeNet代码</h1>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="tag-cate">
                <div class="right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/" class="post-category">
                                语音
                            </a>
                        
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/%E5%B7%A5%E5%85%B7%E5%8C%85/" class="post-category">
                                工具包
                            </a>
                        
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/%E5%B7%A5%E5%85%B7%E5%8C%85/WeNet/" class="post-category">
                                WeNet
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-08-22
                </div>
                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/blog/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="数据预处理">1.  数据预处理</h1>
<ul>
<li>compute_cmvn_stats：生成global_cmvn
<ul>
<li>mean_stat：训练集上各维特征的总和</li>
<li>var_stat：训练集上各维特征的平方和</li>
<li>frame_num：训练集总帧数</li>
</ul>
</li>
</ul>
<h1 id="iterabledataset-processor">2.  IterableDataset - Processor</h1>
<ul>
<li>根据输入list生成IterableDataset
<ul>
<li>数据类型
<ul>
<li>raw：json列表，存储key、wav(、start、end)、txt</li>
<li>shard：压缩包列表。随机：压缩包间随机，压缩包内部顺序遍历</li>
</ul>
</li>
<li>训练集按rank、worker_id划分数据；CV集各卡均用全集</li>
</ul>
</li>
<li>tokenize
<ul>
<li><font color="green">non_lang_syms：用[……]、&lt;……&gt;、{……}分割文本（如’12[12]34&lt;34&gt;56{56}{78}'分割为[‘12’, ‘[12]’, ‘34’, ‘&lt;34&gt;’, ‘56’, ‘{56}’, ‘’, ‘{78}’, ‘’]），转大写。若token在non_lang_syms中，直接使用；否则BPE/按空格分割/按字符分割</font></li>
<li>BPE<pre class="line-numbers language-none"><code class="language-none">import sentencepiece
sp &#x3D; sentencepiece.SentencePieceProcessor()
sp.load(bpe_model)
sp.encode_as_pieces(word)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li>symbol_table：token转id</li>
</ul>
</li>
<li>filter：过滤过短或过长的样本
<ul>
<li>max_length(10240)、min_length(10)：帧数，帧移10ms</li>
<li>token_max_length(200)、token_min_length(1)</li>
<li>min_output_input_ratio(0.0005)、max_output_input_ratio(1)</li>
</ul>
</li>
<li>resample</li>
<li>speed_perturb：各样本随机变速[0.9, 1.0, 1.1]。默认不使用</li>
<li><font color="green">提取fbank/mfcc</font>：<font color="red">音频要求16bit</font><pre class="line-numbers language-none"><code class="language-none">torchaudio.compliance.kaldi as kaldi
feat &#x3D; kaldi.fbank(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, energy_floor&#x3D;0.0, sample_frequency&#x3D;sample_rate)
feat &#x3D; kaldi.mfcc(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, num_ceps&#x3D;num_ceps, high_freq&#x3D;high_freq, low_freq&#x3D;low_freq, sample_frequency&#x3D;sample_rate)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li>spec_aug：频谱mask，num_t_mask、num_f_mask、max_t、max_f，max_w未使用。默认使用</li>
<li>spec_sub：频谱替换，num_t_sub、max_t，随机替换为该时刻前的特征。默认不使用</li>
<li>shuffle：<font color="red">局部shuffle，shuffle_size默认10000</font>。默认使用</li>
<li>sort：局部按帧数排序，便于帧数相近的样本放到同一batch，sort_size=500。默认使用</li>
<li>batch
<ul>
<li>静态batch：除最后一个batch，batch size固定</li>
<li>动态batch：按max_frames_in_batch（含padding）拼batch</li>
</ul>
</li>
<li>padding：batch内样本按帧数降序排列，特征补0，label补-1</li>
<li>torchaudio<pre class="line-numbers language-none"><code class="language-none">torchaudio.backend.sox_io_backend.info(wav_file).sample_rate
torchaudio.load(wav_file)
torchaudio.backend.sox_io_backend.load(filepath&#x3D;wav_file, num_frames&#x3D;end_frame - start_frame, frame_offset&#x3D;start_frame)
torchaudio.transforms.Resample(orig_freq&#x3D;sample_rate, new_freq&#x3D;resample_rate)(waveform)
wav, _ &#x3D; torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, [[&#39;speed&#39;, str(speed)], [&#39;rate&#39;, str(sample_rate)]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<h1 id="模型结构">3.  模型结构</h1>
<h2 id="encoder">3.1.  encoder</h2>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;
    classDef font_green color:green;

    输入("输入：x: [B, T, D], <br> mask_pad: [B, 1, T]，有效帧对应1，拼batch时补的帧对应0")
    输入 --> CMVN
    CMVN --> 下采样

    subgraph 下采样
        不下采样
        4倍下采样
        6倍下采样
        8倍下采样

        subgraph 不下采样
            direction TB
            Linear1("Linear(input_size, output_size)")
            Linear1 --> LayerNorm1("LayerNorm") --> Dropout1("Dropout")
        end

        subgraph 4倍下采样
            direction TB
            Conv2d1("Conv2d(1, output_size, 3, 2)")
            Conv2d2("Conv2d(output_size, output_size, 3, 2)")
            Linear2("Linear(output_size * (((input_size - 1) // 2 - 1) // 2), output_size)")
            mask_pad1("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2]")

            Conv2d1 --> ReLU1(ReLU) --> Conv2d2 --> ReLU2(ReLU) --> Linear2
        end

        subgraph 6倍下采样
            direction TB
            Conv2d3("Conv2d(1, output_size, 3, 2)")
            Conv2d4("Conv2d(output_size, output_size, 5, 3)")
            Linear3("Linear( , output_size)")
            mask_pad2("mask_pad = mask_pad[:, :, :-2:2][:, :, :-4:3]")

            Conv2d3 --> ReLU3(ReLU) --> Conv2d4 --> ReLU4(ReLU) --> Linear3
        end

        subgraph 8倍下采样
            direction TB
            Conv2d5("Conv2d(1, output_size, 3, 2)")
            Conv2d6("Conv2d(output_size, output_size, 3, 2)")
            Conv2d7("Conv2d(output_size, output_size, 3, 2)")
            Linear4("Linear( , output_size)")
            mask_pad3("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2][:, :, :-2:2]")

            Conv2d5 --> ReLU5(ReLU) --> Conv2d6 --> ReLU6(ReLU) --> Conv2d7 --> ReLU7(ReLU) --> Linear4
        end
    end

    下采样 --> 位置embedding
    subgraph 位置embedding
        pos_emd("pos_emb(pos, 2i) = sin(pos/(10000^(2i/dim))) <br> pos_emb(pos, 2i+1) = cos(pos/(10000^(2i/dim))) <br> 拼batch计算时，offset小于0的位置采用pos_emb[0]"):::font_green
        位置embedding1("x = x * math.sqrt(dim) + pos_emb"):::font_green
        相对位置embedding("x *= math.sqrt(dim)"):::font_green
        无位置embedding("x不变，pos_emb全0")
        Dropout2("dropout(x), dropout(pos_emb)")

        tmp1(" ") --> pos_emd
        pos_emd -- 位置embedding --> 位置embedding1 --> Dropout2
        pos_emd -- 相对位置embedding --> 相对位置embedding --> Dropout2
        tmp1 -- 无位置embedding --> 无位置embedding --> Dropout2
    end

    位置embedding --> 计算chunk_mask
    计算chunk_mask --> transformer*num_blocks -->tmp2
    计算chunk_mask --> conformer*num_blocks -->tmp2
    tmp2(" ") -- normalize_before -->LayerNorm2("LayerNorm")

    subgraph 计算chunk_mask
        use_dynamic_chunk{"use_dynamic_chunk"} -- Yes --> decoding_chunk_size{"decoding_chunk_size"}
        decoding_chunk_size -- 小于0 --> tmp18("不分chunk，历史、未来全部可见")
        decoding_chunk_size -- 大于0 --> 采用固定的chunk_size --> num_decoding_left_chunks{"num_decoding_left_chunks"}
        decoding_chunk_size -- 等于0 --> tmp17("随机动态chunk_size，帧数取值范围为[1, 25]或全部可见（概率50%），支持use_dynamic_left_chunk") --> num_decoding_left_chunks

        use_dynamic_chunk -- No --> static_chunk_size{"static_chunk_size"}
        static_chunk_size -- 小于等于0 --> tmp18
        static_chunk_size -- 大于0 --> 采用固定的chunk_size

        num_decoding_left_chunks -- 小于0 --> tmp19("历史全部可见，当前chunk可见，未来chunk不可见")
        num_decoding_left_chunks -- 大于等于0 --> 仅可见指定的历史chunk数
    end

    subgraph transformer*num_blocks
        subgraph transformer_attention
            tmp3(" ") --> normalize_before1{"normalize_before"}
            normalize_before1 -- Yes --> LayerNorm3("LayerNorm") --> MultiHeadedAttention1("MultiHeadedAttention"):::font_red
            normalize_before1 -- No --> MultiHeadedAttention1

            MultiHeadedAttention1 --> concat_after1{"concat_after"}
            concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp4(" ")
            concat_after1 -- No --> Dropout3("Dropout") --> tmp4

            tmp3 -- + --> tmp4

            tmp4 --> normalize_before2{"normalize_before"}
            normalize_before2 -- Yes --> tmp5(" ")
            normalize_before2 -- No --> LayerNorm4("LayerNorm") --> tmp5
        end

        subgraph transformer_FeedForward
            tmp5 --> normalize_before3{"normalize_before"}
            normalize_before3 -- Yes --> LayerNorm5("LayerNorm") --> PositionwiseFeedForward1("PositionwiseFeedForward: <br> w2(dropout(activation(w1*x+b1)))+b2 <br> w1:[input, hidden] w2:[hidden, input]"):::font_red
            normalize_before3 -- No --> PositionwiseFeedForward1
            PositionwiseFeedForward1 --> Dropout4("Dropout")

            tmp5 -- + --> Dropout4

            Dropout4 --> normalize_before4{"normalize_before"}
            normalize_before4 -- Yes --> tmp6(" ")
            normalize_before4 -- No --> LayerNorm6("LayerNorm") --> tmp6
        end
    end

    subgraph conformer*num_blocks
        subgraph conformer_macaron
            tmp7(" ") --> macaron{"macaron"}
            macaron -- Yes --> normalize_before5{"normalize_before"}
            macaron -- No --> tmp8(" ")

            normalize_before5 -- Yes --> LayerNorm7("LayerNorm") --> PositionwiseFeedForward2("PositionwiseFeedForward"):::font_red
            normalize_before5 -- No --> PositionwiseFeedForward2
            PositionwiseFeedForward2 --> Dropout5("Dropout") --> scale("*0.5")

            tmp7 -- + --> scale

            scale --> normalize_before6{"normalize_before"}
            normalize_before6 -- Yes --> tmp8
            normalize_before6 -- No --> LayerNorm8("LayerNorm") --> tmp8
        end

        subgraph conformer_attention
            tmp8 --> normalize_before7{"normalize_before"}
            normalize_before7 -- Yes --> LayerNorm9("LayerNorm") --> tmp9(" ")
            normalize_before7 -- No --> tmp9

            subgraph Attention
                direction TB

                计算qkv("q、k、v均采用Linear计算 <br> 更新att_cache: 历史[k, v]，(1, head, chunk_size * num_decoding_left_chunks + T, d_k * 2)"):::font_red
                计算attention系数("计算attention系数：采用了chunk_mask <br> Dropout"):::font_red
                attention("scaled dot product self-attention")
                Linear5("Linear(output_size, output_size)")

                Rel("pos_bias_u、pos_bias_v: [head, d_k]，xavier_uniform_初始化，可训练 <br> softmax{[(q + pos_bias_u)k + (q + pos_bias_v)(W*pos_emb)]/math.sqrt(d_k)}v <br> rel_shift: 未采用"):::font_red

                tmp9 --> 计算qkv -- MultiHeadedAttention --> 计算attention系数 --> attention --> Linear5

                计算qkv -- RelPositionMultiHeadedAttention --> Rel --> Linear5
            end

            Linear5 --> concat_after3{"concat_after"}
            concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp10(" ")
            concat_after3 -- No --> Dropout6("Dropout") --> tmp10

            tmp8 -- + --> tmp10

            tmp10 --> normalize_before8{"normalize_before"}
            normalize_before8 -- Yes --> tmp11(" ")
            normalize_before8 -- No --> LayerNorm10("LayerNorm") --> tmp11
        end

        subgraph conformer_conv
            tmp11 --> conv{"conv"}
            conv -- Yes --> normalize_before9{"normalize_before"}

            normalize_before9 -- Yes --> LayerNorm11("LayerNorm") --> tmp12(" ")
            normalize_before9 -- No --> tmp12

            tmp12 --> mask_pad4("mask_pad的帧均置0"):::font_red --> causal{"causal"}
            causal -- Yes -->
            padding("第一次计算时左侧填充(kernel_size-1)帧0，<br> 后续从cache中取缓存的历史有效输入"):::font_red --> pointwise_conv1
            causal -- No --> pointwise_conv1

            pointwise_conv1("pointwise_conv: 1D卷积，channel扩充为2倍"):::font_red --> GLU("GLU，channel减半"):::font_red --> depthwise_conv
            click GLU "https://pytorch.org/docs/stable/generated/torch.nn.functional.glu.html"

            depthwise_conv("nn.Conv1d(channels, channels, kernel_size, stride=1, padding=padding, groups=channels, bias=bias) <br> 若非causal，左右填充(kernel_size - 1) // 2帧0"):::font_red
            depthwise_conv --> BatchNorm1d/LayerNorm --> activation("activation，如SiLU/swish")
            click activation "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"

            activation --> pointwise_conv2("pointwise_conv: 1D卷积"):::font_red --> mask_pad5("mask_pad的帧均置0"):::font_red -->Dropout7("Dropout")

            tmp11 -- + --> Dropout7

            Dropout7 --> normalize_before10{"normalize_before"}
            normalize_before10 -- Yes --> tmp13(" ")
            normalize_before10 -- No --> LayerNorm12("LayerNorm") --> tmp13
            conv -- No --> tmp13
        end

        subgraph conformer_FeedForward
            tmp13 --> normalize_before11{"normalize_before"}
            normalize_before11 -- Yes --> LayerNorm13("LayerNorm") --> PositionwiseFeedForward3("PositionwiseFeedForward"):::font_red
            normalize_before11 -- No --> PositionwiseFeedForward3

            PositionwiseFeedForward3 --> Dropout8("Dropout") --> macaron2{"macaron"}
            macaron2 -- Yes --> scale2(*0.5) --> tmp14(" ")
            macaron2 -- No --> tmp14(" ")

            tmp13 -- + --> tmp14

            tmp14 --> normalize_before12{"normalize_before"}
            normalize_before12 -- Yes --> tmp15(" ")
            normalize_before12 -- No --> LayerNorm14("LayerNorm") --> tmp15

            tmp15--> conv2{"conv"}
            conv2 -- Yes --> LayerNorm15("LayerNorm") --> tmp16(" ")
            conv2 -- No --> tmp16(" ")
        end
    end
</div>

<ul>
<li>load_cmvn：计算均值、方差倒数。支持json格式、kaldi文本格式输入</li>
<li>下采样：2D卷积下采样至1/4或1/6或1/8，感受野分别为7、11、15帧（延迟分别为6、10、14) <br> <font color="green">存在的问题：x_mask。如下采样至1/4时，取<code>x_mask[:, :, :-2:2][:, :, :-2:2]</code>，第1个点中心时刻为第3帧，感受野为0-6帧，第一个点的mask应取第6帧</font></li>
<li>forward_chunk：输入(chunk_size - 1) * subsample_rate + subsample.right_context + 1帧有效数据，输出chunk_size帧，并更新att_cache、cnn_cache</li>
<li>forward_chunk_by_chunk：输入有效帧，模拟逐chunk流式计算</li>
</ul>
<h2 id="decoder">3.2.  decoder</h2>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;
    classDef font_green color:green;

    subgraph TransformerDecoder
        tmp(" ") -- train --> 输入1("输入label、label长度") --> tgt_mask("mask padding、未来输出") --> embedding --> +位置embedding --> transformer*num_blocks
        tmp -- decode --> 输入2("自回归：输入上一时刻解码结果") --> embedding

        transformer*num_blocks --> normalize_before1{"normalize_before"}
        normalize_before1 -- Yes --> LayerNorm1("LayerNorm") --> tmp1(" ")
        normalize_before1 -- No --> tmp1

        tmp1 --> output{"use_output_layer"}
        output -- Yes --> Linear1("Linear(attention_dim, vocab_size)") --> train_decode{"train/decode"}
        train_decode -- train --> tmp2(" ")
        train_decode -- decode --> log_softmax --> tmp2
        output -- No --> tmp2

        subgraph transformer*num_blocks
            direction TB

            subgraph self_attention
                direction TB
                tmp4(" ") --> normalize_before2{"normalize_before"}
                normalize_before2 -- Yes --> LayerNorm2("LayerNorm") --> tmp5(" ")
                normalize_before2 -- No --> tmp5

                tmp5 --> MultiHeadedAttention1("MultiHeadedAttention: q: 上一时刻输出，k/v: 历史输出，mask未来输出"):::font_red
                MultiHeadedAttention1 --> concat_after1{"concat_after"}
                concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp6(" ")
                concat_after1 -- No --> Dropout3("Dropout") --> tmp6

                tmp4 -- + --> tmp6

                tmp6 --> normalize_before3{"normalize_before"}
                normalize_before3 -- Yes --> tmp7(" ")
                normalize_before3 -- No --> LayerNorm3("LayerNorm") --> tmp7
            end

            subgraph inter_attention
                direction TB
                tmp7 --> normalize_before4{"normalize_before"}
                normalize_before4 -- Yes --> LayerNorm4("LayerNorm") --> tmp8(" ")
                normalize_before4 -- No --> tmp8

                tmp8 --> MultiHeadedAttention2("MultiHeadedAttention: q: decoder隐层表示，k/v: encoder输出，mask未来输出"):::font_red
                MultiHeadedAttention2 --> concat_after3{"concat_after"}
                concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp9(" ")
                concat_after3 -- No --> Dropout4("Dropout") --> tmp9

                tmp7 -- + --> tmp9

                tmp9 --> normalize_before5{"normalize_before"}
                normalize_before5 -- Yes --> tmp10(" ")
                normalize_before5 -- No --> LayerNorm5("LayerNorm") --> tmp10

            end

            subgraph FeedForward
                tmp10 --> normalize_before6{"normalize_before"}
                normalize_before6 -- Yes --> LayerNorm6("LayerNorm") --> PositionwiseFeedForward:::font_red
                normalize_before6 -- No --> PositionwiseFeedForward
                PositionwiseFeedForward --> Dropout5("Dropout")

                tmp10 -- + --> Dropout5

                Dropout5 --> normalize_before7{"normalize_before"}
                normalize_before7 -- Yes --> tmp11(" ")
                normalize_before7 -- No --> LayerNorm7("LayerNorm") --> tmp11
            end

            decode("decode：缓存每一时刻decoder各block的输出；输入tgt递增存储每一时刻的输出")
        end
    end

    subgraph BiTransformerDecoder
        tmp3(" ") -- train/rescore --> 输出left_decoder与right_decoder结果:::font_red
        tmp3 -- decode --> 采用left_decoder:::font_red
    end
</div>

<h2 id="transducer">3.3.  Transducer</h2>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;
    classDef font_green color:green;

    input("输入上一时刻预测值") --> Embedding1("Embedding(vocab_size, embed_size)") --> Dropout1("Dropout")
    Dropout1 -- RNNPredictor --> RNNPredictor
    Dropout1 -- EmbeddingPredictor --> EmbeddingPredictor
    Dropout1 -- ConvPredictor --> ConvPredictor

    subgraph RNNPredictor
        RNN("RNN/LSTM/GRU，记录历史h、c，初始为全0"):::font_red --> linear1("Linear(hidden_size, output_size)") --> tmp1
    end

    subgraph EmbeddingPredictor
        direction TB

        input1("拼接定长的历史输入 <br> input(B,L,1,C,E)，B: batch size, L: seq_len, C: context_size, E: embed_size"):::font_red --> attention:::font_red

        subgraph attention
            input1 --> dot
            W("W(H,C,E)，H: num_heads") --> dot
            dot("broadcast dot product, (B,L,H,C,E)"):::font_red --> sum1("sum, (B,L,H,C)") --> unsqueeze("unsqueeze, (B,L,H,1,C)") --> matmul
            input1 --> matmul("matmul，(B,L,H,1,E)") --> sum2("squeeze, sum，(B,L,E)") --> scale("scale，/(H*C)")

            click dot "https://arxiv.org/pdf/2109.07513.pdf"
        end

        scale --> proj("Linear(embed_size, embed_size)") --> LayerNorm --> swish --> tmp1
    end

    subgraph ConvPredictor
        input2("拼接定长的历史输入，初始输入blank") --> conv --> LayerNorm2("LayerNorm")--> activation --> tmp1
    end

    subgraph join
        input3("输入：encoder_output") --> prejoin_linear1{"prejoin_linear"}
        tmp1("predictor output") --> prejoin_linear2{"prejoin_linear"}
        prejoin_linear1 -- Yes --> prejoin_linear3("Linear(enc_output_size, join_dim)") --> tmp2(" ")
        prejoin_linear1 -- No --> tmp2
        prejoin_linear2 -- Yes --> prejoin_linear4("Linear(pred_output_size, join_dim)") --> tmp3(" ")
        prejoin_linear2 -- No --> tmp3

        tmp2 --> tmp4("+"):::font_red
        tmp3 --> tmp4

        tmp4 --> postjoin_linear1{"postjoin_linear"}
        postjoin_linear1 -- Yes --> postjoin_linear2("Linear(join_dim, join_dim)") --> tmp5(" ")
        postjoin_linear1 -- No --> tmp5

        tmp5 --> activation2("activation") --> linear2("Linear(join_dim, vocab_size)")
    end
</div>

<h2 id="ctc-attentiontransducer">3.4.  CTC-Attention/Transducer</h2>
<ul>
<li>CTC：输入encoder_output --&gt; Linear(“Linear(, vocab_size)”)</li>
<li>attention：KLDivLoss + label smoothing</li>
</ul>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><msub><mi>L</mi><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi><mo>−</mo><mi mathvariant="normal">L</mi><mn>2</mn><mi mathvariant="normal">R</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><msub><mi>L</mi><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi><mo>−</mo><mi mathvariant="normal">R</mi><mn>2</mn><mi mathvariant="normal">L</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{\mathrm{ATT}}(x, y)=(1-\alpha) L_{\mathrm{ATT-L2R}}(x, y)+\alpha {L}_{\mathrm{ATT-R2L}}(x, y) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span><span class="mbin mtight">−</span><span class="mord mathrm mtight">L</span><span class="mord mathrm mtight">2</span><span class="mord mathrm mtight">R</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mord"><span class="mord mathdefault">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span><span class="mbin mtight">−</span><span class="mord mathrm mtight">R</span><span class="mord mathrm mtight">2</span><span class="mord mathrm mtight">L</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">C</mi><mo>−</mo><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msub><mi>L</mi><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">C</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy="false">)</mo><msub><mi>L</mi><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{\mathrm{CTC-ATT}}(x, y)=\lambda L_{\mathrm{CTC}}(x, y)+(1-\lambda) {L}_{\mathrm{ATT}}(x, y) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">C</span><span class="mbin mtight">−</span><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">C</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">λ</span><span class="mclose">)</span><span class="mord"><span class="mord"><span class="mord mathdefault">L</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">C</mi><mo>−</mo><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi><mo>−</mo><mi mathvariant="normal">R</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><msub><mi>L</mi><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi><msub><mi>L</mi><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">T</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mi>L</mi><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">T</mi><mi mathvariant="normal">C</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{\mathrm{CTC-ATT-RNNT}}(x, y)=\gamma L_{\mathrm{RNNT}}(x, y)+\beta L_{\mathrm{ATT}}(x, y)+\lambda L_{\mathrm{CTC}}(x, y) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">C</span><span class="mbin mtight">−</span><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span><span class="mbin mtight">−</span><span class="mord mathrm mtight">R</span><span class="mord mathrm mtight">N</span><span class="mord mathrm mtight">N</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">R</span><span class="mord mathrm mtight">N</span><span class="mord mathrm mtight">N</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">T</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">T</span><span class="mord mathrm mtight">C</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<h2 id="解码">3.5.  解码</h2>
<p>目前支持四种解码算法：</p>
<ul>
<li>
<p>CTC greedy beam search，帧级别输出，解码过程不合并前缀，最终n-best上进行ctc序列处理。</p>
</li>
<li>
<p>CTC prefix beam search：帧级别解码，合并相同的ctc序列前缀。</p>
</li>
<li>
<p>Attention decoder beam search：基于cross-attention的label级别解码。</p>
</li>
<li>
<p>CTC + attention rescoring：第一遍采用CTC prefix beam search，该结果可作为流式结果实时返回。将CTC decoder的n-best结果，通过attention decoder进行teacher forcing rescoring（指定各时刻输出，仅提取分数），根据得分重新排序。</p>
<p>score = 反向attention decoder分 * w_r + 正向attention decoder分 * (1-w_r) + CTC分 * w_ctc</p>
</li>
</ul>
<h3 id="transducer解码">3.5.1.  Transducer解码</h3>
<ul>
<li>greedy search</li>
</ul>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;
    classDef font_green color:green;

    输入一帧encoder_out --> nblk{"上一输出是否为blank"}
    nblk -- Yes --> predictor.forward_step --> decode
    nblk -- No --> 采用之前计算的pred_out --> decode

    decode --> blank{"是否为blank"} -- No --> 输出 --> per_frame_noblk{"单帧encoder_out解码出的非blank token>=per_frame_noblk"}:::font_red
    per_frame_noblk -- No --> predictor.forward_step
    per_frame_noblk -- Yes --> 输入一帧encoder_out
    blank -- Yes --> 输入一帧encoder_out
</div>

<ul>
<li>prefix beam search</li>
</ul>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;

    RNNT("RNNT: 仅支持一帧encoder_out输出一个token"):::font_red --> shallow_fusion("后验概率加权求和")
    CTC --> shallow_fusion --> topk("取topk，得到N * N个候选") --> prefix("解码结果（不含blank）相同的合并后验概率") --> 取topk
</div>

<ul>
<li>RNNT + attention rescoring</li>
</ul>
<div class="mermaid">
  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%
flowchart TD
    classDef font_red color:red;
    classDef font_green color:green;

    decode1("CTC/RNNT prefix beam search") --> torchaudio.functional.rnnt_loss:::font_green
    decode1 --> attention_rescore
    decode1 -- w_c --> score加权求和
    torchaudio.functional.rnnt_loss -- w_t --> score加权求和
    attention_rescore -- w_a --> score加权求和
</div>

<h2 id="模型训练">3.6.  模型训练</h2>
<ul>
<li>LRScheduler</li>
</ul>
<p class="katex-block katex-error" title="ParseError: KaTeX parse error: Expected &#039;}&#039;, got &#039;_&#039; at position 69: …{\text { warmup_̲step }} &amp; , \te…">l_{r}=\begin{cases}
    l_{r} * \frac{\text { step }}{\text { warmup_step }} &amp; , \text { step } \leqslant \text { warmup_step } \\
    l_{r} * \sqrt{\frac{\text { warmup_step }}{\text { step }}} &amp; , \text { step }&gt;\text { warmup_step}
\end{cases}
</p>
<ul>
<li>fp16梯度同步</li>
</ul>
<pre class="line-numbers language-none"><code class="language-none">model.cuda()
model &#x3D; torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters&#x3D;True)
from torch.distributed.algorithms.ddp_comm_hooks import (default as comm_hooks,)
model.register_comm_hook(state&#x3D;None, hook&#x3D;comm_hooks.fp16_compress_hook)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>
<p><font color="green"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/amp_examples.html">混合精度训练</a></font></p>
</li>
<li>
<p>梯度累积</p>
</li>
</ul>
<pre class="line-numbers language-none"><code class="language-none">from contextlib import nullcontext
if is_distributed and batch_idx % accum_grad !&#x3D; 0:
    context &#x3D; model.no_sync
else:
    context &#x3D; nullcontext
with context():
    loss_dict &#x3D; model(feats, feats_lengths, target, target_lengths)
    loss &#x3D; loss_dict[&#39;loss&#39;] &#x2F; accum_grad
    loss.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>梯度裁剪</li>
</ul>
<pre class="line-numbers language-none"><code class="language-none">from torch.nn.utils import clip_grad_norm_
if use_amp:
    scaler.unscale_(optimizer)
    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)
    # We don&#39;t check grad here since that if the gradient
    # has inf&#x2F;nan values, scaler.step will skip optimizer.step().
    scaler.step(optimizer)
    scaler.update()
else:
    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)
    if torch.isfinite(grad_norm):
        optimizer.step()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>average model：取cv_loss最小或最后的 n个epoch的模型，取参数均值</li>
</ul>
<h2 id="参考资源">3.7.  参考资源</h2>
<ul>
<li><font color="green">相对position embedding：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a> </font></li>
<li><font color="green">Embedding predictor：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.07513.pdf">https://arxiv.org/pdf/2109.07513.pdf</a> </font></li>
</ul>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <i class="fas fa-exclamation-circle"></i>
            <span>
                本文不允许转载。
            </span>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            
        </div>
    </div>

    

    

    

    

    

    

    

    

    <article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/blog/yu-yin/yu-yin-xue/ying-yu-fa-yin-chang-jian-fa-yin-xian-xiang/">
                    <div class="card-content">
                        <span class="card-title">英语常见发音现象</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-09-24
                            <i class="far fa-edit fa-fw icon-date"></i>2024-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/" class="post-category">
                                    语音
                                </a>
                            
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/%E8%AF%AD%E9%9F%B3%E5%AD%A6/" class="post-category">
                                    语音学
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/">
                    <div class="card-content">
                        <span class="card-title">多维度、多粒度语音评分</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-07-23
                                <i class="far fa-edit fa-fw icon-date"></i>2022-09-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/" class="post-category">
                                    语音
                                </a>
                            
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/%E8%AF%AD%E9%9F%B3%E8%AF%84%E6%B5%8B/" class="post-category">
                                    语音评测
                                </a>
                            
                            <a href="/blog/categories/%E8%AF%AD%E9%9F%B3/%E8%AF%AD%E9%9F%B3%E8%AF%84%E6%B5%8B/%E8%AF%84%E5%88%86/" class="post-category">
                                    评分
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('50')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 梦园<br />'
            + '文章作者: nanyang<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/blog/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/blog/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/blog/libs/codeBlock/codeShrink.js"></script>



    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/blog/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="5001011615"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/blog/libs/aplayer/APlayer.min.js"></script>
<script src="/blog/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2024</span>
            
            <a href="/blog/about" target="_blank">nanyang</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
            
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/NanYANG2015" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:2271348390@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2271348390" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2271348390" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/blog/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/blog/libs/materialize/materialize.min.js"></script>
    <script src="/blog/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/blog/libs/aos/aos.js"></script>
    <script src="/blog/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/blog/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/blog/js/matery.js"></script>

    
        <!-- <script src='https://unpkg.com/mermaid@latest/dist/mermaid.min.js'></script> -->
        <script src='/blog/libs/mermaid/mermaid.min.js'></script>
        <script>
          if (window.mermaid) {
            mermaid.initialize({theme: 'neutral'});
          }
        </script>
    

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

    
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/blog/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
