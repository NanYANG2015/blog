<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>TTS数据集</title>
      <link href="/blog/yu-yin/yu-yin-he-cheng/tts-shu-ju-ji/"/>
      <url>/blog/yu-yin/yu-yin-he-cheng/tts-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="VCTK">1. VCTK</h1><h1 id="LJSpeech">2. LJSpeech</h1><h1 id="LibriTTS">3. LibriTTS</h1><p>585h，2456 说话人，24kHz。音频和文本来自 Librispeech。</p><ul><li>Zen H, Dang V, Clark R, et al. LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech[J]. Interspeech 2019, 2019.</li></ul><h1 id="LibriTTS-R">4. LibriTTS-R</h1><ul><li><p>作者：Google, 2023</p></li><li><p>对 LibriTTS 应用 speech restoration（语音恢复），改善音质。主观实验表明，相较于 LibriTTS，采用 LibriTTS-R 训练的 TTS 模型的语音自然度较高，且与 ground-truth 语音自然度相当。</p></li><li><p>speech restoration: 将语音转换到录音室音质。</p><p>模型：Miipher，text-informed 参数重合成语音恢复模型。采用说话人 encoder [20] 提取说话人 embedding（小于2s的音频重复后再提取），对原始音频降采样到 16kHz 提取 w2v-BERT 特征，采用基于 DF-Conformer 的特征 cleaner 预测干净音频的 w2v-BERT 特征，最后采用 WaveFit-5 神经声码器合成波形。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB381bf104ea76767cf638a1047687becc?method=download&shareKey=5164aa5e618571c340fd77033b1a6d03" width="443"><ul><li><p>选择 Miipher 的原因：Miipher 采用 w2v-BERT 特征（在大量有损语音上训练）替代传统的 log-mel 谱，并利用文本信息，解决了噪声掩盖音素、音素缺失（非线性音频处理、下采样等可能导致音素的重要频率分量缺失）问题。</p></li><li><p>训练</p><ul><li>私有数据集，含 2680h 含噪、录音室质量 音频对。<ul><li>目标语音：670h 录音室录制的多语种音频，24kHz。</li><li>噪声数据集：TAU Urban Audio-Visual Scenes 2021 dataset + 内部噪声集（咖啡馆、厨房、汽车等） + 噪声源。</li><li>信噪比：[5, 30] dB</li><li>采用随机 RIR 生成器。<ul><li>J. B. Allen and D. A. Berkley, “Image method for efficiently simulating small-room acoustics,” J. Acoust. Soc. Am., 1979.</li></ul></li><li>编解码：随机应用 MP3、Vorbis、A-law、Adaptive Multi-Rate Wideband (AMR-WB)、OPUS，随机比特率。</li><li>参数详见 Miipher 论文。</li></ul></li><li>先分别预训练特征 cleaner、声码器，再用特征 cleaner 的输出特征微调声码器。</li></ul></li><li><p><a href="https://google.github.io/df-conformer/miipher/">https://google.github.io/df-conformer/miipher/</a></p></li><li><p>非官方实现：<a href="https://github.com/Wataru-Nakata/miipher">https://github.com/Wataru-Nakata/miipher</a></p></li></ul></li><li><p>通过 WER、同一数据集中同一说话人的不同语音的说话人 embedding 的余弦相似度，确认恢复语音的文本、说话人保持不变。</p></li><li><p>频谱对比</p><img src="https://note.youdao.com/yws/api/personal/file/WEB8ff5cc25e042746c395a209179d8c1a6?method=download&shareKey=1b4f0b727c64098047599feecf086e60" width="748"><p>从左到右，LibriTTS 中的样本分别受下采样、环境噪声、混响、非线性语音增强影响，LibriTTS-R 分别对其进行了恢复。</p></li><li><p>Koizumi Y, Zen H, Karita S, et al. LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus[J]. arXiv preprint arXiv:2305.18802, 2023.</p></li><li><p><a href="http://www.openslr.org/141/">http://www.openslr.org/141/</a></p></li><li><p><a href="https://google.github.io/df-conformer/librittsr/">https://google.github.io/df-conformer/librittsr/</a></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音合成 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HuBERT</title>
      <link href="/blog/yu-yin/mo-xing/hubert/"/>
      <url>/blog/yu-yin/mo-xing/hubert/</url>
      
        <content type="html"><![CDATA[<p>HuBERT: Hidden-Unit BERT，自监督语音表示学习，预测 masked 帧的声音单元，学习声学模型、语言模型的联合信息。</p><p>创新点：</p><ul><li>输入原始波形。</li><li>离线 k-means 聚类生成预训练帧级对齐 label、聚类集成（Cluster Ensembles）、聚类的迭代优化。</li></ul><h1 id="模型结构">1. 模型结构</h1><img src="https://note.youdao.com/yws/api/personal/file/WEBadb1556608d2e51b58df51ea434e174d?method=download&shareKey=86093c761ff02e4eafde0417da5672e7" width="389"><ul><li><p>输入原始波形。</p></li><li><p>wav2vec 2.0 结构</p><p>16kHz 音频，CNN encoder 320倍降采样，即 20ms 帧率。</p></li><li><p>mask</p><p>随机选取 8% 的 CNN encoder 输出帧作为起点，长度为 10，替换为 <font color="green">mask embedding</font>。</p></li><li><p>参数量</p><p>BASE 90M, LARGE 300M, X-LARGE 1B。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB62034141a58deb24a0b36124dc89488b?method=download&shareKey=eddba0f9941fc84750712684dba6e17b" width="385"></li></ul><h1 id="预训练">2. 预训练</h1><ul><li><p>label</p><p>在 39 维 MFCC 上采用 k-means 聚为100类，作为第1次迭代的 label。由于隐含层特征维度更高，受内存限制，后续随机采样10%的训练数据，将隐含层表示聚为500类。</p></li><li><p>损失函数</p><p>$$p_f^{(k)}(c \mid \tilde{X}, t)&#x3D;\frac{\exp \left(\operatorname{sim}\left(A^{(k)} o_t, e_c\right) &#x2F; \tau\right)}{\sum_{c^{\prime}&#x3D;1}^C \exp \left(\operatorname{sim}\left(A^{(k)} o_t, e_{c^{\prime}}\right) &#x2F; \tau\right)}$$</p><p>其中，$A^{(k)}$ 为 第 $k$ 个聚类模型（聚类集成）的 projection 矩阵，$e_c$ 为 codeword c 的embedding，$sim(,)$ 计算两个向量的余弦相似度，$\tau&#x3D;0.1$（使 softmax 函数结果更尖锐）。</p></li><li><p>k-means</p><p><font color="green">采用 scikit-learn 实现的 MiniBatchKMeans 算法，batch size 10k 帧。采用 k-means++ [57] with 20 random starts 进行初始化。</font></p></li><li><p>聚类集成</p><p>利用多个 codebook size 不同的聚类模型，产生不同粒度的训练目标，类似多任务学习。</p></li><li><p>积量化（product quantization，PQ）[39]</p><p>将特征空间分为多个子空间，每个子空间分别量化，则目标空间的理论大小为所有 codebook size 的乘积。对于高维特征、子空间尺度差异很大的异构特征，PQ 可以有效地实现基于欧氏距离的量化，如 k-means。</p><ul><li>采用 117 维 MFCC 特征，含上下文（3帧），0阶、1阶差分、2阶差分 分别被量化为 100 entries。</li></ul></li><li><p>HuBERT BASE</p><p>Librispeech，32GPU，每张卡 batch size 最多 87.5s，2次迭代，第1次迭代 250k 步，第2次迭代 400k 步，每 100k 步约 9.5h。第2次迭代采用第1次迭代模型第 6 个 transformer 层的输出进行聚类生成的 label 。</p></li><li><p>HuBERT LARGE、X-LARGE</p><p>Libri-light，1次迭代，400k 步，128、256 GPU，每张卡 batch size 56.25s、22.5s。用 BASE 第2次迭代模型的 第9个 transformer 层的输出进行聚类。</p></li><li><p>优化器</p><p>Adam $\beta&#x3D;(0.9,0.98)$ , 前 8% 步 warm up，BASE&#x2F;LARGE&#x2F;X-LARGE 的峰值学习率分别为 5e-4&#x2F;1.5e-3&#x2F;3e-3，学习率线性变化。</p></li><li><p>SOTA 模型对比</p><table><thead><tr><th></th><th>DiscreteBERT</th><th>wav2vec 2.0</th><th>HuBERT</th></tr></thead><tbody><tr><td>输入</td><td>量化单元，有损</td><td>原始波形</td><td>原始波形</td></tr><tr><td>隐层表示</td><td></td><td>量化 waveform encoder 输出</td><td>量化 transformer  层输出。消融实验表明性能更好，可能由于 CNN encoder 模型容量有限</td></tr><tr><td>声学单元</td><td>vq-wav2vec，单一、固定 label</td><td></td><td>k-means 聚类，聚类集成，迭代优化</td></tr><tr><td>损失函数</td><td>预测 masked 单元</td><td>对比损失，需要仔细设计负样本采样；<br> auxiliary diversity loss；<br> 需要适当的Gumbel-softmax temperature annealing schedule</td><td>预测 masked 单元</td></tr></tbody></table></li></ul><h1 id="fine-tuning">3. fine-tuning</h1><ul><li><p>除了 CNN encoder，fine-tuning 其它参数，projection 层替换为随机初始化的 softmax 层。</p><ul><li>freeze-step：多少步 transformer 参数固定，仅训练新的 softmax 矩阵。</li></ul></li><li><p>ASR CTC损失，预测26个英文字母、空格、撇号、blank。</p></li><li><p>采用 <font color="green">Fairseq wav2letter++ [59] beam search 解码器，结合LM </font></p><p>$$\log p_{C T C}(Y \mid X)+w_1 \log P_{L M}(Y)+w_2 |Y|$$</p><p>其中|Y| 为预测文本长度。</p></li></ul><h1 id="实验">4. 实验</h1><ul><li><p>数据集</p><ul><li>预训练：Librispeech (960h)、Libri-light (60kh)</li><li>fine-tuning：Libri-light 10min, 1h, 10h, Librispeech train-clean-100 (100h), train-* (960h)<ul><li>上述 Libri-light 的3个子集，均一半来自 Librispeech train-clean-*，剩下的来自 train-other-500。</li></ul></li></ul></li><li><p>性能</p><img src="https://note.youdao.com/yws/api/personal/file/WEB77917921e6a0110c2b3e847fa89f3340?method=download&shareKey=ed8f6daceb889a07970c7b68f5cc10ae" width="794"><img src="https://note.youdao.com/yws/api/personal/file/WEB71db9dc1a4c1dfe21877e48e90f50e43?method=download&shareKey=f5c82c973ca79a1b337bac7233b125e2" width="762"><ul><li>对比了半监督算法（迭代伪标签IPL [12]、slimIPL [54]、noisy student[61]）、自监督算法（DeCoAR 2.0 [50], DiscreteBERT [51], wav2vec 2.0 [6]）</li><li>与 SOTA 的 wav2vec 2.0 性能一致或有提升。</li></ul></li><li><p>聚类 label $z_t$ 的评价指标<br>采用 ASR 模型强制对齐得到帧级音素label $y_t$，</p><p>联合分布<br>$$p_{y z}(i, j)&#x3D;\frac{\sum_{t&#x3D;1}^T\left[y_t&#x3D;i \wedge z_t&#x3D;j\right]}{T}$$</p><p>边缘概率<br>$$p_z(j)&#x3D;\sum_i p_{y z}(i, j)$$</p><p>$$p_y(j)&#x3D;\sum_j p_{y z}(i, j)$$</p><p>每个音素 $i$ 对应的最可能的聚类 label $j$<br>$$z^*(i)&#x3D;\arg \max <em>j p</em>{y z}(i, j)$$</p><p>同理，<br>$$y^*(j)&#x3D;\arg \max <em>i p</em>{y z}(i, j)$$</p><p>条件概率<br>$$p_{y \mid z}(i \mid j)&#x3D;p_{y z}(i, j) &#x2F; p_z(j)$$</p><ul><li><p>音素纯度<br>$$\mathbb{E}<em>{p_z(j)}\left[p</em>{y \mid z}\left(y^*(j) \mid j\right)\right]$$</p><p>表示如果将 k-means label 替换为 它对应的最可能的音素，帧级音素准确率。若每个 k-means label 唯一对应 1个音素，则音素纯度为1。</p></li><li><p>聚类纯度</p><p>$$\mathbb{E}<em>{p_y(i)}\left[p</em>{z \mid y}\left(z^*(i) \mid i\right)\right]$$</p><p>聚类纯度越高，表明同一音素的帧更可能对应同一聚类 label。当聚类类别数增加时，该值会下降。</p></li><li><p>phone-normalized 互信息（PNMI）</p><p>$$\begin{aligned} \frac{I(y ; z)}{H(y)} &amp; &#x3D;\frac{\sum_i \sum_j p_{y z}(i, j) \log \frac{p_{y z}(i, j)}{p_y(i) p_z(j)}}{\sum_i p_y(i) \log p_y(i)} \ &amp; &#x3D;1-\frac{H(y \mid z)}{H(y)}\end{aligned}$$</p><p>已知 k-means label $z$，对音素 label $y$ 的熵的减少量。</p></li></ul></li><li><p>消融实验</p><table><thead><tr><th>实验</th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>k-means 稳定性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf4420dc406c6d88d2ada6590a849725b?method=download&shareKey=400d7143fa4a13c031033d7c9fd91198"></td><td>同一参数、多次实验标准差很小；<br> 聚类模型训练集数据量提升PNMI会改善，但增益有限，因此采样部分训练集进行 k-means 聚类是可行的；<br> 相较于 MFCC，聚类迭代优化 PNMI 显著提升</td></tr><tr><td>聚类所用特征的层数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7325b2e4a53333be5d279ce50f0516b6?method=download&shareKey=e20622a825887ddf006978c23ed87980"> <br> 第0层表示 transformer 层的输入 <br> 基线：MFCC (cluster purity, phone purity, PNMI) &#x3D; (0.099, 0.335, 0.255) C &#x3D; 100，(0.031, 0.356, 0.287) C&#x3D;500</td><td>BASE-it1 最后几个 transformer 层 用于聚类的效果急剧下降，可能和第1次迭代预训练标签质量较差有关。</td></tr><tr><td>损失函数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6f928f272cac99502c7161d5785de58f?method=download&shareKey=f4e6e8f390d7cfb1d4c2479dbb1e8d22"> <br> chenone[64]：基于字符，强制对齐生成标签</td><td>若仅考虑未被 mask 的位置，该模型类似聚类模型&#x2F;声学模型。<br>仅考虑 masked 位置，驱使模型学习声学模型、语言模型的联合信息，效果最好。</td></tr><tr><td>聚类集成</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB20cc38279c55aac79d177009e320e25e?method=download&shareKey=d8d37bc6a4e4b85aab28715b10cf46b3"><br> 积量化：采用 117 维 MFCC 特征，含上下文（3帧），0阶、1阶差分、2阶差分 分别被量化为 100 entries</td><td>对比表5、表6，聚类集成比单个聚类模型性能好</td></tr><tr><td>超参数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5fd73ef4bfe7b4a97b3a369b7e12b2d9?method=download&shareKey=9568d7ce1eb06efb3a6b492b4e39976b"> <img src="https://note.youdao.com/yws/api/personal/file/WEB02250c74874949959a7c9f96d27dd4b8?method=download&shareKey=692177e2e4ee1cee4326877ce335eb48"></td><td>选取 8% 的帧作为 mask 起点性能较好；<br> 增大 batch size 可以显著提升性能；<br> 训练更长时间模型性能更好</td></tr></tbody></table></li></ul><h1 id="论文">5. 论文</h1><ul><li>Hsu W N, Bolte B, Tsai Y H H, et al. Hubert: Self-supervised speech representation learning by masked prediction of hidden units[J]. IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing, 2021, 29: 3451-3460.</li><li>代码、预训练模型、fine-tuned 模型：<a href="https://github.com/pytorch/fairseq/tree/master/examples/hubert">https://github.com/pytorch/fairseq/tree/master/examples/hubert</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT</title>
      <link href="/blog/nlp/mo-xing/bert/"/>
      <url>/blog/nlp/mo-xing/bert/</url>
      
        <content type="html"><![CDATA[<p>BERT: Bidirectional Encoder Representations from Transformers。无监督预训练的语言表示模型。</p><p><font color="red">创新点：</font></p><ul><li><p>双向性：采用 Transformer 的 Encoder 结构、masked LM 预训练策略，能够同时考虑左右上下文的信息，更好地建模上下文信息和语义关系，学习深度双向表示，而传统的表示学习只学习单向 LM，或者独立训练 left-to-right（LTR）、right-to-left LM，再简单地拼接两个方向的表示。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB6d2dd6042429f8d7362ba9a48585da8a?method=download&shareKey=03683aa46038f4e89675a79b985b5e00" width="703"></li><li><p>预训练策略：通过 masked LM、预测是否为下一个句子（NSP）两个预训练目标，来学习 token 级的上下文语义表示 和 句子间的关系。对于涉及文本对的下游任务，拼接文本对 + self-attention 有效地包含了文本对的 bidirectional cross attention。可以简单地加输出层 fine-tuning，得到广泛的NLP任务的SOTA模型，而不需要特定于任务的架构修改。</p></li></ul><h1 id="模型结构">1. 模型结构</h1><h2 id="输入表示">1.1. 输入表示</h2><img src="https://note.youdao.com/yws/api/personal/file/WEB2076aca79516cbb11b75e1f084cce145?method=download&shareKey=f107d1cd5cb7e44a8f03992d8985d099" width="699"><p>其中，token embedding 采用 WordPiece embedding，共 3万 tokens。</p><h2 id="模型结构-1">1.2. 模型结构</h2><p>Transformer 结构</p><p>$\mathrm{BERT}_{\mathrm{BASE}}$: L&#x3D;12, H&#x3D;768, A&#x3D;12, 总参数量 110M，与 GPT 模型大小相同。</p><p>$\mathrm{BERT}_{\mathrm{LARGE}}$: L&#x3D;24, H&#x3D;1024, A&#x3D;16, 总参数量 340M。</p><p>其中，L 为 Transformer block 数，H 为 hidden size，A 为 self-attention 的头数，feed-forward size 为 4H。</p><h2 id="无监督预训练目标">1.3. 无监督预训练目标</h2><ul><li>masked LM (MLM，完形填空)<ul><li>每个序列随机 mask 15% 的 token。80% 替换为 [MASK] token，10% 替换为随机 token，10% 保留原 token。被 mask 的 token 对应的输出表示接1层分类层，预测被 mask 的 token 的词表ID。</li><li>分词后，对于partial word pieces，mask 时不特别考虑。</li></ul></li><li>预测是否为下一个句子 (NSP)<ul><li>50% 为实际的下一个句子，50% 从语料库中随机选取。[CLS] token 对应的最终输出表示，接1层分类层。</li><li>本文的“句子”可以是连续文本的任意跨度，而不是语言学意义上的1句，从而可以学习长序列。</li><li>对问答、自然语言推理等涉及句子间关系的任务都非常有益。</li><li>最终模型 NSP 准确率为 97%-98%。</li><li><font color="teal">若不 fine-tuning，[CLS] token 对应的最终输出表示不是有意义的句子表示。</font></li></ul></li></ul><h2 id="用于特定任务">1.4. 用于特定任务</h2><p>将预训练表示用于下游任务的两种策略：</p><ul><li>fine-tuning：在预训练模型上添加较少的随机初始化的参数，并在下游任务上对所有参数联合微调。如GPT、BERT。</li><li>基于特征：从预训练模型中提取固定的特征表示，采用特定于任务的模型架构。如ELMo。<ul><li>拼接 BERT 最后 4 层的表示。</li></ul></li></ul><p>在 1 个 Cloud TPU 上训练最多 1h，在 1 个 GPU 上几个小时，就可以复现本文结果。</p><h1 id="实验">2. 实验</h1><h2 id="参数">2.1. 参数</h2><ul><li>绝对位置embedding，最大长度为512。</li><li>激活函数：gelu</li><li>优化器：各层 dropout 概率 0.1，L2 weight decay 0.01，学习率 1e-4，前 1w 步 warmup，学习率线性衰减，Adam，$\beta_1&#x3D;0.9$，$\beta_2&#x3D;0.999$。</li><li>预训练语料库：约 3.3 billion 单词。</li><li>预训练：batch size 256，1 百万步，约为 40 epochs。</li><li>由于长序列 attention 计算量较大，为了加速预训练，90% 的 step 采用序列长度 128，剩下的 10% 采用序列长度 512 来学习位置 embedding。<font color="green">batch 的序列长度为 batch 内序列的最大长度，是动态变化的？</font></li><li>在16个TPU芯片上训练$\mathrm{BERT}<em>{\mathrm{BASE}}$，64个TPU芯片上训练$\mathrm{BERT}</em>{\mathrm{LARGE}}$，预训练需要4天。</li></ul><h2 id="对比、消融实验">2.2. 对比、消融实验</h2><h3 id="预训练">2.2.1. 预训练</h3><ul><li><p>训练目标</p><img src="https://note.youdao.com/yws/api/personal/file/WEBcf8bec39f002e85aa5fb5027951b5e5f?method=download&shareKey=920d74916d4d5ec4f76f379662396ba7" width="354"><p>在所有任务上，LTR LM 都不如 masked LM。</p><p>NSP 训练目标对问答、自然语言推理等涉及句子间关系的任务都非常有益。</p></li><li><p>mask 策略</p><img src="https://note.youdao.com/yws/api/personal/file/WEB1fd4195f93f978a1079f18c1b5e74f08?method=download&shareKey=fb92e712f10869f86ed9672d46c077e0" width="343"><p>若仅使用 [MASK]、或仅使用随机替换，性能较差。</p></li><li><p>收敛速度</p><img src="https://note.youdao.com/yws/api/personal/file/WEB39173a9a2ea7b31c1008345bb24c5f0a?method=download&shareKey=a22a8addcd76420edd8b8b83b94b59b7" width="340"><p>由于 masked LM 只对每个 batch 中 15% 的 token 进行预测，相较于 LTR LM，收敛速度更慢，但性能更好。</p></li><li><p>训练步数<br>相较于 500k 步的预训练，1M 步提升了约 1% 的准确度。</p></li><li><p>模型大小<br>模型越大，预训练表示更有表现力，性能越好（前提是该模型已经被充分地预训练），特别是在训练集较小的下游任务上。但是，$\mathrm{BERT}_{\mathrm{LARGE}}$ 在小数据集上 fine-tuning 时有时不稳定。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB72c21baeb6fdf2488dd9d5ae9193eaa9?method=download&shareKey=c61e41cf5801f4ab251b5518ebc2ccb0" width="340"></li></ul><h3 id="fine-tuning">2.2.2. fine-tuning</h3><ul><li><p>虽然最优超参数值是特定于任务的，但以下可能值范围对于所有任务都能 work well。由于fine-tuning 非常快，最优超参数可以在开发集上穷举搜索。</p><ul><li>batch size：16、32</li><li>学习率：5e-5, 3e-5, 2e-5</li><li>epoch 数：2, 3, 4</li></ul></li><li><p>11 个 NLP 任务（GLUE 数据集包含的 8 个NLP任务（排除了1个有问题的数据集）、SQuAD v1.1、SQuAD 2.0、SWAG）上均取得了 SOTA 结果，包括 token 级任务（如 NER、QA）和句子级任务（如自然语言推理）。</p> <img src="https://note.youdao.com/yws/api/personal/file/WEBe2aef02cb0d986bf1a9c23c56e23d3e0?method=download&shareKey=2e29b4a63754a221ae493376b3ff1897" width="704"><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB072fe8e332c1500ac0cc84064971fa90?method=download&shareKey=319c32231b3458bb01e6ce25118cbe1e"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf6dbf658b24ec8736f0f1195f0fe988c?method=download&shareKey=72b4de7d008bf77f03794e87fbbe0775"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB63401f0b0c341fcc1011ece08c537f7f?method=download&shareKey=8723c249d59edf055ee22bcd9a63f47c"></td></tr></tbody></table><ul><li>GLUE 数据集介绍详见论文附录B。</li><li>The Stanford Question Answering Dataset (SQuAD v1.1): 包含10万个众包问题&#x2F;答案对，给定1个问题和1个wikipidea中包含答案的段落，任务是预测答案在段落中的跨度。<ul><li><font color="green">fine-tuning 时引入开始向量 $S \in \mathbb{R}^H$ 和结束向量 $E$</font>，第 $i$ 个token为答案区间起点的概率为$P_i&#x3D;\frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}$。</li><li>训练目标：答案区间正确的开始位置、结束位置的 log-likelihood 的和。</li><li>预测：候选答案区间的得分为 $S \cdot T_i+E \cdot T_j$，选取 $j \geq i$ 且得分最大的区间。</li><li>fine-tuning：先用 TriviaQA 数据集，再用 SQuAD v1.1 数据集。即使不用 TriviaQA 数据集，仍然在很大程度上优于所有的现有系统。</li></ul></li><li>SQuAD 2.0：段落中可能不包含答案。<ul><li>用开始、结束位置均对应 [CLS] token 来建模。对比 没有答案的得分 $s_{null} &#x3D; S \cdot C + E \cdot C$ 和 非零跨度的最佳得分 $\hat{s_{i, j}}&#x3D;\max <em>{j \geq i} S \cdot T_i+E \cdot T_j$，若 $\hat{s</em>{i, j}}&gt;s_{\mathrm{null}}+\tau$ 则预测为包含答案，其中 $\tau$ 在开发集上选择以最大化 F1 值。</li></ul></li><li>SWAG (Situations With Adversarial Generations)：给定 1 个句子，从 4 个选项中选择最合理的延续。<ul><li>将给定句子与候选分别拼接为 4 个输入序列，<font color="green">引入 1 个向量</font>，其与 [CLS] 的最终输出表示的点积作为每个候选的得分，并用 softmax 层归一化。</li></ul></li></ul></li></ul><h3 id="基于特征的方法">2.2.3. 基于特征的方法</h3><img src="https://note.youdao.com/yws/api/personal/file/WEBd1026d3f97d6217753114ba637726d93?method=download&shareKey=9ac5c0784b9a1b4ccb76822110f43bc9" width="344"><p>NER 任务：将第一个 sub-token 的表示作为输入。</p><p>拼接 BERT 最后 4 层的表示，性能最好。</p><h3 id="BERT-ELMo-GPT-表示学习模型对比">2.2.4. BERT, ELMo, GPT 表示学习模型对比</h3><ul><li>BERT、GPT 微调。<ul><li>双向性和两个预训练目标是 BERT 性能更优的主要原因（见论文Tabel 5的消融实验）。</li><li>其它区别<ul><li>训练语料：GPT BooksCorpus(800M words), BERT BooksCorpus(800M words) + English Wikipedia(2500M words，只使用文本段落，忽略列表、表格、标题)。</li><li>GPT 只在微调时引入[SEP]、[CLS] token，BERT 在预训练时学习 [SEP]、[CLS] 和 segment embedding。</li></ul></li></ul></li><li>ELMo 基于特征。</li></ul><h1 id="论文">3. 论文</h1><ul><li>Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional Transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</li><li>代码、预训练模型：<a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></li></ul><h1 id="个人思考">4. 个人思考</h1><ul><li>表示学习的理解<ul><li>神经网络LM：神经网络倒数第二层的表示，经过最后一个分类层，输出词表中每个单词的概率分布。由于上文&#x2F;下文&#x2F;上下文不同时，当前词的概率分布不同，因此，模型倒数第二层的表示 蕴含了输入文本的信息。包括：句子中各个词之间的语义关系，句子中其它词与当前词的语义相关性，不同词的语义重要性等等。体现了模型的语义理解和表达能力。</li><li>预训练模型：例如，在 LTR LM 预训练中，每个 token 对应的输出表示都蕴含了 包含当前 token 的上文信息，可以用于下游任务。</li><li>masked LM 预训练<ul><li>[MASK] token 对应的输出表示蕴含了上下文信息。由于[MASK] 在句子中的位置随机，所以句子中各个位置的输出表示都蕴含了上下文信息。<font color="green"> 但是，由于 [MASK] token 无语义信息，仅使用 [MASK] token 替换当前词时，当前位置的输出表示 与 当前输入的关联性较差，因此，当下游任务采用基于特征的方法时，相较于采用 BERT 提出的 mask 策略，性能较差。</font></li><li>保留当前词：当上下文不足以推断当前位置信息时，该策略会驱使模型也考虑当前位置的输入。而同时采用其它两种替换策略，避免了模型原样输出。同时，缓解预训练与下游任务的不匹配，因为实际预测时不会出现[MASK]。</li><li>随机替换：该策略可以促使模型学习上下文和当前词的关系。实际应用中，输入可能包含错误单词，可以提高模型的鲁棒性。另外，由于模型无法区分句子中的原始单词 和 随机替换的单词，也不知道需要预测的单词的位置，所以驱使模型学习每个位置的token表示。<font color="green">（避免模型在正常词的位置原样输出，[MASK]对应的位置输出上下文表示）</font></li></ul></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ECAPA-TDNN</title>
      <link href="/blog/yu-yin/shuo-hua-ren-shi-bie/ecapa-tdnn/"/>
      <url>/blog/yu-yin/shuo-hua-ren-shi-bie/ecapa-tdnn/</url>
      
        <content type="html"><![CDATA[<h1 id="结构特点">1. 结构特点</h1><p>集成了计算机视觉领域最新的增强模块：Res2Net、SE block，聚合多层的特征，并采用 channel-dependent 帧注意力改进了统计池化模块。</p><h1 id="系统结构">2. 系统结构</h1><img src="https://note.youdao.com/yws/api/personal/file/WEBe03b7ab1ccd2402b22ce13eb3be6ce5a?method=download&shareKey=c864e0ebfeafd8d51f27e9d819df55e5" width="443"><img src="https://note.youdao.com/yws/api/personal/file/WEBb244ce5a3bcad0e1a25fa95a504b6f89?method=download&shareKey=3f37b0c013ccdeb9c16a91877158c65c" width="440"><h1 id="特征提取">3. 特征提取</h1><ul><li>数据增强：为每句话生成6个额外的样本<ul><li>kaldi 加噪（MUSAN babble、noise）、加混响（RIR数据集[21]）</li><li>SoX 变速（tempo up, tempo down）、FFmpeg 压缩（交替 opus 或 aac 压缩）</li><li>speechbrain 实现<ul><li>concat_augment：可选择多种数据增强方式 串行（样本量不变） 或 1个batch内并行（batch_size *&#x3D; X）</li><li>时域增强<ul><li>变速：采用基于sinc的插值进行重采样，比如重采样到原始音频长度的[90%, 100%, 110%]。<font color="green">注意：由于特征提取时默认所有信号为原始采样率，所以重采样到90%采样率的实际效果，为语速*1.11，音高*1.11 </font></li><li>采用滤波器随机mask部分频带。<font color="red">作用：帮助模型学习到依赖所有频带，而不是其中的少部分</font></li><li>随机mask部分音频片段，可选是否填充白噪声<code>4A * torch.rand() - 2A</code>，其中<code>A</code>为原始音频的平均幅值，默认全部置0。作用：帮助模型学习到依赖整个信号</li><li>上述3种增强方式 可选是否添加，串行</li></ul></li><li>环境扰动<ul><li>加混响：采用room 冲击响应<ul><li><a href="http://www.openslr.org/resources/28/rirs_noises.zip">http://www.openslr.org/resources/28/rirs_noises.zip</a></li></ul></li><li>加babble</li><li>加噪</li><li>上述3种增强方式 可选是否添加，串行</li></ul></li><li>由此导致的音频长度改变 需要通过末尾截断 或 补0 来与原音频保持一致</li><li>实现详见<code>speechbrain.processing.speech_augmentation</code></li></ul></li></ul></li><li>输入特征：80维 MFCCs，<font color="green">随机裁剪为2s的片段</font>，倒谱均值减法，不加VAD。<ul><li>speechbrain 实现：60维 Fbank</li></ul></li><li>SpecAugment</li></ul><h1 id="Res2Net">4. Res2Net</h1><p><a href="https://nanyang2015.github.io/blog/ji-qi-xue-xi/cnn/res2net/">Res2Net</a>：表示多尺度的特征，增大感受野。</p><p>论文与 speechbrain 实现的差异：论文中 Res2 Dilated Conv1D 前后的dense layer，前一个 dense layer 降低通道维度，后一个 dense layer 恢复通道维度，从而控制参数量。</p><h1 id="Squeeze-Excitation-Block">5. Squeeze-Excitation Block</h1><p><a href="https://nanyang2015.github.io/blog/ji-qi-xue-xi/cnn/senet/">SE Block</a>：显式地建模通道间的相关性，根据输入的全局特征，自适应地对各个通道的 feature map 重新加权。</p><h1 id="多层特征聚合">6. 多层特征聚合</h1><p>出发点：浅层的特征也有助于更鲁棒的embedding。</p><ul><li>拼接所有SE-Res2Blocks的输出feature map，经过1层dense层。</li><li>SE-Res2Block 的残差连接实现为前面所有SE-Res2Blocks的输出的和。求和，而不是拼接，用于约束模型参数量。（speechbrain 未实现）</li></ul><h1 id="channel-dependent-attentive-statistics-pooling">7. channel-dependent attentive statistics pooling</h1><ul><li><font color="red">出发点：将时间注意力机制进一步扩展到通道维度，使得网络可以关注不会同时激活的说话人特征，如元音和辅音属性（不同 channel 的时间注意力系数不一样）。</font></li><li>实现<ul><li><p><font color="red">将 $\boldsymbol{h}_t$ 与其整句的 非加权的均值、标准差拼接（$C*&#x3D;3$），使得attention能自适应句子的全局属性，如噪声、录音环境。</font></p></li><li><p>channel attention：channel 维的 bottleneck 结构的两层全连接，所有时刻参数共享。</p><ul><li>speechbrain 实现<pre class="line-numbers language-none"><code class="language-none">self.tdnn &#x3D; TDNNBlock(channels * 3, attention_channels, 1, 1)  # 降维self.tanh &#x3D; nn.Tanh()self.conv &#x3D; Conv1d(in_channels&#x3D;attention_channels, out_channels&#x3D;channels, kernel_size&#x3D;1)  # 恢复原始形状attn &#x3D; self.conv(self.tanh(self.tdnn(attn)))attn &#x3D; F.softmax(attn, dim&#x3D;2)  # 沿时间轴做softmax<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><font color="red">attentive statistics pooling：attention 到不同帧，类似VAD以检测无关的非语音帧。</font></p><p>加权均值：<br>$$\tilde{\mu}<em>c&#x3D;\sum_t^T \alpha</em>{t, c} h_{t, c}$$</p><p>加权标准差：<br>$$\tilde{\sigma}<em>c&#x3D;\sqrt{\sum_t^T \alpha</em>{t, c} (h_{t, c}-\tilde{\mu}<em>c)^2}&#x3D;\sqrt{\sum_t^T \alpha</em>{t, c} h_{t, c}^2-\tilde{\mu}_c^2}$$</p><p>pooling层的输出为 $\tilde{\mu}$、$\tilde{\sigma}$ 拼接</p></li></ul></li></ul><h1 id="bottleneck-结构的两层全连接">8. bottleneck 结构的两层全连接</h1><p>bottleneck 层的输出可以作为低维的说话人embedding。</p><h1 id="训练">9. 训练</h1><ul><li>采用参考文献[23]中的 triangular2 policy，循环学习率，[1e-8, 1e-3]，一次循环为 130k 迭代。</li><li><font color="green">AAM-softmax，margin 0.2，softmax prescaling of 30 for 4 cycles。</font></li></ul><h1 id="实验结果">10. 实验结果</h1><img src="https://note.youdao.com/yws/api/personal/file/WEBe9098186e1d400650ef1821ea33629ef?method=download&shareKey=f434762b2145eb1db885eb270b0c89d4" width="727"><img src="https://note.youdao.com/yws/api/personal/file/WEB08c933cf955a2b189352633460c292fb?method=download&shareKey=0a0a54795b59fa9de701da3df3d15f9e" width="350"><h1 id="论文">11. 论文</h1><ul><li>Desplanques B, Thienpondt J, Demuynck K. ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification[J]. arXiv preprint arXiv:2005.07143, 2020.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 说话人识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SENet</title>
      <link href="/blog/ji-qi-xue-xi/cnn/senet/"/>
      <url>/blog/ji-qi-xue-xi/cnn/senet/</url>
      
        <content type="html"><![CDATA[<ul><li><p><font color="red">出发点：显式地建模通道间的相关性，根据输入的全局特征，自适应地对各个通道的feature map 重新加权。</font></p></li><li><p>模块结构</p><img src="https://note.youdao.com/yws/api/personal/file/WEB4ed16068c8341262f00526b614216eb7?method=download&shareKey=2d73fbb8a7eb2bde12495222c2cfce0a" width="589"><img src="https://note.youdao.com/yws/api/personal/file/WEB56580c1f90254168b06438d27a116ece?method=download&shareKey=685a5692827f2751871a4000ab160045" width="462"><ul><li><p>Squeeze</p><p>global average pooling：分别计算每个通道的激活均值。</p><p>$$ z_c&#x3D;\mathbf{F}<em>{s q}\left(\mathbf{u}<em>c\right)&#x3D;\frac{1}{H \times W} \sum</em>{i&#x3D;1}^H \sum</em>{j&#x3D;1}^W u_c(i, j) $$</p></li><li><p>Excitation</p><p>自适应权重：</p><p>$$\mathbf{s}&#x3D;\mathbf{F}_{e x}(\mathbf{z}, \mathbf{W})&#x3D;\sigma\left(\mathbf{W}_2 \delta\left(\mathbf{W}_1 \mathbf{z}\right)\right)$$</p><p>其中，$\sigma$ 为 sigmoid 函数，$\delta$ 为ReLU，通过 $\mathbf{W}_1 \in \mathbb{R}^{\frac{C}{r} \times C}$、$\mathbf{W}_2 \in \mathbb{R}^{C \times \frac{C}{r}}$ bottleneck结构的两层全连接限定模型复杂度。</p><p>加权：channel-wise 乘法</p><p>$$\widetilde{\mathbf{x}}<em>c&#x3D;\mathbf{F}</em>{\text {scale }}\left(\mathbf{u}_c, s_c\right)&#x3D;s_c \cdot \mathbf{u}_c$$</p><ul><li>设计的考虑点<ul><li>能学习到通道间非线性的相关性；</li><li>允许多个通道同时被激励，而不是one hot的；</li><li>每个通道的权重根据输入自适应地计算。</li></ul></li></ul></li></ul></li><li><p>性能</p><ul><li><p><font color="red">很容易与其它结构整合。在许多任务上都取得了性能提升。</font></p></li><li><p>CPU推理时间</p><p>对于 $224 \times 224$ 的图像，ResNet-50 为164ms，SE-ResNet-50 为 167ms。</p><p>* GPU库中 global pooling 未做优化</p></li><li><p>增加的参数量</p><p>$$\frac{2}{r} \sum_{s&#x3D;1}^S N_s \cdot C_s^2$$</p><p>其中，$S$ 为 stage 数，$N_s$ 为第s个stage的层数，这些层通道数相同，为$C_s$。</p><p>通常，最后一个 stage 的 SE blocks 参数量相对较多，在参数量是关键考虑因素的场景下，可以移除该部分。可以实现参数量仅相对增加 4%，而相较于不移除，ImageNet 上的 top-1 error 仅下降 &lt;0.1%。</p></li></ul></li><li><p>实验</p><ul><li><p>模型的性能并不是随着 $r$ 的增加而单调提升，可能与 $r$ 过大时过拟合有关。设置 $r&#x3D;16$ 在模型准确度和复杂度之间取得了较好的权衡。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB0b7c919f145654b8478606ddf3765ee0?method=download&shareKey=eb52f523ac477cb3a72e15b6f9dd5547" width="368"></li><li><p>可视化</p><p>从 ImageNet 中选取了语义、外观差异较大的4个类，每个类选取50个样本，计算每个阶段最后一个SE block 的平均激活，并将所有类的平均激活作为参考。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB8d17a71ee60fac532851e3f7d0e2cf10?method=download&shareKey=fab4d939765032da527da4d78ab6b369" width="764"><ul><li>SE_2_3、SE_3_4：较低的层中，各channel的重要性分布，对于不同的类几乎相同。即较低层的特征通常更一般，在分类任务中具有类别不可知性。</li><li>SE_4_6、SE_5_1：更深的层中，各channel的重要性分布，对于不同的类差异较大。而通过 SE block 自适应地重新加权，有助于特征提取和专门化。</li><li>SE_5_2、SE_5_3：在网络末端，各channel的重要性趋于相同。因此，在最后阶段删除SE block，可以显著地减少总体参数量，但性能损失很小。</li></ul></li></ul></li><li><p>论文</p><p>J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> CNN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Res2Net</title>
      <link href="/blog/ji-qi-xue-xi/cnn/res2net/"/>
      <url>/blog/ji-qi-xue-xi/cnn/res2net/</url>
      
        <content type="html"><![CDATA[<ul><li><p>出发点：表示多尺度的特征、增大感受野。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB61ffb780869eb695d691e4d54876ac5b?method=download&shareKey=3f447136c62dbd906f1b59cde85798da" width="385"><p>$$\mathbf{y}_i&#x3D; \begin{cases}\mathbf{x}_i &amp; i &#x3D; 1 \ \mathbf{K}_i\left(\mathbf{x}_i\right) &amp; i &#x3D; 2 \ \mathbf{K}_i\left(\mathbf{x}<em>i+\mathbf{y}</em>{i-1}\right) &amp; 2&lt;i \leqslant s\end{cases}$$</p></li><li><p>名称由来：残差块里又有残差连接。</p></li><li><p>多尺度的理解：$\mathbf{x}_2$ 经过1个 $3 \times 3$ 的卷积得到 $\mathbf{y}_2$，而 $\mathbf{y}_2$ 会作为 $\mathbf{K}_3$ 的输入，再次经过1个 $3 \times 3$ 的卷积。2次 $3 \times 3$ 的卷积相当于1个 $5 \times 5$ 的卷积，因此，$\mathbf{K}_3$ 融合了 $3 \times 3$ 的感受野的特征 和 $5 \times 5$ 的感受野的特征。以此类推。得到的 $\mathbf{y}$ 可以表征多尺度的特征。</p></li><li><p>性能</p><ul><li><p>很容易与其它结构整合。</p></li><li><p>由于具有较强的多尺度特征表征能力，在许多任务上都取得了性能提升，如图片分类、目标检测、语义分割、显著目标检测等，对于不同尺寸的对象性能都有提升。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB8f041f566f23b3eaa720ab325b3570c6?method=download&shareKey=8b34ae8793fd313603365c485b8a7c82" width="320"><img src="https://note.youdao.com/yws/api/personal/file/WEBad279f36483cb4953f11ee2987747fa1?method=download&shareKey=21ea4d5cec3f46fcf6856e76b1dc810a" width="377"></li><li><p><font color="green">虽然 $\mathbf{y}_i$ 需要顺序计算，但Res2Net模块引入的额外运行时间通常可以忽略不计。</font></p></li><li><p>相较于模型深度 depth、1层的 channel 数 width、cardinality[68]，增加 scale 可以获得更快的性能提升。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB0b0ad93f6733adeef588208ca17e008c?method=download&shareKey=3b201a76aede1628dec4ee4d9199853a" width="380"><p>*scale为5、6时性能提升有限，可能由于CIFAR数据集中的图片太小（$32 \times 32$）。</p><p>*当图片中的对象尺寸已经可以被Res2Net模块覆盖时，继续增加 scale ，性能提升有限；若固定模型复杂度，增加 scale 会导致每个感受野的通道数减少，可能会降低模型处理特定尺度特征的能力。</p></li></ul></li><li><p>论文<br>Gao S H, Cheng M M, Zhao K, et al. Res2net: A new multi-scale backbone architecture[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 43(2): 652-662.</p><ul><li>第2章相关工作<ul><li>计算机视觉领域的Backbone Networks：AlexNet、VGGNet、Network in Network、GoogLeNet、Inception、ResNet、DenseNet、DLA等。</li><li>视觉任务的多尺度表示：spatial pyramid pooling、feature pyramid、atrous convolutional。</li></ul></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> CNN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>不当停顿</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/te-zheng/bu-dang-ting-dun/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/te-zheng/bu-dang-ting-dun/</url>
      
        <content type="html"><![CDATA[<ul><li>感知停顿：令听者感到话语中断（、破坏语义&#x2F;韵律单位）的语音现象</li><li>不当停顿与二语水平：不当停顿与学习者的单词辨认、意群划分、内容理解能力有关</li><li>停顿时长阈值<ul><li>朗读、复述：100ms</li><li>自发语音：200-300ms</li><li>句间：400ms （取论文中边界位置不当停顿的平均时长）</li></ul></li><li>常出现的位置<ul><li>短语内：如自我修正</li><li>较简单和意义紧密相联的句法单位之间<ul><li>短语间<ul><li>主语为人称代词的主谓结构之间，如：Take it back to the shop where you (259ms) bought it.</li><li>句末短状语之前，如：Do you want to go to the cinema (460ms) tonight?</li></ul></li><li>从句间<ul><li>常见于宾语从句和主句之间，如：I don’t know (400ms) what I can do with it.</li><li>并列从句之间：Do you want to go to the cinema tonight，(649ms) or have you got to stay late at work again?</li></ul></li></ul></li><li><font color="red">注意：并不是所有标点符号的位置都应该停顿 </font>，如：<ul><li>直接引语和句末报告短语之间，如：”Will they come tomorrow?” (557ms) Betty asked.</li><li>句首附加成分（如well、yes、no、oh）之后，如：Well, (663ms) let’s have a look at the newspaper.</li></ul></li></ul></li><li><font color="red"> 无实际静默的主观停顿：主要包括3个方面 </font><ul><li>发音问题<ul><li>无协同发音，如：likes swimming，未连读，读了两个&#x2F;s&#x2F;</li><li>元音拖音</li><li>词尾增读&#x2F;ə&#x2F;</li></ul></li><li>错误的重音模式：应弱读的单词（特别是介词）被重读，容易感觉其后有停顿</li><li>不恰当的音高重设，使得调群被错误地分割。如：Tell me that story about the clever monkey, (70ms) mummy.</li></ul></li><li>程欣, 陈桦. 二语朗读中不当停顿的感知研究[J]. 外语与外语教学, 2020, 1(01): 81.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 特征 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ETS 《Automated Speaking Assessment》</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ets-automated-speaking-assessment/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ets-automated-speaking-assessment/</url>
      
        <content type="html"><![CDATA[<h1 id="Automated-speaking-assessment">1. Automated speaking assessment</h1><ul><li>Automated speaking assessment: Using language technologies to score spontaneous speech[M]. Routledge, 2019.</li></ul><p><em>Innovations in LANGUAGE LEARNING and ASSESSMENT at ETS</em> 系列共3卷：</p><ul><li>Volume 1: Second Language Educational Experiences for Adult Learners</li><li>Volume 2: English Language Proficiency Assessments for Young Learners</li><li>Volume 3: Automated speaking assessment: Using language technologies to score spontaneous speech</li></ul><p>本书是ETS 2019年出版的，侧重于自发语音评分。由于该书的目标群体包括非专业人士，所以技术细节相对较少。</p><h1 id="Educational-Testing-Service-ETS">2. Educational Testing Service (ETS)</h1><blockquote><p>美国教育考试服务中心，成立于1947年，是全球最大的民营非营利性教育考试和评估机构。ETS每年对全球180多个国家逾5000多万次考试进行开发、管理和评分，包括TOEFL (Test of English as a Foreign Language), TOEIC (Test of English for International Communication), GRE (Graduate Record Examination)等。其研究范围非常广泛，涵盖全球英语教学、学习和评估（认知模型和学习设计原则），评估的设计与开发（信度与效度，涉及统计学、心理测量学和评估技术等），基于人工智能的个性化学习路径（持续评估进度、即时反馈、建议）等。其产品包括SpeechRater（自发语音评分和反馈）、e-rater scoring engine（写作技巧评估和反馈）。</p></blockquote><ul><li><a href="https://www.ets.org/about/what.html">https://www.ets.org/about/what.html</a></li><li><a href="https://en.wikipedia.org/wiki/Educational_Testing_Service">https://en.wikipedia.org/wiki/Educational_Testing_Service</a></li><li><a href="https://baike.baidu.com/item/ets/12424958">https://baike.baidu.com/item/ets/12424958</a></li></ul><h1 id="自动语音评分系统">3. 自动语音评分系统</h1><p>对于研究人员和从业者来说，重要的是不要孤立地考虑自动评分系统，而是将其作为整个评估周期的一部分，包括测试规范、任务设计、特征提取、分数生成、分数报告和分数使用。</p><ul><li>测试设计：首先，确定语言测试的设计目的是什么，期望给予什么反馈。基于此，可以决定引出什么领域（比如学术场景）下的什么样的语言行为。之后，可以更精细地分析所考察的各个维度，同时考虑如何以最适合自动评分的方式实现。</li><li>Construct：评估旨在考察的一组知识、技能和能力，由语言使用场景的需求确定。如果自动评分系统难以准确、全面地测量，测试设计者应该考虑结合人工评分和自动评分是否更合适。</li><li>口头表达的持续时间：使用人工或自动评分，当回答很短时，口语能力的某些方面可能难以可靠地评估，比如流利度。一种解决方案是跨多个回答聚合信息，或者仅使用可以从较短的响应中可靠地提取的测量值，例如内容准确性。</li><li>音质：确保使用高质量的音频非常重要，会影响自动评分系统的性能。并且，应该制定一个程序来标记任何可能音质较差的数据，以便它们能够得到额外的关注。</li><li>有效性：自动语音评分的有效性，还应该考虑测试的性质、如何使用分数等。<ul><li>低风险：比如用于了解学生的学习进度。</li><li>高风险：比如用于升学、招聘等。</li></ul></li><li><blockquote><p>自动评分系统缺乏背景知识，难以对创意、逻辑、想法的质量进行评价。</p><p>自动评分系统用于高风险评估的唯一评分者必须满足3个条件：透明的内部机制、有效性证据的广泛基础（比如自动评分与受试者在某些交流环境下的语言行为强相关，表明了其语言水平）、检测可能的异常性能的质量控制系统。</p></blockquote><ul><li>Zhang, M. , 2013. Contrasting automated and human scoring of essays. ETS R &amp; D Connections 21, 1–11.</li></ul></li></ul><h1 id="ASR">4. ASR</h1><ul><li>non-native ASR的难点：发音错误、不流畅（破坏了语法结构；填充词通常很短，容易与其它单词（如a）混淆）、语法或用词错误。<ul><li>非母语说话人最普遍的3种不流畅现象：filled pauses（比如uh, um, and hm）、不完整的单词（比如Tha-）、重复（比如he he he）。</li></ul></li><li>TOEFL iBT口语考试的非母语自发语音，最优的WER约为20%-25%。依据对人类转写员一致性的研究，TOEFL iBT 数据的WER下限可能在 15% 左右。同时也意味着非母语ASR训练集中噪声水平比较高。</li><li>除识别结果外，ASR还可输出置信度分（混淆网络）、AM分和LM分、时间边界，用于后续评分特征的计算。</li><li>建议<ul><li>对于音素级的声学模型，重要的是训练集尽可能覆盖所有可能的音素上下文，而不是目标领域的内容。</li><li>对于大多数应用，找到一个声学条件相似的开源数据集，结合少量领域内数据，可以取得不错的声学模型效果。</li><li>标准化数据收集流程：包括麦克风、采样率、音频文件格式等。如果不确定目标应用程序的设置，最好使用高质量的设备、高采样率、无损压缩或不压缩。此外，重要的是录音环境（如环境噪声、室内混响）、说话人（如年龄、性别、语言水平）、语音（自发语音或朗读）等的特征与目标应用程序匹配。记录每条音频任何相关的元数据，便于后处理。</li><li>可以采用诸如ROVER投票之类的方法，将对同一音频的多个转写员的转写结果合并成一个、更高质量的转写。<ul><li>Fiscus J G. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)[C]&#x2F;&#x2F;1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings. IEEE, 1997: 347-354. (引用量：1615)</li></ul></li><li>转写指南应涵盖非语音声音（如噪声、笑声等）、非字母表示的单词（如数字、日期、货币等，一般应采用如three而不是3）的处理。</li><li>语言模型训练集应尽可能地接近应用场景。所需的数据量很大程度上取决于应用场景下数据的可变性。</li></ul></li></ul><h1 id="filtering-model">5. filtering model</h1><blockquote><ul><li><p>混合评分流程</p><img src="https://note.youdao.com/yws/api/personal/file/WEBc8532827c64dd0e36b6c72d7404fe031?method=download&shareKey=4a3b2eb463fbdfc5a480154fc1ddc6ac" width="647px"><p>*若extended filter判为可评分，再输入baseline filter</p></li><li><p>不可评分（non-scorable）的回答类型</p><ul><li>technical difficulty(TD)：严重音质问题，如音量小、截幅<ul><li>静音SI：听不到任何环境音</li><li>噪声掩盖作答NC</li><li>失真DR：如快速回放、慢速回放、过度放大等</li><li>采样缺失MS：采样点丢失</li><li>其它OT</li></ul></li><li>0分回答<ul><li>无语音XS：录音正常但说话人无语音，如呼吸、咳嗽等</li><li>非英语作答XE</li><li>可懂度低XU</li><li>离题XT</li><li>预制的回答：如抄袭</li><li>其它XO</li><li>由于受试者不配合的行为在其数据中极为罕见，所以本文不检测非英语作答、离题、预制的回答等</li></ul></li><li>ASR结果不可信：如ASR结果不合预期的过短或过长，置信度分低</li><li>自动评分不可信：如ASR结果为空</li></ul></li><li><p>baseline filter</p><ul><li>针对TD、0分回答、自动评分不可信</li><li>特征<ul><li>基础特征（22个）：如单词数、语音片段时长、停顿频率、停顿时长</li><li>声学特征（16个）：如能量、音调的均值、标准差，频谱（如MFCC）的变化</li><li>ASR特征（7个）：如AM分、LM分、置信度分的均值、标准差</li></ul></li><li>采用特征选择算法，从上述45个特征中选取了6个特征<ul><li>单词数</li><li>除去不流畅、停顿的片段总时长</li><li>音频能量均值</li><li>音频能量最大值</li><li>音频能量方差</li><li>基于MFCC的音质分nrprob</li></ul></li><li>采用决策树C4.5模型</li></ul></li><li><p>extended filter</p><ul><li>还处理ASR结果不可信的情况。通过分析大分差数据，基于规则构建。</li><li>ConfidenceScoreFilter：词级平均置信度分&lt;0.4</li><li>ShortResponseFilter：仅两个单词或以下，语音过短评价不可靠</li><li>LongResponseFilter：朗读或复述题，词数&gt;&#x3D;2*预期值。因为SpeechRater会因多说的内容扣分，而人工评分允许重复等</li></ul></li><li><p>评价</p><ul><li><p>数据集</p><img src="https://note.youdao.com/yws/api/personal/file/WEB6f071533e49919ece88056f1298dff6f?method=download&shareKey=69dba328a14d9c968e0190239dad3762" width="857px"><p>*Table5：未标记ASR结果不可信的数据，所以未统计</p></li><li><p>评价指标</p><ul><li>precision：检测为不可评分的数据中真正不可评分数据的占比</li><li>recall：不可评分数据被检测到的占比</li><li>f-score</li></ul></li><li><p>评价结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEB6f70b78a1d1a03cc65d9c2156e29df31?method=download&shareKey=4351b82dde2600091db8ee0b67fe1a6e" width="590px"><img src="https://note.youdao.com/yws/api/personal/file/WEBb653d451ce44038ed7f615a9716b895f?method=download&shareKey=039ecb1c0a98b1a3eece0002d00e32a0" width="709px"><ul><li>由于人工未标记ASR结果不可信的数据，所以extended filter虚警率高符合预期。</li><li>采用extended filter可以检测出更多人机偏差较大的数据。HMSD：人机评分绝对差（table8-9 排除了人工标记不可评的数据）</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEBab4164d29d519b516b8dde7368d4cd2c?method=download&shareKey=90676260ad564a2daa060efebc972192" width="578px"><ul><li>imputation：检测为不可评的数据 取该受试者可评数据的机器分的均值。</li><li>hybrid：检测为不可评的数据 人工评分。</li></ul></li></ul></li><li><p>hybrid评分的缺点：人工评分的人力和时间成本。</p></li><li><p>参考文献</p><ul><li>Bejar, I.I. , 2011. A validity-based approach to quality control and assurance of automated scoring. Assess. Edu. 18 (3), 319–341 .</li></ul></li></ul></blockquote><ul><li>Yoon S Y, Zechner K. Combining human and automated scores for the improved assessment of non-native speech[J]. Speech Communication, 2017, 93: 43-52.</li><li>不可评分的回答类型，除上述外，还有：<ul><li>笼统的回答：仅包含填充词的回答或笼统的回答，例如“I don’t know”、“It is too difficult to answer”、“well” 等。</li><li>重复题干：重复提问或朗读听力材料。</li></ul></li><li>除无语音外，其它类型的回答可能由于流利度较好而得到较高的分数。</li><li>non-scorable检测<ul><li>通过计算ASR结果与参考答案的文本相似度检测离题、重复题干等。采用word embedding可以避免要求词的精确匹配。缺点：需要事先为每个题目收集大量的参考答案。</li><li>SpeechRater: 实际应用场景下游戏作答的比例非常低，因此主要检测无语音和不适合自动评分的数据。<ul><li>采用音质、能量、语音长度等特征，决策树模型，检测不可评和0分数据。</li><li>基于规则的filters：过滤作答过短、录音的平均能量过大或过小、ASR结果不可信的数据，避免评分不可靠。</li></ul></li></ul></li><li>【音质】Jeon J H, Yoon S Y. Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment[C]&#x2F;&#x2F;Thirteenth Annual Conference of the International Speech Communication Association. 2012.</li><li>【非英语】Yoon S Y, Higgins D. Non-English response detection method for automated proficiency scoring system[C]&#x2F;&#x2F;Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications. 2011: 161-169.</li><li>【抄袭】Wang X, Evanini K, Bruno J, et al. Automatic plagiarism detection for spoken responses in an assessment of english language proficiency[C]&#x2F;&#x2F;2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2016: 121-128.</li><li>【离题】Malinin A, Van Dalen R, Knill K, et al. Off-topic response detection for spontaneous spoken english assessment[C]&#x2F;&#x2F;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016: 1075-1084.</li><li>【garbage model】Cheng, J. , Shen, J. , 2011. Off-topic detection in automated speech assessment applications. In: Proceedings of InterSpeech, pp. 1597–1600 .</li></ul><h1 id="评分特征">6. 评分特征</h1><ul><li>评分标准包含3个维度：发音和流利度、语用（词汇和语法）、内容和语篇。</li><li>SpeechRater计算了大约 170 个特征，本书不包含约60个左右与受限或可预测语音相关的特征。排除了特征间相关系数&gt;0.9的特征。</li><li>特征处理<ul><li>为了支持特征与评分之间的非线性关系、归一化特征分布，将原始特征进行非线性变换，比如取平方根、取对数等<ul><li><font color="red"> 以下部分特征备注了对应的变换，部分特征未备注 </font></li></ul></li><li>符号翻转（乘以-1）：使其与人工分正相关?</li><li>99% winsorization处理：根据特征分布，阈值取分布上端、下端的各5%，超过离群点阈值的特征值匹配到对应的阈值</li><li>归一化（0均值、1标准差）</li></ul></li><li>数据集：从2012-2015 TOEFL iBT考试中收集的约24万个回答。</li></ul><h2 id="发音">6.1. 发音</h2><table><thead><tr><th></th><th>特征</th><th>Pearson相关系数</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>音段特征</td><td>AcousticModelScore</td><td><font color="red"> 0.43 </font></td><td>整句AM分&#x2F;音素数</td><td></td></tr><tr><td></td><td>Pronunciation1</td><td>-0.37</td><td>所有单词AM分的和</td><td></td></tr><tr><td></td><td>Pronunciation2</td><td>0.27</td><td>Pronunciation1&#x2F;单词数</td><td></td></tr><tr><td></td><td>Pronunciation3</td><td>0.39</td><td>Pronunciation1&#x2F;音素数</td><td></td></tr><tr><td></td><td>VowelDuration</td><td><font color="red"> -0.4 </font></td><td>句中各个元音的平均时长分别与在native数据集上统计的各音素的平均时长的绝对差</td><td></td></tr><tr><td>重音相关</td><td>StressedSyllPercent</td><td>0.38</td><td>重读音节占比</td><td></td></tr><tr><td></td><td>StressDistanceSyllMean</td><td>-0.37</td><td>重读音节距离的均值（in syllables）</td><td></td></tr><tr><td></td><td>StressDistanceSyllSD</td><td>-0.33</td><td>重读音节距离的平均差（in syllables）</td><td>log</td></tr><tr><td></td><td>StressDistanceMean</td><td><font color="red"> -0.47 </font></td><td>重读音节距离的均值（s）</td><td>log</td></tr><tr><td></td><td>StressDistanceSD</td><td><font color="red"> -0.41 </font></td><td>重读音节距离的平均差（s）</td><td></td></tr><tr><td>语调相关</td><td>ToneDistanceSyllMean</td><td>0.08</td><td>带调音节距离的均值（in syllables）</td><td>sqrt</td></tr><tr><td></td><td>ToneDistanceSyllSD</td><td>0.10</td><td>带调音节距离的平均差（in syllables）</td><td>sqrt</td></tr><tr><td>韵律相关</td><td>VowelPercent</td><td>-0.30</td><td>元音时长&#x2F;总单词时长的百分比</td><td>\</td></tr><tr><td></td><td>VowelDurationSD</td><td>-0.26</td><td>元音音程标准差</td><td></td></tr><tr><td></td><td>ConsonantDurationSD</td><td>-0.20</td><td>辅音音程标准差</td><td></td></tr><tr><td></td><td>SyllableDurationSD</td><td>-0.31</td><td>音节时长标准差</td><td></td></tr><tr><td></td><td>ConsonantSDNorm</td><td>-0.22</td><td>辅音音程标准差&#x2F;辅音音程平均时长</td><td></td></tr><tr><td></td><td>SyllableSDNorm</td><td>-0.24</td><td>音节时长标准差&#x2F;音节平均时长</td><td></td></tr><tr><td></td><td>VowelPVI</td><td>-0.39</td><td>元音音程的PVI</td><td></td></tr><tr><td></td><td>ConsonantPVI</td><td>-0.36</td><td>辅音音程的PVI</td><td></td></tr><tr><td></td><td>SyllablePVI</td><td>-0.36</td><td>音节的PVI</td><td>log</td></tr><tr><td></td><td>VowelPVINorm</td><td>-0.25</td><td>归一化的元音音程PVI</td><td></td></tr><tr><td></td><td>ConsonantPVINorm</td><td>-0.32</td><td>归一化的辅音音程PVI</td><td>\</td></tr><tr><td></td><td>SyllablePVINorm</td><td>-0.29</td><td>归一化的音节PVI</td><td></td></tr></tbody></table><ul><li>特征与<font color="red">总分人工分（非维度分）</font>间的相关系数，<font color="green">P值&lt;0.01，有显著的统计学差异</font>。</li><li>两个声学模型：ASR中的AM用non-native语音训练，且仅用于ASR；第2个AM用native（主要）、高阶的non-native语音训练，采用上述识别结果计算AM分。<ul><li>Chen L, Zechner K, Xi X. Improved pronunciation features for construct-driven assessment of non-native spontaneous speech[C]&#x2F;&#x2F;Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the Association for Computational Linguistics. 2009: 442-449.</li></ul></li><li>重读、语调检测决策树：采用30个左右能量、音调、时长、word-intensity、单词内音节位置、词典重音、与上一重读或带调音节的距离（syllables）相关的特征，利用人工标注的数据集，训练决策树检测子句或句子中的重读音节或带调的音节（升调、降调、无调）。<ul><li>Zechner K, Xi X, Chen L. Evaluating prosodic features for automated scoring of non-native read speech[C]&#x2F;&#x2F;2011 IEEE workshop on automatic speech recognition &amp; understanding. IEEE, 2011: 461-466.</li></ul></li><li>相较于音节相关的重读特征，时间相关的特征与人工分的相关性更高，可能由于还表征了语速。</li><li>语调：更高水平的说话人可能表现出更长的带调短语（基于音节，而不是时间），且长度变化更大（由特征相关系数的正负推断）。</li><li>韵律特征：Chen L, Zechner K. Applying rhythm features to automatically assess non-native speech[C]&#x2F;&#x2F;Twelfth annual conference of the international speech communication association. 2011.</li><li>展望：提取语调轮廓相关的特征。</li></ul><h2 id="流利度">6.2. 流利度</h2><table><thead><tr><th></th><th>特征</th><th>Pearson相关系数</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>停顿相关</td><td>FilledPauseRate</td><td>-0.23</td><td>filled pause (uh, um)数&#x2F;秒</td><td>sqrt</td></tr><tr><td></td><td>Silences</td><td>-0.26</td><td>静音次数</td><td></td></tr><tr><td></td><td>SilenceMean</td><td>-0.32</td><td>静音时长均值(s)</td><td>log</td></tr><tr><td></td><td>SilenceAbsoluteDeviation</td><td>-0.32</td><td>静音时长平均差</td><td></td></tr><tr><td></td><td>SilenceRate1</td><td><font color="red"> -0.50 </font></td><td>静音次数&#x2F;总词数</td><td></td></tr><tr><td></td><td>SilenceRate2</td><td><font color="red"> -0.45 </font></td><td>静音次数&#x2F;总时长(s)</td><td></td></tr><tr><td></td><td>LongSilenceDeviation</td><td>-0.26</td><td>长静音的平均差(s)</td><td></td></tr><tr><td></td><td>SilenceDistribution1</td><td><font color="red"> -0.45 </font></td><td>子句内长静音&#x2F;子句内静音</td><td></td></tr><tr><td></td><td>SilenceDistribution2</td><td>-0.28</td><td>子句内静音的时长均值</td><td></td></tr><tr><td>语速相关</td><td>SpeakingRate</td><td><font color="red"> 0.54 </font></td><td>词数&#x2F;秒（排除音频首尾的静音）</td><td></td></tr><tr><td></td><td>ArticulationRate</td><td>0.38</td><td>词数&#x2F;秒（总时长-静音、filled pauses时长）</td><td></td></tr><tr><td></td><td>LengthOfRunWords</td><td><font color="red"> 0.45 </font></td><td>chunk（连续的词序列，中间没有filled pauses、超过0.195s的静音）内平均词数</td><td>sqrt</td></tr><tr><td></td><td>RunLengthWordsSD</td><td><font color="red"> 0.41 </font></td><td>chunk平均差</td><td></td></tr><tr><td>修正相关</td><td>InterruptionPointRate1</td><td>-0.23</td><td>Repair IP数&#x2F;子句总数</td><td>sqrt</td></tr><tr><td></td><td>InterruptionPointRate2</td><td>-0.26</td><td>Repair IP数&#x2F;总词数</td><td>sqrt</td></tr><tr><td></td><td>RepetitionRate</td><td>-0.28</td><td>重复数&#x2F;总词数</td><td></td></tr></tbody></table><ul><li>静音：两个单词间的静音时长 &gt; 0.145s；子句内长静音：时长 &gt; 0.195s；长静音：时长 &gt; 0.495s。</li><li>IP（interruption point）：一段语音被丢弃或修正，开始新的句子或短语的位置。</li><li>定位语音中的修正现象，包括：restart（丢弃句子，重新开始一句话）、重复（单词或短语）、修正（一个或多个单词被替换为“正确”的单词）。<ul><li>Chen L, Yoon S Y. Detecting structural events for assessing non-native speech[C]&#x2F;&#x2F;Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications. 2011: 38-45.</li><li>Chen L, Yoon S Y. Application of structural events detected on ASR outputs for automated speaking assessment[C]&#x2F;&#x2F;Thirteenth Annual Conference of the International Speech Communication Association. 2012.</li></ul></li><li>修正现象与总分相关度较低。</li><li>表达关心或道歉时，较慢的语速可能更表明其交际能力。</li></ul><h2 id="词汇、语法">6.3. 词汇、语法</h2><table><thead><tr><th>维度</th><th>子类别</th><th>特征</th><th>Pearson相关系数（绝对值）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>词汇</td><td>词汇丰富度</td><td>Types</td><td><font color="red"> 0.49 </font></td><td>unique单词数</td><td>\</td></tr><tr><td></td><td></td><td>TypeTokenRatio</td><td>0.10</td><td>unique单词数&#x2F;总词数</td><td></td></tr><tr><td></td><td>平均单词难度</td><td>AverageVocabularyFrequency</td><td>0.36</td><td></td><td></td></tr><tr><td></td><td></td><td>LogVocabularyFrequency</td><td>0.16</td><td>各unique单词的log词频的和&#x2F;unique单词数</td><td>\</td></tr><tr><td></td><td>词频表</td><td>VocabularyRank1</td><td>0.17</td><td>回答中在最高频的100词中的单词数&#x2F;回答总词数</td><td></td></tr><tr><td></td><td></td><td>VocabularyRank3</td><td>0.11</td><td>Rank3：词频排序301-700</td><td></td></tr><tr><td></td><td></td><td>VocabularyRank6</td><td>0.14</td><td>Rank6：词频排序大于3000</td><td>\</td></tr><tr><td>语法</td><td>基于词性</td><td>SyntacticSimilarityScore1</td><td>0.30</td><td>回答与人工分为1的语料库中的回答的POS序列相似度</td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore2</td><td>0.34</td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore3</td><td>0.39</td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore4</td><td><font color="red"> 0.42 </font></td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityMax</td><td>0.31</td><td>相似度最高的人工分</td><td></td></tr><tr><td></td><td>基于子句</td><td>ClausesCount2</td><td>0.33</td><td></td><td></td></tr><tr><td></td><td></td><td>DependentClause</td><td>0.26</td><td>附属限定从句数量</td><td></td></tr><tr><td></td><td></td><td>DependentInfinitives</td><td>0.21</td><td>附属非限定从句数量</td><td>log(x+0.5)</td></tr><tr><td></td><td>基于短语</td><td>NounPhrases</td><td>0.38</td><td>名词短语数量</td><td></td></tr><tr><td></td><td></td><td>PrepositionalPhrases</td><td>0.33</td><td>介词短语数量</td><td>\</td></tr><tr><td></td><td></td><td>VerbPhrases</td><td><font color="red"> 0.40 </font></td><td>动词短语数量</td><td>\</td></tr><tr><td></td><td></td><td>ComplexNominals</td><td>0.22</td><td>基于embedding的复杂名词短语</td><td></td></tr><tr><td></td><td></td><td>CoordinatePhrases</td><td>0.10</td><td>并列短语（并列连词连接的短语）数量</td><td>sqrt</td></tr><tr><td></td><td></td><td>NounPhrasesNorm</td><td>0.11</td><td>名词短语数量&#x2F;总词数</td><td></td></tr></tbody></table><ul><li>特征提取流程：ASR -&gt; 句子边界检测 -&gt; 词性标记、句法分析 -&gt; 特征计算。</li><li>平均单词难度：从TOEFL学术语言语料库中统计了大量学术用语的频次，Rank为在按词频逆序排列的词汇表中的次序。<ul><li>Biber D, Conrad S, Reppen R, et al. Speaking and writing in the university: A multidimensional comparison[J]. TESOL quarterly, 2002, 36(1): 9-48.</li></ul></li><li>词频表（Lexical frequency profile）：根据在TOEFL学术语言语料库中的频次分为了6个等级：Rank1（最高频的1-100）、Rank2（101-300）、Rank3（301-700）、Rank4（701-1500）、Rank5（1501-3000）、Rank6（大于3000）。</li><li>基于词性的特征：Bhat S, Yoon S Y. Automatic assessment of syntactic complexity for spontaneous speech scoring[J]. Speech Communication, 2015, 67: 42-57.</li><li>SpeechRater使用的词性标记：Penn Treebank词性集（36个）+6个为口语设计的标记。<ul><li>Marcinkiewicz M A. Building a large annotated corpus of English: The Penn Treebank[J]. Using Large Corpora, 1994, 273.（引用量9805）</li></ul></li><li>词性序列特征可以在一定程度上评价语法准确性。另外，相较于基于句法分析的特征，基于词性的特征对不可避免的ASR错误更加鲁棒。但解释性较差。</li><li>基于子句、短语的特征：Chen M, Zechner K. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech[C]&#x2F;&#x2F;Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human Language Technologies. 2011: 722-731.<ul><li>附属从句、限定从句、非限定从句 <a href="https://langeek.co/en/grammar/course/713/dependent-clauses/intermediate">https://langeek.co/en/grammar/course/713/dependent-clauses/intermediate</a></li></ul></li><li>未归一化的特征与回答的长度强相关。</li><li>暂无评价词汇、语法准确性的特征：一方面，非母语自发语音ASR WER接近20%，（另外由于语言模型的作用，）内部分析表明识别结果仅包含原始语法错误的约30%；另一方面，从较短的口语回答中提取的特征波动较大，比如，1个句子边界检测错误就可能严重影响特征的准确性。<ul><li>作文自动评分中有评价词汇、语法的自动检错技术。</li></ul></li><li>其它特征<ul><li>词汇丰富度<ul><li>D measure：由于TTR受文本长度影响，D measure是TTR的一种复杂数学变换，不受长度影响。<ul><li>Malvern D, Richards B, Chipere N, et al. Lexical diversity and language development[M]. New York: Palgrave Macmillan, 2004.</li></ul></li><li>trigrams频次得分</li></ul></li><li>句法复杂性<ul><li>句子平均长度</li><li>T-unit（主句，包含其附属的从句或非子句单元）内动词短语的数量</li></ul></li><li>单词、语法准确性<ul><li>没有错误的T-unit占比</li><li>动词时态、第三人称单数、复数标记、介词、冠词的准确性</li></ul></li></ul></li><li>所需技术：子句或句子边界检测、词性标注、句法解析器。</li></ul><h2 id="内容">6.4. 内容</h2><ul><li>基于参考答案：与提前给定的1个参考答案计算文本相似度。<ul><li>WordNet-based text-to-text similarity metrics：Xiong W, Evanini K, Zechner K, et al. Automated content scoring of spoken responses containing multiple parts with factual information[C]&#x2F;&#x2F;Speech and Language Technology in Education. 2013.</li><li>1个参考答案无法覆盖所有可能的正确回答，但是在实际的大规模测试中收集多个参考答案成本较高。</li></ul></li><li>基于回答：计算与已经评分的回答的文本相似度。<ul><li>内容向量分析、潜在语义分析、Pointwise Mutual Information（互信息）<ul><li>Xie S, Evanini K, Zechner K. Exploring content features for automated speech scoring[C]&#x2F;&#x2F;Proceedings of the 2012 conference of the North American chapter of the Association for Computational Linguistics: Human language technologies. 2012: 103-111.</li><li>优化<ul><li>word embedding</li><li>Doc2Vec：将回答转换为100维的向量<ul><li>Le Q, Mikolov T. Distributed representations of sentences and documents[C]&#x2F;&#x2F;International conference on machine learning. PMLR, 2014: 1188-1196.（引用量10384）</li></ul></li></ul></li></ul></li><li>ROUGE：用于评价文本摘要和机器翻译质量的标准指标。采用少量（少于10）满分回答。<ul><li>Loukina A, Zechner K, Chen L. Automatic evaluation of spoken summaries: the case of language assessment[C]&#x2F;&#x2F;Proceedings of the ninth workshop on innovative use of NLP for building educational applications. 2014: 68-78.</li></ul></li><li>对于每个题目，需要事先人工标注足量的回答。</li></ul></li><li>基于提示：比如引发自由表述的听力或阅读材料。相较于前两者，相关度较低，但成本低。</li></ul><h2 id="语篇">6.5. 语篇</h2><ul><li>基于surface（单词和连接词链）的特征<table><thead><tr><th>特征</th><th>与语篇人工分的Pearson相关系数（基于人工转写）</th><th>（基于ASR输出）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>Pronouns</td><td>0.204</td><td>0.186</td><td>代名词数量</td><td></td></tr><tr><td>PronounsNorm2</td><td>-0.128</td><td>-0.106</td><td>代名词词数&#x2F;unique名词数量</td><td></td></tr><tr><td>Conjunctions</td><td>0.174</td><td>0.209</td><td>连词数量</td><td>\</td></tr><tr><td>ConnectiveTypes</td><td>0.381</td><td>0.352</td><td>unique语篇连接词数量</td><td></td></tr><tr><td>Connectives</td><td>0.337</td><td>0.33</td><td>语篇连接词数量</td><td>\</td></tr><tr><td>ConnectiveChains1</td><td>0.068</td><td>0.155</td><td>回答与各参考答案的语篇连接词链的BLEU分的最大值</td><td></td></tr><tr><td>ConnectiveChains2</td><td>0.282</td><td>0.268</td><td>回答与各参考答案的语篇连接词链的编辑距离的最小值</td><td></td></tr></tbody></table><ul><li>语篇连接词（来自Penn Discourse Treebank中的语篇连接词列表），分为4类：<ul><li>所有从属连词：从属连词引入句法上依赖于主句的子句，常见关系：时间（比如when、as soon as）、因果（比如because）、让步（比如although、even though）、目的（比如so that、in order that）、条件（比如if、unless）。</li><li>所有并列连词：比如and、but、or。</li><li>部分Adverbial连接词：表达两个事件或状态之间的语篇关系，比如however、therefore、then、as a result、in addition、in fact等。</li><li>相邻句子间的隐式连接词。</li></ul></li><li>连接词链特征：采用高分数据，提取连接词链（仅保留代词、连词、语篇连接词，其他词都被删除）。计算测试数据与每个参考回答的连接词链相似度。<ul><li>相似度计算：BLEU分数、编辑距离、WER（归一化的编辑距离）</li></ul></li><li>特征较简单，旨在对ASR错误和口语中的语法错误更鲁棒。</li><li>Wang X, Evanini K, Zechner K, et al. Modeling Discourse Coherence for the Automated Scoring of Spontaneous Spoken Responses[C]&#x2F;&#x2F;SLaTE. 2017: 132-137.</li></ul></li><li>基于RST的特征<ul><li>修辞结构理论（Rhetorical Structure Theory，RST）<ul><li>首先确定基本语篇单元（elementary discourse units，EDU）（不重叠的文本片段），通过修辞关系连接相邻单元构成层次树结构。</li><li>78种修辞关系：53种单核关系（两个相邻单元，其一更重要） + 23种多核关系（两个或多个单元，权重相同）。<ul><li>Carlson L, Marcu D. Discourse tagging reference manual[J]. ISI Technical Report ISI-TR-545, 2001, 54(2001): 56.</li></ul></li><li>RST Discourse Treebank：一个标准测试集，包含从Penn Treebank 中精选的385篇华尔街日报文章，并基于RST框架进行了语篇结构标注。</li><li>标注工具：<a href="https://www.isi.edu/~marcu/discourse/">https://www.isi.edu/~marcu/discourse/</a></li><li>可基于上述数据集构建自动RST解析器，检测语法错误，并提供反馈。</li></ul></li><li>数据集：ETS对1440条TOEFL iBT测试中的自发口语进行了标注。<ul><li><p>相较于书面文本，非母语的自发语音经常包含语法错误、填充、修正、重复、false start、未完成的话语等。因此在RST Discourse Treebank标记指南的基础上，添加如下关系：</p><ul><li>不流畅：其中不流畅的片段是satellite，对应的流畅片段是核。</li><li>Awkward：语篇结构不恰当的部分。<ul><li>aukuard-Reason：预期的关系很明确，但表达不连贯。</li><li>aukward-Other：该片段与周围的话语没有明确关系。</li></ul></li><li>未完成的句子：考试时间结束但考生未说完。其中，未完成的片段是satellite，语篇树的根结点是核。</li><li>Discourse Particle：比如<em>you know</em>、<em>right</em>等，它们是相邻片段的satellite。</li><li>Wang X, Bruno J, Molloy H, et al. Discourse annotation of non-native spontaneous spoken responses using the rhetorical structure theory framework[C]&#x2F;&#x2F;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2017: 263-268.</li></ul></li><li><p>对语篇连贯性人工评分，分为1（不连贯）、2（有点连贯）、3（高度连贯） 3档，并要求对于评分为2的数据，尽量忽略不流畅或语法错误，标记出不连贯的具体位置。</p></li><li><p>示例</p><img src="https://note.youdao.com/yws/api/personal/file/WEB262ab5bd2c3af12b5701a09012d4363e?method=download&shareKey=ff921fe7831c7a7f826d9b431c4d0079" width="578px"></li></ul></li><li>特征<table><thead><tr><th>特征</th><th>Pearson相关系数-总分（人工转写、人工标注RST树）</th><th>语篇分</th><th>总分（ASR、自动解析的RST树）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>EDUs</td><td><font color="red"> 0.612 </font></td><td>0.366</td><td>0.424</td><td>EDU数量</td><td></td></tr><tr><td>Relations</td><td><font color="red"> 0.624 </font></td><td>0.391</td><td>0.401</td><td>关系数量</td><td></td></tr><tr><td>AwkwardRelations</td><td><font color="red"> -0.425 </font></td><td>-0.533</td><td>-0.096</td><td>不恰当的语篇结构关系数量</td><td></td></tr><tr><td>RhetoricalRelations</td><td><font color="red"> 0.719 </font></td><td>0.536</td><td>0.418</td><td>排除不流畅、awkward的关系数量</td><td></td></tr><tr><td>RhetoricalRelationTypes</td><td><font color="red"> 0.675 </font></td><td>0.547</td><td>0.314</td><td>unique rhetorical relation 数量</td><td></td></tr><tr><td>RhetoricalRelationPercent</td><td><font color="red"> 0.586 </font></td><td>0.609</td><td>0.225</td><td>rhetorical relation 占比</td><td></td></tr><tr><td>TreeDepth</td><td><font color="red"> 0.402 </font></td><td>0.249</td><td>0.329</td><td>RST树的深度</td><td></td></tr><tr><td>EDUDepthRatio</td><td><font color="red"> 0.536 </font></td><td>0.308</td><td>0.316</td><td>EDU数量&#x2F;RST树深度</td><td></td></tr></tbody></table></li><li>Wang X, Gyawali B, Bruno J V, et al. Using Rhetorical Structure Theory to assess discourse coherence for non-native spontaneous speech[C]&#x2F;&#x2F;Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019. 2019: 153-162.</li><li>展望<ul><li>提高对ASR错误的鲁棒性，比如根据置信度分排除部分单词或短语。</li><li>开发内容特征：细粒度地评价指定观点是否存在。</li><li>开发与主题发展结构相关的特征：比如比喻的使用、论证策略、对事实示例的恰当使用等。</li></ul></li></ul></li></ul><h1 id="评分模型">7. 评分模型</h1><ul><li>人工设计的特征可以保证与特定的评分维度一致，使得评分模型易于解释。但是可能和人类评分的方式不完全一致，比如人类关注沟通的准确有效性，这是比句法复杂性特征复杂得多的关系。此外，ETS尝试了采用深度学习构建端到端评分系统，到目前为止，相较于SpeechRater基线的改进不大（从Pearson r&#x3D;0.585到r&#x3D;0.602），但其缺乏可解释性，且可能学习到与评分维度不相关的特征。<ul><li>Chen L, Tao J, Ghaffarzadegan S, et al. End-to-end neural network based automated speech scoring[C]&#x2F;&#x2F;2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018: 6234-6238.</li></ul></li><li>纯数据驱动的特征选择可能导致评分模型学习到与口语水平无关变量（如语音长度）的影响。SpeechRater结合理论基础和数据驱动进行特征选择。为了确保最终模型是合适的，至少在高风险测试中，自动选择的特征集由专家审查，并在必要时进行调整。<ul><li><p>自动特征选择：LASSO</p><p>Loukina A, Zechner K, Chen L, et al. Feature selection for automated speech scoring[C]&#x2F;&#x2F;Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications. 2015: 12-19.</p></li></ul></li><li>评分模型<ul><li>决策树：更接近人类评分员的打分过程，比如在不同分数段考虑不同的评分特征、支持评分与特征间的非线性关系。</li><li>随机森林回归：可以视为多个评分员的平均分。</li><li>SVM：可以识别不同分数水平之间的边界，实现数据的最佳分离。</li><li>但是，相较于线性回归，上述模型通常并无改进或改进很小。</li><li>深度神经网络的缺点：可解释性差，难以获得可用的、有意义的反馈信息。</li></ul></li><li>为了支持特征与评分之间的非线性关系，可以事先将原始特征进行非线性变换，比如取平方根、取对数等。</li><li>特征在线性回归模型中的权重可能和其与人工分的相关系数正负不一致，可能是由于特征之间的多重共线性不明确（unclear multicollinearity）和&#x2F;或权重较小导致的。</li><li>使用更大的训练集进行模型训练尚未被证明是有利的；大约1-2万个回答通常足以实现 TOEFL iBT 数据上的最佳模型性能。</li><li>评分模型<blockquote><img src="https://note.youdao.com/yws/api/personal/file/WEBe8bdfbf008095926fb10bbdb11158ebd?method=download&shareKey=d7a2b29e988f59a3c86bcc049cf6fd33" width="615px"><img src="https://note.youdao.com/yws/api/personal/file/WEB826535314fa7107a57c42094059601ce?method=download&shareKey=2698bc0877fb1ab709328489d9685bff" width="584px"><img src="https://note.youdao.com/yws/api/personal/file/WEBb46b7820f1a80f6954d20d0c4f73bd30?method=download&shareKey=53537b60c4c3b2902dfce5aec4198d47" width="614px"><table><thead><tr><th>数据集</th><th>说明</th><th>训练集总数据量</th><th>平均每个模型的训练集数据量</th><th>平均每个模型的测试集数据量</th></tr></thead><tbody><tr><td>main</td><td>随机选取、单人评分</td><td>464,664</td><td>77,444</td><td>36505</td></tr><tr><td>main*</td><td>与exemplar等数据量</td><td>12,398</td><td>2,066</td><td></td></tr><tr><td>exemplar</td><td>双人评一致，且由多名专家复审保证评分准确</td><td>12,390</td><td>2,065</td><td>689</td></tr></tbody></table><ul><li>评分特征：77个，覆盖发音和流利度、语用（词汇和语法），不评价内容维度。</li></ul><ul><li>当训练集足够大时，用不同训练集训练的系统在同一测试集上性能几乎相同。大数据量对于抵消训练集中存在的hard case的影响较为重要。虽然相较于随机构建的训练集，采用人人一致性较高的训练集，达到最佳性能所需的数据量更少，但是收集足够数量的人人一致性较高数据的成本可能更高。</li><li>相较于随机构建的测试集，在人人一致性较高的测试集上人机一致性较高(r &#x3D; 0.80 vs. r &#x3D; 0.66)。相较于在标签存在争议的数据上预测出错，在标签一致确认的数据上犯错，是更严重的有效性问题。因此，人人一致的测试集有助于评估评分系统的性能。</li><li>相较于线性回归，其它线性、非线性机器学习模型至多只有较微小的评分性能提升。</li><li>在同一数据集上训练的不同评分模型，或者在不同数据集上训练的同一模型，预测结果的相关性很高（r &#x3D; 0.97 &#x2F; 0.98）。模型组合策略可能不会特别有效。</li></ul></blockquote><ul><li>Loukina A, Zechner K, Bruno J, et al. Using exemplar responses for training and evaluating automated speech scoring systems[C]&#x2F;&#x2F;Proceedings of the thirteenth workshop on innovative use of NLP for building educational applications. 2018: 1-12.</li></ul></li><li>由于ETS将自动语音评分用于考试，所以其很重要的一个考量是自动评分的透明性和有效性。</li><li>评价指标：人机评分的一致性。包括各种统计指标：kappa、quadratically weighted kappa、Pearson相关系数、一致性比例等。</li></ul><h1 id="自动反馈">8. 自动反馈</h1><ul><li>提供自动反馈的商业语言学习软件<ul><li>SRI International’s EduSpeak：发音错误检测</li><li>NativeAccent：由Carnegie Speech开发，检测发音、韵律错误，通过音频（正确发音的音频）、文本（比如关于声道活动的文本描述）、图片（比如声道图）等形式提供反馈</li><li>English Discoveries：由ETS的子公司开发，根据单词的声学分数进行染色 <a href="http://englishdiscoveries.net/english-discoveries/">http://englishdiscoveries.net/english-discoveries/</a></li></ul></li><li>SpeechRater自动反馈系统<ul><li>特征选择标准：可靠性、特征的透明性和可理解性、帮助用户提升口语水平的可操作性、同一说话人在不同类型的测试任务上相对一致（相较于提供特定任务的反馈，可以提供特定说话人在所有任务上的综合反馈）、与人工分相关度较高、特征集合覆盖了各个维度。</li><li>SpeechRater报告中选用的特征<table><thead><tr><th>报告中使用的名称（SpeechRater特征名）</th><th>维度</th><th>分数报告中提供的描述</th></tr></thead><tbody><tr><td>语速（SpeakingRate）</td><td>流利度</td><td>语速表示每分钟说的词数。水平较高的说话人往往语速较快（注意：如果语速过快，别人可能很难听懂）。</td></tr><tr><td>连续语音（LengthOfRunWords）</td><td>流利度</td><td>连续语音表示不包含停顿、填充词（如um等）的语音片段的平均词数。水平较高的说话人往往可以在不停顿、不使用填充词的情况下说出更多的词。</td></tr><tr><td>停顿频率（SilenceRate1）</td><td>流利度</td><td>停顿频率表示您说话时停顿的频率。水平较高的说话人停顿频率较低。注意：停顿的位置也很重要，比如在句子末尾停顿比在表述一个想法的中间停顿要好。</td></tr><tr><td>重复（RepetitionRate）</td><td>流利度</td><td>重复频率表示您重复一个词或短语的频率，比如“I need to go to, to go to the library”。水平较高的说话人往往重复频率较小。</td></tr><tr><td>韵律（StressedSyllPercent）</td><td>发音</td><td>韵律表示音节是否被正确地重读。水平较高的说话人往往在适当的音节上明显地重读。</td></tr><tr><td>元音时长（VowelDuration）</td><td>发音</td><td>元音时长表示您的元音发音时长与母语人士的对比。水平较高的说话人的元音时长往往更接近母语人士。</td></tr><tr><td>词汇深度（AverageVocabularyFrequency）</td><td>词汇</td><td>词汇深度表示您的词汇范围。水平较高的说话人倾向于使用各种词。分数越高表示您使用的词越不常用和&#x2F;或越精确。</td></tr></tbody></table>*报告中使用的名称是为了便于用户理解</li><li>专家选择，心理测量验证<ul><li>在同一考试不同任务上的一致性：Cronbach’s alpha</li><li>与人工分的相关度：Pearson相关系数</li><li>冗余度：特征间的Pearson相关系数</li></ul></li><li>反馈报告设计<ul><li><p>由分数报告设计团队和测试开发团队等设计，并进行了两轮可用性研究，用于确定用户能够理解和解释所提供信息的程度，并收集反馈。</p></li><li><p>特征值：特征值的范围难以直接解释。两种方式：采用低-高、最小-最多等定性描述词；采用百分位数定量标记。可用性研究最终决定展示为特征值在一组参考值中的相对位置。注意：需要将不同的特征放在同一参考框架上，便于对比不同的特征。</p></li><li><p>特征值是否应该明确地对应于分数：是。参考组：对每一档分数范围，统计中间50%（四分位范围，inter-quartile ranges，IQR，即第25%-第75%） 的受试者的特征。由于不同分数段的特征间有重叠，用户可能难以理解，加入了“常见问题”进行解释。</p><img src="https://note.youdao.com/yws/api/personal/file/WEB3be7c98727fe465d376a71b4b73c599b?method=download&shareKey=fdcc53770f3bcb23a27f4c8448ee1fd6" width="440px"><ul><li>箭头：受试者的特征值在参考组特征值中的百分位数（应该对特征值进行了排序，顺序或逆序）。</li><li>重叠：由于人工分综合考虑了口语水平的各个维度，因此不同水平考生的单一特征值间有重叠。</li></ul></li><li><p>是否包含教学建议：是。由于无法根据每个学生的优劣势提供个性化的建议、特征可能有多种解释（比如停顿通常是由于流利度较差，但也可能是想法之间的简单停顿）、导致分数报告过长，因此，未根据分数报告中的特征提供相应的学习技巧，而是简单地提供了指向ETS备考资源的链接。</p></li><li><p>需要明确解释反馈报告的局限性。</p></li><li><p>示例<br><a href="https://note.youdao.com/yws/api/personal/file/WEB825b08e4d81b0b48f83b9514eceb1c73?method=download&shareKey=7dd4ae0dfb62e07c9cf6345931f13c93">图1</a><br><a href="https://note.youdao.com/yws/api/personal/file/WEB95191f582c107ac31b19e36f3a734d47?method=download&shareKey=a0f19b9335df9a314ccf130e2b9f00c4">图2</a><br><a href="https://note.youdao.com/yws/api/personal/file/WEBffb192f45435b3becc39ffb68edf8633?method=download&shareKey=f5ac7fb4075458222ff4db6c5dffed22">图3</a></p></li></ul></li></ul></li></ul><h1 id="口语对话系统SDS">9. 口语对话系统SDS</h1><ul><li><p>相较于独白式的自发口语，口语对话可以更全面地评价口语能力，比如轮流策略的使用、不同<font color="green"> registers </font> 的恰当使用、语义理解等，也更接近真实的语用场景。</p></li><li><p>系统结构：ASR -&gt; 口语理解（spoken language understanding, SLU） -&gt; 对话管理器（dialog manager, DM，决定SDS下一步怎么做，比如问用户另一个问题、给用户呈现信息等） -&gt; 语言生成（language generatin, LG） -&gt; 语音合成（TTS）。</p></li><li><p>HALEF框架：一个开源、模块化、基于云的口语对话系统框架，兼容多个W3C和开放行业标准。<a href="http://www.halef.org/">http://www.halef.org/</a></p><ul><li><p>HALEF 语音对话系统示意图</p><img src="https://note.youdao.com/yws/api/personal/file/WEBdf45da51c2aac5e2f524ad65ad485b5c?method=download&shareKey=7de7e638a91d12d335a87dab49cf444a" width="576px"></li><li><p>Ramanarayanan V, Suendermann-Oeft D, Lange P, et al. Assembling the jigsaw: How multiple open standards are synergistically combined in the HALEF multimodal dialog system[M]&#x2F;&#x2F;Multimodal Interaction with W3C Standards. Springer, Cham, 2017: 295-310.</p></li><li><p>其它语音或多模态对话系统</p><ul><li>学术实现：Olympus、Alex、Virtual Human Toolkit、OpenDial(<a href="http://www.opendial-toolkit.net/">http://www.opendial-toolkit.net</a>)</li><li>工业实现：Voxeo(<a href="https://evolution.voxeo.com/prophecy/licensing.jsp)%E3%80%81Alexa(https://developer.amazon.com/en-US/alexa)">https://evolution.voxeo.com/prophecy/licensing.jsp)、Alexa(https://developer.amazon.com/en-US/alexa)</a></li><li>缺点：其中较多实现使用特殊的架构、接口、编程语言，而很少关注语音、多模态信号处理的现有标准。</li></ul></li></ul></li><li><p>在TOEFL MOOC中提供了3个口语对话任务</p><ul><li>The Coffee Spot：与咖啡师（由口语对话系统播放）互动，从菜单中点一份食物和一份饮料，学习者被提示回答有关定制订单的不同问题（比如饮料的大小、订单是“在这里”还是“外带”）。考察：阅读简单文本（菜单）、理解和遵循书面英语说明、通过英语会话实现预定目标的能力。</li><li>The Group Project：与虚拟同学互动，讨论课堂项目。学习者需要邀请他们的虚拟同学与他们见面、重温他们准备好的幻灯片，并回答虚拟朋友提出的其他问题。除了“The Coffee Spot”中的能力外，还考察以适当的方式提出请求的能力。</li><li>The Job Interview：与虚拟的大学职业中心顾问进行模拟面试。被提示回答背景、资格等一系列实际工作面试中可能遇到的典型问题。</li><li>难度递增</li></ul></li><li><p>应用开发流程</p><ul><li><p>任务设计。比如，为了引发较长的语音以评价其流利度等，可以要求学习者提供建议（Do you mind if I asked you for some advice?），并提供详细的理由 (Why do you think so? Can you tell more?)。</p></li><li><p>应用发布。采用HALEF中的OpenVXML，它是一种基于流程图的软件设计工具，封装了语音对话系统所需的各种资源，比如语法模型或语言模型、用于播放的录音；它还使设计人员能够指定控制SDS功能的各种参数，比如语音活动超时阈值、打断设置、语法格式等。<br><a href="https://halef.readthedocs.io/en/latest/openvxml.html">https://halef.readthedocs.io/en/latest/openvxml.html</a></p></li><li><p>作为聊天应用程序众包部署（Amazon Mechanical Turk 众包平台）。使用聊天机器人版本的HALEF系统（称为HALEFbot）测试分支对话树，从真实用户处引出文本回应，便于快速且低成本地发现比如用户的意外响应、对话分支中的逻辑错误等，并快速完善。</p></li><li><p>对话优化。</p></li><li><p>语音化。采用之前收集的数据训练定制的语言模型。使用TTS将文本提示转换为音频。同理，也可以采用众包技术进行优化，比如更新语言模型。</p></li><li><p>录制会话。相较于TTS，人工录制的提示可以让语音体验更加身临其境和真实。</p></li><li><p>反馈开发。示例：</p><img src="https://note.youdao.com/yws/api/personal/file/WEBa50a972bbfdd32f921cbdfdf83fd7969?method=download&shareKey=118f0c2c235eb0f37f1cec080aa72df3" width="576px"></li><li><p>部署。监控系统的鲁棒性、是否存在崩溃或显著影响学习体验的问题。系统运行一段时间后，转写学生的响应以进一步优化ASR效果。分析是否存在相关响应未被分支对话适当处理，并对对话设计进行优化。</p></li></ul><p>*每一个步骤都是在上一步骤完成后或者上一系统达到预期效果后再进行优化升级的。</p></li><li><p>自动评分</p><ul><li>Ramanarayanan V, Lange P L, Evanini K, et al. Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human-Machine Spoken Dialog Interactions[C]&#x2F;&#x2F;INTERSPEECH. 2017: 1711-1715.</li></ul></li><li><p>挑战</p><ul><li>心理测量研究需要确定应该从对话任务中收集的观察的数量和种类。</li></ul></li></ul><h1 id="信度与效度">10. 信度与效度</h1><p>信度（reliability）与效度（validity）是教育测试中两个最核心的概念。信度通常指测试结果在不同测试场景下的可复制性以及测试分数在多大程度上反映了“真实分数”，效度通常指对分数的解释和基于分数做出的推论的意义。</p><h2 id="信度">10.1. 信度</h2><p>两种类型的信度：评分准确性（scoring accuracy）、测试准确性（assessment accuracy）。此外，还应考虑测试测量感兴趣的属性的有效性。</p><ul><li>本章涉及的额外的特征<table><thead><tr><th>特征</th><th>子维度</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>AverageASRConfidenceScore</td><td>发音</td><td>各单词的ASR置信度分的和&#x2F;单词数量</td><td>\</td></tr><tr><td>SilenceRate3</td><td>流利</td><td>停顿次数&#x2F;句子长度（排除首尾停顿）</td><td>\</td></tr><tr><td>GrammaticalAccuracyScore4</td><td>语法</td><td>语法表达与人工分满分（4）数据的相似度分</td><td>\</td></tr><tr><td>GrammaticalAccuracyMax</td><td>语法</td><td>语法表达相似度分最高的人工分</td><td>\</td></tr></tbody></table></li><li>评分准确性<ul><li>true score：随机选择评分员，期望的人工分</li><li>评分员常见现象<ul><li>漂移（随着时间的推移，倾向于更严厉或更宽松的评分）</li><li>光环效应（受到同一考生先前回答分配的分数的影响）</li><li>集中趋势（避免量表的极端）</li><li>疲劳</li></ul></li><li>根据经典测试理论，X&#x3D;T+e，其中X表示观察到的人工分，T表示未观察到的真实分数（true score），e表示未观察到的测量误差，e中所有元素的期望值均为0，T和e不相关。</li><li>测量方式<ul><li>最佳线性预测（BLP）</li><li>相较于预测为常量-期望E，均方误差减小的比例（proportional reduction of mean squared error，PRMSE），值越大表示预测越有效。</li><li>stepwise regression<ul><li>回归模型可以仅保留partial $R^2$ (逐步加入特征，模型$R^2$的增量) &gt;&#x3D;0.01的特征。</li></ul></li></ul></li></ul></li><li>测试准确性<ul><li><p>true score：随机选择一项测试，期望的评分</p></li><li><p>测量方式：generalized Cronbach’s coefficient alpha，测量内部一致性</p></li><li><p>实验结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEBd7750038b423f64b28adc76345ec636c?method=download&shareKey=f68ed1895db218852f1f614a76b276aa" width="414px"><p>*值越大表明其在不同的测试项目中更一致；较小的值可能与不同的测试项目关注不同的能力有关。</p></li></ul></li><li>结论<ul><li>相较于采用人工分，增加SpeechRater特征的PRMSE较小，因此，在人工分基础上，结合自动评分的有效性增量非常有限。并且，仅使用SpeechRater特征，PRMSE相对较小。</li><li>在人工分基础上，仅有少量特征对预测true score有明显的帮助：WordTypes、GrammaticalAccuracyMax、AverageASRConfidenceScore、SilenceRate3（该项研究仅使用28个特征，详见书）。特征间的相关性较高。未来工作：采用更少冗余的特征集。</li></ul></li></ul><h2 id="效度">10.2. 效度</h2><p>construct相关性和代表性、与人工评分的相关性、与独立的外部测量的相关性、跨任务和测试形式的分数泛化性、分数能多大程度上反映考生的真实水平、是否能基于自动评分进行适当的决策。</p><h2 id="其它">10.3. 其它</h2><ul><li>测试的真实性：语言学习任务在多大程度上反映了现实生活中可能遇到的语用场景。</li><li>对不同测试群体的公平性。</li></ul><h1 id="挑战">11. 挑战</h1><ul><li>非母语自发语音ASR<br>TOEFL iBT口语考试的非母语自发语音，最优的WER约为20%-25%。依据对人类转写员一致性的研究，TOEFL iBT 数据的WER下限可能在 15% 左右。同时也意味着非母语ASR训练集中噪声水平较高。另一方面，说话人之间存在差异，对于某些说话人，ASR的性能可能接近最佳，而对于其他说话人，例如 30% 的WER，会导致许多特征计算模块的输入出现严重失真。</li><li>评分特征<ul><li>开发新特征覆盖衡量语音水平的各个维度，特别是语用和结构维度。</li><li>ASR和用于特征提取的NLP组件的错误。</li><li>口头回答通常较短，减少了特征计算的证据基础。改进：同一说话人的多个回答拼接。</li></ul></li><li>过滤模型：如果测试的利害关系相当高，则可以预期应试者会发明新的、创造性的方法来欺骗自动评分系统，因此，对部署的自动评分系统进行持续监控，对于确保分数有效性至关重要。</li><li>持续监控与升级</li><li>测试设计：比如回答的长短、期望考察的维度、任务类型。</li></ul><h1 id="其它-1">12. 其它</h1><ul><li>使用自动语音评分技术提供非母语语音反馈的商业语言学习应用<ul><li><a href="https://elsaspeak.com/home">ELSA</a></li><li><a href="www.liulishuo.com/en/">Liulishuo</a></li><li><a href="www.duolingo.com">Duolingo</a></li><li><a href="www.speakingpal.com">SpeakingPal</a></li><li><a href="www.carnegiespeech.com">Carnegie Speech</a></li><li><a href="www.rosettastone.com">Rosetta Stone</a></li></ul></li><li>TOEFL iBT口语评分标准 <a href="https://www.ets.org/pdfs/toefl/toefl-ibt-speaking-rubrics.pdf">https://www.ets.org/pdfs/toefl/toefl-ibt-speaking-rubrics.pdf</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>易混淆发音</title>
      <link href="/blog/yu-yin/yu-yin-xue/yi-hun-yao-fa-yin/"/>
      <url>/blog/yu-yin/yu-yin-xue/yi-hun-yao-fa-yin/</url>
      
        <content type="html"><![CDATA[<ul><li><p>n-ŋ</p><ul><li>sin     sing</li><li>thin    thing</li><li>ran     rang</li></ul></li><li><p>eɪ-iː</p><ul><li>fate    feat</li><li>way     we</li><li>say     see</li></ul><p>区分eɪ和iː <a href="https://www.ximalaya.com/sound/69244789">https://www.ximalaya.com/sound/69244789</a></p><p>区分eɪ和ɪ <a href="https://www.ximalaya.com/sound/69243511">https://www.ximalaya.com/sound/69243511</a></p></li><li><p>v-w</p><ul><li>visit&#x2F;view      we</li><li>very            well</li><li>valid           wag</li><li>vast            want</li></ul></li><li><p>h-f</p><ul><li>have    five</li></ul></li><li><p>ʒ-ʃ</p><ul><li>vision      version</li><li>pleasure    fisher</li></ul></li><li><p>æ-aɪ</p><ul><li>have    five</li></ul></li><li><p>l-r</p></li><li><p>l-n</p><ul><li>light   night</li><li>let     net</li><li>low     no</li><li>like    narrow</li><li>laugh   nasty</li></ul></li><li><p>s-θ</p><ul><li>sin     thin</li><li>sink    think</li><li>sank    thank</li><li>mouse   mouth</li><li>pass    path</li></ul></li><li><p>ð-z</p><ul><li>that        zero</li></ul></li><li><p>æ-e</p><ul><li>bag     beg</li><li>mat     met</li><li>sad     said</li><li>bad     bed</li><li>land    lend    London</li><li>sand    send</li><li>pack    peck</li><li>pan     pen</li></ul></li><li><p>ɪ-iː</p><ul><li>ship    sheep</li><li>sin     seen</li><li>fill    feel</li><li>hill    heel</li><li>sick    seek</li><li>pitch   peach</li><li>slip    sleep</li><li>grin    green</li></ul><p>区分：<a href="https://www.bilibili.com/video/av498505663/">https://www.bilibili.com/video/av498505663/</a></p></li><li><p>p-pʊ</p><ul><li>please</li></ul></li><li><p>ʃ-ʃɪ</p><ul><li>shrill</li></ul></li><li><p>k-kə</p><ul><li>clean</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>英语常见发音现象</title>
      <link href="/blog/yu-yin/yu-yin-xue/ying-yu-fa-yin-chang-jian-fa-yin-xian-xiang/"/>
      <url>/blog/yu-yin/yu-yin-xue/ying-yu-fa-yin-chang-jian-fa-yin-xian-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="送气-gt-不送气（浊化）">1. 送气 -&gt; 不送气（浊化）</h1><p>重读音节中，&#x2F;s&#x2F;+清辅音&#x2F;t&#x2F; &#x2F;p&#x2F; &#x2F;k&#x2F; &#x2F;tr&#x2F;+元音，清辅音发音为对应的浊辅音&#x2F;d&#x2F; &#x2F;b&#x2F; &#x2F;g&#x2F; &#x2F;dr&#x2F;，如student、speak、sky、strict <br><br>讲解及示例见：<a href="https://www.yingyutu.com/aspirated">https://www.yingyutu.com/aspirated</a></p><h1 id="闪音">2. 闪音</h1><p>美式英语两个元音之间的&#x2F;t&#x2F;发为[ɾ]，注意不是&#x2F;d&#x2F;，如city、not at all <br><br>讲解及示例见：<a href="https://www.yingyutu.com/flap">https://www.yingyutu.com/flap</a></p><h1 id="无声除阻">3. 无声除阻</h1><p>爆破音是指发音器官在口腔中形成阻碍，然后气流冲破阻碍而发出的音，也被称为塞音。发这些音时有三个阶段：成阻（发音部位紧闭）、持阻（让气流积聚，形成压力）、除阻（除去气流的阻碍）。当一个塞音紧跟一个需成阻音时，这个塞音会变弱许多，不完全发出来。这是因为前者的除阻后跟后者的成阻，在快速语流中，这样不方便。所以前一个塞音并不释放气流，只做动作，时间上仍然保留原时间。</p><ul><li>爆破音（&#x2F;p, b, t, d, k, ɡ&#x2F;）+爆破音，如doctor, good boy</li><li>爆破音+破擦音（&#x2F;tʃ, dʒ, tr, dr&#x2F;），如picture, good job</li><li>爆破音+摩擦音（&#x2F;f, v, s, z, θ, ð, ʃ, ʒ, r, h&#x2F;），如eighth &#x2F;eɪ(t)θ&#x2F;、keep silent &#x2F;kiː(p)&#x2F;</li></ul><p>讲解及示例见：<a href="https://www.yingyutu.com/unreleased">https://www.yingyutu.com/unreleased</a></p><h1 id="喉塞音">4. 喉塞音</h1><p>喉塞音是一种由声门关闭引起的气流瞬时中断而成的塞音。英式英语和美式英语中，&#x2F;t&#x2F;发为&#x2F;ʔ&#x2F;，如button</p><blockquote><p>The &#x2F;t&#x2F; is pronounced as a glottal stop &#x2F;ʔ&#x2F; (the sound in the middle of the word ‘uh-oh’) when it is between a vowel, &#x2F;n&#x2F;, or &#x2F;r&#x2F; (including all r-controlled vowels) and followed by an &#x2F;n&#x2F;(including a syllabic &#x2F;n&#x2F;), &#x2F;m&#x2F;, or non-syllabic &#x2F;l&#x2F;.</p></blockquote><p>讲解及示例见：<a href="https://www.yingyutu.com/glottal">https://www.yingyutu.com/glottal</a></p><h1 id="连读">5. 连读</h1><p>linking &#x2F;r&#x2F;：英音中，单词末尾的&#x2F;r&#x2F;一般不发音，但当其后的单词以元音开头时，&#x2F;r&#x2F;发音且与元音连读。如for ever &#x2F;fəˈrevə&#x2F;，here are&#x2F;hɪərə&#x2F;</p><h1 id="同化">6. 同化</h1><p>连续语流，若发音较快、较随便，有些音受前后音的影响，变得与之相近或相同。同化后，发音步骤简化，更省力。</p><ul><li>融合同化<ul><li>&#x2F;t&#x2F;+&#x2F;j&#x2F;→&#x2F;tʃ&#x2F;，如Nice to meet you.</li><li>&#x2F;d&#x2F;+&#x2F;j&#x2F;→&#x2F;dʒ&#x2F;，如Did you do it?</li><li>&#x2F;s&#x2F;+&#x2F;j&#x2F;→&#x2F;ʃ&#x2F;, 如God bless you.</li><li>&#x2F;z&#x2F;+&#x2F;j&#x2F;→&#x2F;ʒ&#x2F;, 如As you wish.</li><li>&#x2F;g&#x2F;+&#x2F;j&#x2F;→&#x2F;dʒ&#x2F;，如I beg your pardon.</li></ul></li><li>前一个音受后一个音影响，发音部位变化<ul><li>&#x2F;t&#x2F;+&#x2F;p, b, m&#x2F;，&#x2F;t&#x2F;→&#x2F;p&#x2F;，如that boy &#x2F;ðæp bɔɪ&#x2F;</li><li>&#x2F;t&#x2F;+&#x2F;k, g&#x2F;，&#x2F;t&#x2F;→&#x2F;k&#x2F;，如that girl &#x2F;ðæk ɡɜːl&#x2F;</li><li>&#x2F;d&#x2F;+&#x2F;p, b, m&#x2F;，&#x2F;d&#x2F;→&#x2F;b&#x2F;，如good-bye</li><li>&#x2F;d&#x2F;+&#x2F;k, g&#x2F;，&#x2F;d&#x2F;→&#x2F;g&#x2F;，如good guy</li><li>&#x2F;n&#x2F;+&#x2F;p, b, m&#x2F;，&#x2F;n&#x2F;→&#x2F;m&#x2F;，如ten miles</li><li>&#x2F;n&#x2F;+&#x2F;k, g&#x2F;，&#x2F;n&#x2F;→&#x2F;ŋ&#x2F;，如in general</li><li>&#x2F;nt&#x2F;+&#x2F;p, b, m&#x2F;，&#x2F;nt&#x2F;→&#x2F;mp&#x2F;，如don’t mind</li><li>&#x2F;nt&#x2F;+&#x2F;k, g&#x2F;，&#x2F;nt&#x2F;→&#x2F;ŋk&#x2F;，如don’t go</li><li>&#x2F;nd&#x2F;+&#x2F;p, b, m&#x2F;，&#x2F;nd&#x2F;→&#x2F;mb&#x2F;，如second place</li><li>&#x2F;nd&#x2F;+&#x2F;k, g&#x2F;，&#x2F;nd&#x2F;→&#x2F;ŋg&#x2F;，如second class</li><li>&#x2F;s&#x2F;+&#x2F;ʃ, j&#x2F;，&#x2F;s&#x2F;→&#x2F;ʃ&#x2F;，如this ship, this year</li><li>&#x2F;z&#x2F;+&#x2F;ʃ, j&#x2F;，&#x2F;z&#x2F;→&#x2F;ʒ&#x2F;，如these years</li></ul></li><li>词末浊辅音+词首清辅音，词末浊辅音清化<ul><li>&#x2F;z&#x2F;→&#x2F;s&#x2F;，如has to, lose face</li><li>&#x2F;v&#x2F;→&#x2F;f&#x2F;，如of course, have to</li><li>&#x2F;ð&#x2F;→&#x2F;θ&#x2F;，如with pleasure</li></ul></li></ul><h1 id="加音">7. 加音</h1><p>前一单词以元音结尾，后一单词以元音开头时</p><ul><li>&#x2F;ə&#x2F;, &#x2F;ɪə&#x2F;, &#x2F;ɑː&#x2F;, &#x2F;ɔː&#x2F; +元音，加&#x2F;r&#x2F;，如China and India, the idea of, pa and ma, law and order</li><li>&#x2F;ɪ&#x2F;, &#x2F;iː&#x2F;, &#x2F;eɪ&#x2F;, &#x2F;aɪ&#x2F;, &#x2F;ɔɪ&#x2F; +元音，加&#x2F;j&#x2F;，如I am</li><li>&#x2F;uː&#x2F;, &#x2F;ʊ&#x2F;, &#x2F;əʊ&#x2F;, &#x2F;aʊ&#x2F; +元音，加&#x2F;w&#x2F;，如who is</li></ul><h1 id="省音">8. 省音</h1><ul><li>单词内正确省音，如：<ul><li>similar &#x2F;sɪmələ(r)&#x2F;，different &#x2F;‘dɪf(ə)r(ə)nt&#x2F;</li><li>成音节：单词最后一个音节为辅音+&#x2F;ə&#x2F;+&#x2F;l&#x2F;或&#x2F;n&#x2F;，如trouble &#x2F;ˈtrʌb(ə)l&#x2F;、season &#x2F;ˈsiːz(ə)n&#x2F;</li><li>&#x2F;t(ə)ri&#x2F;：单词末尾为tary、tery或tory时，如secretary &#x2F;ˈsekrət(ə)ri&#x2F;</li></ul></li><li>&#x2F;t&#x2F;、&#x2F;d&#x2F;前后为辅音（其后辅音非&#x2F;h&#x2F;）时省音（&#x2F;d&#x2F;后辅音为&#x2F;w&#x2F;, &#x2F;d&#x2F;, &#x2F;r&#x2F;或&#x2F;s&#x2F;时，不省音），如next day &#x2F;neks deɪ&#x2F;、you and me</li><li>h−dropping 常见于he, him, her, his, have, who，如get him, tell her</li><li>&#x2F;v&#x2F;后跟辅音，如a lot of time &#x2F;ə lɒt ə taɪm&#x2F;, give me that</li><li>词尾辅音+词首同一辅音，只读一次，如got to、want to、some more</li></ul><h1 id="缩读">9. 缩读</h1><p>如 I am&#x3D;I’m, is, are, not, have, has, had, will, shall</p><h1 id="弱读">10. 弱读</h1><p>元音常弱化为&#x2F;ə&#x2F;，如</p><table><thead><tr><th>单词</th><th>强读式</th><th>弱读式</th><th>示例</th></tr></thead><tbody><tr><td>as</td><td>&#x2F;æz&#x2F;</td><td>&#x2F;əz&#x2F;</td><td></td></tr><tr><td>and</td><td>&#x2F;ænd&#x2F;</td><td>&#x2F;ənd&#x2F; &#x2F;ən&#x2F; &#x2F;n&#x2F;</td><td>you and&#x2F;n&#x2F; me</td></tr></tbody></table><h1 id="重读">11. 重读</h1><ul><li>以下词类一般重读：名词、实义动词、形容词、副词等；</li><li>以下词类一般不重读：代词、冠词、介词、连词、情态动词、助动词等</li></ul><h1 id="升降调">12. 升降调</h1><ul><li><p>升调：一般和反义疑问句尾、选择疑问句&#x3D;&#x3D;第一个选项后&#x3D;&#x3D;</p><p>如：Is he tall &amp;nearrow; or short?</p></li><li><p>降调：选择疑问句、特殊疑问句、祈使句、感叹句、陈述句 句尾</p><p>如：Is he tall or short? &amp;searrow;</p><p>Don’t open the window. &amp;searrow;</p></li><li><p>长句语调：并列的词，and前升，and后降</p><p>如：I’ve decided to stay away from fried food &amp;nearrow; and soft drinks. &amp;searrow;</p></li></ul><h1 id="参考资料">13. 参考资料</h1><ul><li>同化、加音、弱读 王式仁. 英语标准发音教程[M]. 高等教育出版社, 2012</li><li>浊化、失去爆破、缩读 刘金龙, 高莉敏.《零基础从头开始学音标》[M]. 华东理工大学, 2018</li><li>省音 《Features of connected speech》 <a href="https://mandradey.wixsite.com/connectedspeech">https://mandradey.wixsite.com/connectedspeech</a></li><li><a href="https://tristone13th.netlify.app/2021/02/09/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E9%9F%B3%E6%8A%80%E5%B7%A7.html">https://tristone13th.netlify.app/2021/02/09/英语语音技巧.html</a></li><li>重读、升降调 <a href="https://mp.weixin.qq.com/s?__biz=MzIyNDI3MTIxNg==&mid=2651145729&idx=1&sn=daf3381b9b3f9af2c8f95e33f2978527&chksm=f3e0681ec497e1087ac8809d96df38293203ef0ed91552da594e67a1aed85b4e7771ed37cb35&token=341174366&lang=zh_CN#rd">讯飞易听说 广东省梅州市高考英语听说考试解读及备考策略分享</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeNet代码</title>
      <link href="/blog/yu-yin/gong-ju-bao/wenet/dai-ma/"/>
      <url>/blog/yu-yin/gong-ju-bao/wenet/dai-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="数据预处理">1. 数据预处理</h1><ul><li>compute_cmvn_stats：生成global_cmvn<ul><li>mean_stat：训练集上各维特征的总和</li><li>var_stat：训练集上各维特征的平方和</li><li>frame_num：训练集总帧数</li></ul></li></ul><h1 id="IterableDataset-Processor">2. IterableDataset - Processor</h1><ul><li>根据输入list生成IterableDataset<ul><li>数据类型<ul><li>raw：json列表，存储key、wav(、start、end)、txt</li><li>shard：压缩包列表。随机：压缩包间随机，压缩包内部顺序遍历</li></ul></li><li>训练集按rank、worker_id划分数据；CV集各卡均用全集</li></ul></li><li>tokenize<ul><li><font color="green">non_lang_syms：用[……]、&lt;……&gt;、{……}分割文本（如’12[12]34&lt;34&gt;56{56}{78}’分割为[‘12’, ‘[12]’, ‘34’, ‘&lt;34&gt;’, ‘56’, ‘{56}’, ‘’, ‘{78}’, ‘’]），转大写。若token在non_lang_syms中，直接使用；否则BPE&#x2F;按空格分割&#x2F;按字符分割</font></li><li>BPE<pre class="line-numbers language-none"><code class="language-none">import sentencepiecesp &#x3D; sentencepiece.SentencePieceProcessor()sp.load(bpe_model)sp.encode_as_pieces(word)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>symbol_table：token转id</li></ul></li><li>filter：过滤过短或过长的样本<ul><li>max_length(10240)、min_length(10)：帧数，帧移10ms</li><li>token_max_length(200)、token_min_length(1)</li><li>min_output_input_ratio(0.0005)、max_output_input_ratio(1)</li></ul></li><li>resample</li><li>speed_perturb：各样本随机变速[0.9, 1.0, 1.1]。默认不使用</li><li><font color="green">提取fbank&#x2F;mfcc</font>：<font color="red">音频要求16bit</font><pre class="line-numbers language-none"><code class="language-none">torchaudio.compliance.kaldi as kaldifeat &#x3D; kaldi.fbank(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, energy_floor&#x3D;0.0, sample_frequency&#x3D;sample_rate)feat &#x3D; kaldi.mfcc(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, num_ceps&#x3D;num_ceps, high_freq&#x3D;high_freq, low_freq&#x3D;low_freq, sample_frequency&#x3D;sample_rate)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li>spec_aug：频谱mask，num_t_mask、num_f_mask、max_t、max_f，max_w未使用。默认使用</li><li>spec_sub：频谱替换，num_t_sub、max_t，随机替换为该时刻前的特征。默认不使用</li><li>shuffle：<font color="red">局部shuffle，shuffle_size默认10000</font>。默认使用</li><li>sort：局部按帧数排序，便于帧数相近的样本放到同一batch，sort_size&#x3D;500。默认使用</li><li>batch<ul><li>静态batch：除最后一个batch，batch size固定</li><li>动态batch：按max_frames_in_batch（含padding）拼batch</li></ul></li><li>padding：batch内样本按帧数降序排列，特征补0，label补-1</li><li>torchaudio<pre class="line-numbers language-none"><code class="language-none">torchaudio.backend.sox_io_backend.info(wav_file).sample_ratetorchaudio.load(wav_file)torchaudio.backend.sox_io_backend.load(filepath&#x3D;wav_file, num_frames&#x3D;end_frame - start_frame, frame_offset&#x3D;start_frame)torchaudio.transforms.Resample(orig_freq&#x3D;sample_rate, new_freq&#x3D;resample_rate)(waveform)wav, _ &#x3D; torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, [[&#39;speed&#39;, str(speed)], [&#39;rate&#39;, str(sample_rate)]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="模型结构">3. 模型结构</h1><h2 id="encoder">3.1. encoder</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    输入("输入：x: [B, T, D], <br> mask_pad: [B, 1, T]，有效帧对应1，拼batch时补的帧对应0")    输入 --> CMVN    CMVN --> 下采样    subgraph 下采样        不下采样        4倍下采样        6倍下采样        8倍下采样        subgraph 不下采样            direction TB            Linear1("Linear(input_size, output_size)")            Linear1 --> LayerNorm1("LayerNorm") --> Dropout1("Dropout")        end        subgraph 4倍下采样            direction TB            Conv2d1("Conv2d(1, output_size, 3, 2)")            Conv2d2("Conv2d(output_size, output_size, 3, 2)")            Linear2("Linear(output_size * (((input_size - 1) // 2 - 1) // 2), output_size)")            mask_pad1("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2]")            Conv2d1 --> ReLU1(ReLU) --> Conv2d2 --> ReLU2(ReLU) --> Linear2        end        subgraph 6倍下采样            direction TB            Conv2d3("Conv2d(1, output_size, 3, 2)")            Conv2d4("Conv2d(output_size, output_size, 5, 3)")            Linear3("Linear( , output_size)")            mask_pad2("mask_pad = mask_pad[:, :, :-2:2][:, :, :-4:3]")            Conv2d3 --> ReLU3(ReLU) --> Conv2d4 --> ReLU4(ReLU) --> Linear3        end        subgraph 8倍下采样            direction TB            Conv2d5("Conv2d(1, output_size, 3, 2)")            Conv2d6("Conv2d(output_size, output_size, 3, 2)")            Conv2d7("Conv2d(output_size, output_size, 3, 2)")            Linear4("Linear( , output_size)")            mask_pad3("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2][:, :, :-2:2]")            Conv2d5 --> ReLU5(ReLU) --> Conv2d6 --> ReLU6(ReLU) --> Conv2d7 --> ReLU7(ReLU) --> Linear4        end    end    下采样 --> 位置embedding    subgraph 位置embedding        pos_emd("pos_emb(pos, 2i) = sin(pos/(10000^(2i/dim))) <br> pos_emb(pos, 2i+1) = cos(pos/(10000^(2i/dim))) <br> 拼batch计算时，offset小于0的位置采用pos_emb[0]"):::font_green        位置embedding1("x = x * math.sqrt(dim) + pos_emb"):::font_green        相对位置embedding("x *= math.sqrt(dim)"):::font_green        无位置embedding("x不变，pos_emb全0")        Dropout2("dropout(x), dropout(pos_emb)")        tmp1(" ") --> pos_emd        pos_emd -- 位置embedding --> 位置embedding1 --> Dropout2        pos_emd -- 相对位置embedding --> 相对位置embedding --> Dropout2        tmp1 -- 无位置embedding --> 无位置embedding --> Dropout2    end    位置embedding --> 计算chunk_mask    计算chunk_mask --> transformer*num_blocks -->tmp2    计算chunk_mask --> conformer*num_blocks -->tmp2    tmp2(" ") -- normalize_before -->LayerNorm2("LayerNorm")    subgraph 计算chunk_mask        use_dynamic_chunk{"use_dynamic_chunk"} -- Yes --> decoding_chunk_size{"decoding_chunk_size"}        decoding_chunk_size -- 小于0 --> tmp18("不分chunk，历史、未来全部可见")        decoding_chunk_size -- 大于0 --> 采用固定的chunk_size --> num_decoding_left_chunks{"num_decoding_left_chunks"}        decoding_chunk_size -- 等于0 --> tmp17("随机动态chunk_size，帧数取值范围为[1, 25]或全部可见（概率50%），支持use_dynamic_left_chunk") --> num_decoding_left_chunks        use_dynamic_chunk -- No --> static_chunk_size{"static_chunk_size"}        static_chunk_size -- 小于等于0 --> tmp18        static_chunk_size -- 大于0 --> 采用固定的chunk_size        num_decoding_left_chunks -- 小于0 --> tmp19("历史全部可见，当前chunk可见，未来chunk不可见")        num_decoding_left_chunks -- 大于等于0 --> 仅可见指定的历史chunk数    end    subgraph transformer*num_blocks        subgraph transformer_attention            tmp3(" ") --> normalize_before1{"normalize_before"}            normalize_before1 -- Yes --> LayerNorm3("LayerNorm") --> MultiHeadedAttention1("MultiHeadedAttention"):::font_red            normalize_before1 -- No --> MultiHeadedAttention1            MultiHeadedAttention1 --> concat_after1{"concat_after"}            concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp4(" ")            concat_after1 -- No --> Dropout3("Dropout") --> tmp4            tmp3 -- + --> tmp4            tmp4 --> normalize_before2{"normalize_before"}            normalize_before2 -- Yes --> tmp5(" ")            normalize_before2 -- No --> LayerNorm4("LayerNorm") --> tmp5        end        subgraph transformer_FeedForward            tmp5 --> normalize_before3{"normalize_before"}            normalize_before3 -- Yes --> LayerNorm5("LayerNorm") --> PositionwiseFeedForward1("PositionwiseFeedForward: <br> w2(dropout(activation(w1*x+b1)))+b2 <br> w1:[input, hidden] w2:[hidden, input]"):::font_red            normalize_before3 -- No --> PositionwiseFeedForward1            PositionwiseFeedForward1 --> Dropout4("Dropout")            tmp5 -- + --> Dropout4            Dropout4 --> normalize_before4{"normalize_before"}            normalize_before4 -- Yes --> tmp6(" ")            normalize_before4 -- No --> LayerNorm6("LayerNorm") --> tmp6        end    end    subgraph conformer*num_blocks        subgraph conformer_macaron            tmp7(" ") --> macaron{"macaron"}            macaron -- Yes --> normalize_before5{"normalize_before"}            macaron -- No --> tmp8(" ")            normalize_before5 -- Yes --> LayerNorm7("LayerNorm") --> PositionwiseFeedForward2("PositionwiseFeedForward"):::font_red            normalize_before5 -- No --> PositionwiseFeedForward2            PositionwiseFeedForward2 --> Dropout5("Dropout") --> scale("*0.5")            tmp7 -- + --> scale            scale --> normalize_before6{"normalize_before"}            normalize_before6 -- Yes --> tmp8            normalize_before6 -- No --> LayerNorm8("LayerNorm") --> tmp8        end        subgraph conformer_attention            tmp8 --> normalize_before7{"normalize_before"}            normalize_before7 -- Yes --> LayerNorm9("LayerNorm") --> tmp9(" ")            normalize_before7 -- No --> tmp9            subgraph Attention                direction TB                计算qkv("q、k、v均采用Linear计算 <br> 更新att_cache: 历史[k, v]，(1, head, chunk_size * num_decoding_left_chunks + T, d_k * 2)"):::font_red                计算attention系数("计算attention系数：采用了chunk_mask <br> Dropout"):::font_red                attention("scaled dot product self-attention")                Linear5("Linear(output_size, output_size)")                Rel("pos_bias_u、pos_bias_v: [head, d_k]，xavier_uniform_初始化，可训练 <br> softmax{[(q + pos_bias_u)k + (q + pos_bias_v)(W*pos_emb)]/math.sqrt(d_k)}v <br> rel_shift: 未采用"):::font_red                tmp9 --> 计算qkv -- MultiHeadedAttention --> 计算attention系数 --> attention --> Linear5                计算qkv -- RelPositionMultiHeadedAttention --> Rel --> Linear5            end            Linear5 --> concat_after3{"concat_after"}            concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp10(" ")            concat_after3 -- No --> Dropout6("Dropout") --> tmp10            tmp8 -- + --> tmp10            tmp10 --> normalize_before8{"normalize_before"}            normalize_before8 -- Yes --> tmp11(" ")            normalize_before8 -- No --> LayerNorm10("LayerNorm") --> tmp11        end        subgraph conformer_conv            tmp11 --> conv{"conv"}            conv -- Yes --> normalize_before9{"normalize_before"}            normalize_before9 -- Yes --> LayerNorm11("LayerNorm") --> tmp12(" ")            normalize_before9 -- No --> tmp12            tmp12 --> mask_pad4("mask_pad的帧均置0"):::font_red --> causal{"causal"}            causal -- Yes -->            padding("第一次计算时左侧填充(kernel_size-1)帧0，<br> 后续从cache中取缓存的历史有效输入"):::font_red --> pointwise_conv1            causal -- No --> pointwise_conv1            pointwise_conv1("pointwise_conv: 1D卷积，channel扩充为2倍"):::font_red --> GLU("GLU，channel减半"):::font_red --> depthwise_conv            click GLU "https://pytorch.org/docs/stable/generated/torch.nn.functional.glu.html"            depthwise_conv("nn.Conv1d(channels, channels, kernel_size, stride=1, padding=padding, groups=channels, bias=bias) <br> 若非causal，左右填充(kernel_size - 1) // 2帧0"):::font_red            depthwise_conv --> BatchNorm1d/LayerNorm --> activation("activation，如SiLU/swish")            click activation "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"            activation --> pointwise_conv2("pointwise_conv: 1D卷积"):::font_red --> mask_pad5("mask_pad的帧均置0"):::font_red -->Dropout7("Dropout")            tmp11 -- + --> Dropout7            Dropout7 --> normalize_before10{"normalize_before"}            normalize_before10 -- Yes --> tmp13(" ")            normalize_before10 -- No --> LayerNorm12("LayerNorm") --> tmp13            conv -- No --> tmp13        end        subgraph conformer_FeedForward            tmp13 --> normalize_before11{"normalize_before"}            normalize_before11 -- Yes --> LayerNorm13("LayerNorm") --> PositionwiseFeedForward3("PositionwiseFeedForward"):::font_red            normalize_before11 -- No --> PositionwiseFeedForward3            PositionwiseFeedForward3 --> Dropout8("Dropout") --> macaron2{"macaron"}            macaron2 -- Yes --> scale2(*0.5) --> tmp14(" ")            macaron2 -- No --> tmp14(" ")            tmp13 -- + --> tmp14            tmp14 --> normalize_before12{"normalize_before"}            normalize_before12 -- Yes --> tmp15(" ")            normalize_before12 -- No --> LayerNorm14("LayerNorm") --> tmp15            tmp15--> conv2{"conv"}            conv2 -- Yes --> LayerNorm15("LayerNorm") --> tmp16(" ")            conv2 -- No --> tmp16(" ")        end    end</div><ul><li>load_cmvn：计算均值、方差倒数。支持json格式、kaldi文本格式输入</li><li>下采样：2D卷积下采样至1&#x2F;4或1&#x2F;6或1&#x2F;8，感受野分别为7、11、15帧（延迟分别为6、10、14) <br> <font color="green">存在的问题：x_mask。如下采样至1&#x2F;4时，取<code>x_mask[:, :, :-2:2][:, :, :-2:2]</code>，第1个点中心时刻为第3帧，感受野为0-6帧，第一个点的mask应取第6帧</font></li><li>forward_chunk：输入(chunk_size - 1) * subsample_rate + subsample.right_context + 1帧有效数据，输出chunk_size帧，并更新att_cache、cnn_cache</li><li>forward_chunk_by_chunk：输入有效帧，模拟逐chunk流式计算</li></ul><h2 id="decoder">3.2. decoder</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    subgraph TransformerDecoder        tmp(" ") -- train --> 输入1("输入label、label长度") --> tgt_mask("mask padding、未来输出") --> embedding --> +位置embedding --> transformer*num_blocks        tmp -- decode --> 输入2("自回归：输入上一时刻解码结果") --> embedding        transformer*num_blocks --> normalize_before1{"normalize_before"}        normalize_before1 -- Yes --> LayerNorm1("LayerNorm") --> tmp1(" ")        normalize_before1 -- No --> tmp1        tmp1 --> output{"use_output_layer"}        output -- Yes --> Linear1("Linear(attention_dim, vocab_size)") --> train_decode{"train/decode"}        train_decode -- train --> tmp2(" ")        train_decode -- decode --> log_softmax --> tmp2        output -- No --> tmp2        subgraph transformer*num_blocks            direction TB            subgraph self_attention                direction TB                tmp4(" ") --> normalize_before2{"normalize_before"}                normalize_before2 -- Yes --> LayerNorm2("LayerNorm") --> tmp5(" ")                normalize_before2 -- No --> tmp5                tmp5 --> MultiHeadedAttention1("MultiHeadedAttention: q: 上一时刻输出，k/v: 历史输出，mask未来输出"):::font_red                MultiHeadedAttention1 --> concat_after1{"concat_after"}                concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp6(" ")                concat_after1 -- No --> Dropout3("Dropout") --> tmp6                tmp4 -- + --> tmp6                tmp6 --> normalize_before3{"normalize_before"}                normalize_before3 -- Yes --> tmp7(" ")                normalize_before3 -- No --> LayerNorm3("LayerNorm") --> tmp7            end            subgraph inter_attention                direction TB                tmp7 --> normalize_before4{"normalize_before"}                normalize_before4 -- Yes --> LayerNorm4("LayerNorm") --> tmp8(" ")                normalize_before4 -- No --> tmp8                tmp8 --> MultiHeadedAttention2("MultiHeadedAttention: q: decoder隐层表示，k/v: encoder输出，mask未来输出"):::font_red                MultiHeadedAttention2 --> concat_after3{"concat_after"}                concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp9(" ")                concat_after3 -- No --> Dropout4("Dropout") --> tmp9                tmp7 -- + --> tmp9                tmp9 --> normalize_before5{"normalize_before"}                normalize_before5 -- Yes --> tmp10(" ")                normalize_before5 -- No --> LayerNorm5("LayerNorm") --> tmp10            end            subgraph FeedForward                tmp10 --> normalize_before6{"normalize_before"}                normalize_before6 -- Yes --> LayerNorm6("LayerNorm") --> PositionwiseFeedForward:::font_red                normalize_before6 -- No --> PositionwiseFeedForward                PositionwiseFeedForward --> Dropout5("Dropout")                tmp10 -- + --> Dropout5                Dropout5 --> normalize_before7{"normalize_before"}                normalize_before7 -- Yes --> tmp11(" ")                normalize_before7 -- No --> LayerNorm7("LayerNorm") --> tmp11            end            decode("decode：缓存每一时刻decoder各block的输出；输入tgt递增存储每一时刻的输出")        end    end    subgraph BiTransformerDecoder        tmp3(" ") -- train/rescore --> 输出left_decoder与right_decoder结果:::font_red        tmp3 -- decode --> 采用left_decoder:::font_red    end</div><h2 id="Transducer">3.3. Transducer</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    input("输入上一时刻预测值") --> Embedding1("Embedding(vocab_size, embed_size)") --> Dropout1("Dropout")    Dropout1 -- RNNPredictor --> RNNPredictor    Dropout1 -- EmbeddingPredictor --> EmbeddingPredictor    Dropout1 -- ConvPredictor --> ConvPredictor    subgraph RNNPredictor        RNN("RNN/LSTM/GRU，记录历史h、c，初始为全0"):::font_red --> linear1("Linear(hidden_size, output_size)") --> tmp1    end    subgraph EmbeddingPredictor        direction TB        input1("拼接定长的历史输入 <br> input(B,L,1,C,E)，B: batch size, L: seq_len, C: context_size, E: embed_size"):::font_red --> attention:::font_red        subgraph attention            input1 --> dot            W("W(H,C,E)，H: num_heads") --> dot            dot("broadcast dot product, (B,L,H,C,E)"):::font_red --> sum1("sum, (B,L,H,C)") --> unsqueeze("unsqueeze, (B,L,H,1,C)") --> matmul            input1 --> matmul("matmul，(B,L,H,1,E)") --> sum2("squeeze, sum，(B,L,E)") --> scale("scale，/(H*C)")            click dot "https://arxiv.org/pdf/2109.07513.pdf"        end        scale --> proj("Linear(embed_size, embed_size)") --> LayerNorm --> swish --> tmp1    end    subgraph ConvPredictor        input2("拼接定长的历史输入，初始输入blank") --> conv --> LayerNorm2("LayerNorm")--> activation --> tmp1    end    subgraph join        input3("输入：encoder_output") --> prejoin_linear1{"prejoin_linear"}        tmp1("predictor output") --> prejoin_linear2{"prejoin_linear"}        prejoin_linear1 -- Yes --> prejoin_linear3("Linear(enc_output_size, join_dim)") --> tmp2(" ")        prejoin_linear1 -- No --> tmp2        prejoin_linear2 -- Yes --> prejoin_linear4("Linear(pred_output_size, join_dim)") --> tmp3(" ")        prejoin_linear2 -- No --> tmp3        tmp2 --> tmp4("+"):::font_red        tmp3 --> tmp4        tmp4 --> postjoin_linear1{"postjoin_linear"}        postjoin_linear1 -- Yes --> postjoin_linear2("Linear(join_dim, join_dim)") --> tmp5(" ")        postjoin_linear1 -- No --> tmp5        tmp5 --> activation2("activation") --> linear2("Linear(join_dim, vocab_size)")    end</div><h2 id="CTC-Attention-x2F-Transducer">3.4. CTC-Attention&#x2F;Transducer</h2><ul><li>CTC：输入encoder_output –&gt; Linear(“Linear(, vocab_size)”)</li><li>attention：KLDivLoss + label smoothing</li></ul><p>$$ L_{\mathrm{ATT}}(x, y)&#x3D;(1-\alpha) L_{\mathrm{ATT-L2R}}(x, y)+\alpha {L}_{\mathrm{ATT-R2L}}(x, y) $$</p><p>$$ L_{\mathrm{CTC-ATT}}(x, y)&#x3D;\lambda L_{\mathrm{CTC}}(x, y)+(1-\lambda) {L}_{\mathrm{ATT}}(x, y) $$</p><p>$$ L_{\mathrm{CTC-ATT-RNNT}}(x, y)&#x3D;\gamma L_{\mathrm{RNNT}}(x, y)+\beta L_{\mathrm{ATT}}(x, y)+\lambda L_{\mathrm{CTC}}(x, y) $$</p><h2 id="解码">3.5. 解码</h2><p>目前支持四种解码算法：</p><ul><li><p>CTC greedy beam search，帧级别输出，解码过程不合并前缀，最终n-best上进行ctc序列处理。</p></li><li><p>CTC prefix beam search：帧级别解码，合并相同的ctc序列前缀。</p></li><li><p>Attention decoder beam search：基于cross-attention的label级别解码。</p></li><li><p>CTC + attention rescoring：第一遍采用CTC prefix beam search，该结果可作为流式结果实时返回。将CTC decoder的n-best结果，通过attention decoder进行teacher forcing rescoring（指定各时刻输出，仅提取分数），根据得分重新排序。</p><p>score &#x3D; 反向attention decoder分 * w_r + 正向attention decoder分 * (1-w_r) + CTC分 * w_ctc</p></li></ul><h3 id="Transducer解码">3.5.1. Transducer解码</h3><ul><li><p>greedy search</p><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    输入一帧encoder_out --> nblk{"上一输出是否为blank"}    nblk -- Yes --> predictor.forward_step --> decode    nblk -- No --> 采用之前计算的pred_out --> decode    decode --> blank{"是否为blank"} -- No --> 输出 --> per_frame_noblk{"单帧encoder_out解码出的非blank token>=per_frame_noblk"}:::font_red    per_frame_noblk -- No --> predictor.forward_step    per_frame_noblk -- Yes --> 输入一帧encoder_out    blank -- Yes --> 输入一帧encoder_out</div></li><li><p>prefix beam search</p><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    RNNT("RNNT: 仅支持一帧encoder_out输出一个token"):::font_red --> shallow_fusion("后验概率加权求和")    CTC --> shallow_fusion --> topk("取topk，得到N * N个候选") --> prefix("解码结果（不含blank）相同的合并后验概率") --> 取topk</div></li><li><p>RNNT + attention rescoring</p><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    decode1("CTC/RNNT prefix beam search") --> torchaudio.functional.rnnt_loss:::font_green    decode1 --> attention_rescore    decode1 -- w_c --> score加权求和    torchaudio.functional.rnnt_loss -- w_t --> score加权求和    attention_rescore -- w_a --> score加权求和</div></li></ul><h2 id="模型训练">3.6. 模型训练</h2><ul><li><p>LRScheduler<br>$$<br>l_{r}&#x3D;\begin{cases}<br>l_{r} * \frac{\text { step }}{\text { warmup_step }} &amp; , \text { step } \leqslant \text { warmup_step } \<br>l_{r} * \sqrt{\frac{\text { warmup_step }}{\text { step }}} &amp; , \text { step }&gt;\text { warmup_step}<br>\end{cases}<br>$$</p></li><li><p>fp16梯度同步</p><pre class="line-numbers language-none"><code class="language-none">model.cuda()model &#x3D; torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters&#x3D;True)from torch.distributed.algorithms.ddp_comm_hooks import (default as comm_hooks,)model.register_comm_hook(state&#x3D;None, hook&#x3D;comm_hooks.fp16_compress_hook)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><font color="green"><a href="https://pytorch.org/docs/stable/notes/amp_examples.html">混合精度训练</a></font></p></li><li><p>梯度累积</p><pre class="line-numbers language-none"><code class="language-none">from contextlib import nullcontextif is_distributed and batch_idx % accum_grad !&#x3D; 0:    context &#x3D; model.no_syncelse:    context &#x3D; nullcontextwith context():    loss_dict &#x3D; model(feats, feats_lengths, target, target_lengths)    loss &#x3D; loss_dict[&#39;loss&#39;] &#x2F; accum_grad    loss.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>梯度裁剪</p><pre class="line-numbers language-none"><code class="language-none">from torch.nn.utils import clip_grad_norm_if use_amp:    scaler.unscale_(optimizer)    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)    # We don&#39;t check grad here since that if the gradient    # has inf&#x2F;nan values, scaler.step will skip optimizer.step().    scaler.step(optimizer)    scaler.update()else:    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)    if torch.isfinite(grad_norm):        optimizer.step()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>average model：取cv_loss最小或最后的 n个epoch的模型，取参数均值</p></li></ul><h2 id="参考资源">3.7. 参考资源</h2><ul><li><font color="green">相对position embedding：<a href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a> </font></li><li><font color="green">Embedding predictor：<a href="https://arxiv.org/pdf/2109.07513.pdf">https://arxiv.org/pdf/2109.07513.pdf</a> </font></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的语音评分</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="2018-ETS-acoustics-transcription">1. 2018 ETS acoustics + transcription</h1><ul><li>作者：ETS</li><li>发表信息：ICASSP 2018</li><li><font color="red">创新点：提示无关的神经网络评分模型（BD-LSTM + attention），输入识别文本word embedding、各单词的后验概率及声学特征，输出评分</font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd31cc498c578d02c7497165e2e58a513?method=download&shareKey=3bd314af1bfa5a11c60bfded4a5d2cd2" alt="系统结构"></p><ul><li>声学模型：DNN-HMM，训练集：819h non-native自发语音</li><li>评分模型输入<ul><li>lexical模型：识别文本word embedding序列。采用预训练的Glove模型，OOV采用全0向量，300维，训练评分模型时fine-tune</li><li>声学模型：每个词的声学模型后验概率、时长、pitch均值、intensity均值</li></ul></li><li>评分模型<ul><li>1D CNN<ul><li>参考[14]，采用3种尺寸的卷积核$\left(conv_{size}-1, conv_{size}, conv_{size}+1\right)$，用于覆盖不同的感受野。各$conv_{n}$个卷积核</li><li>input -&gt; dropout $dp_{CNN}1$ -&gt; 卷积层 -&gt; max pooling（沿时间轴） -&gt; dropout $dp_{CNN}2$</li></ul></li><li>BD-LSTM<ul><li>input -&gt; dropout $dp_{RNN}1$ -&gt; BD-LSTM -&gt; 两个方向的隐层状态拼接 -&gt; dropout $dp_{RNN}2$</li></ul></li><li>BD-LSTM + attention<ul><li>input -&gt; dropout -&gt; BD-LSTM -&gt; attention -&gt; dropout</li></ul></li><li>超参tuning：采用Hyperopt Python包实现的Tree Parzen Estimation (TPE)方法[23]。$conv_{size}&#x3D;4, conv_{n}&#x3D;100, dp_{CNN}1&#x3D;dp_{RNN}1&#x3D;0.25, dp_{CNN}2&#x3D;dp_{RNN}2&#x3D;0.5, LSTM_{dim}^{lex}&#x3D;128, LSTM_{dim}^{ac}&#x3D;32$</li></ul></li></ul><h2 id="评价">1.2. 评价</h2><ul><li><p>数据集：训练集 2930，开发集 731，测试集 1827。4分制。</p></li><li><p>传统模型（基线）</p><ul><li><p>随机森林、GBT（Gradient Boosting Tree）、SVR（Support Vector Regression）。其中，GBT模型人-机评分相关度最高。</p></li><li><p>评分特征</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2b3d68c3d17278fcc388c60f44fbca7b?method=download&shareKey=93dba1172f8282c23aea663f0988cfed" alt="评分特征"></p><table><thead><tr><th>类别</th><th>特征示例</th></tr></thead><tbody><tr><td>流利度</td><td>单词数&#x2F;秒、单词数&#x2F;段、静音段个数、静音段平均时长、长停顿（&gt;0.5s）频率、有声停顿（uh、um）个数</td></tr><tr><td>韵律、语调、重音</td><td>韵律事件（prominences and boundary tones）的占比、之间的平均距离、距离的平均差，元音、辅音、音节时长的占比、标准差、Pairwise Variability Index</td></tr><tr><td>发音</td><td>native AM 强制对齐计算likelihood、ASR词级置信度均值、在native语料上统计各元音的时长均值，计算测试数据元音时长与参考值的差值的均值</td></tr><tr><td>语法</td><td></td></tr><tr><td>用词</td><td>多样性、复杂度</td></tr></tbody></table></li></ul></li><li><p>实验结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEB80c4a6aa4b8e87255eb450053d7e42d9?method=download&shareKey=19bfa2868abd6d958f9a91870d239584" width="50%"><p>*相较于传统评分模型采用n-gram提取评分特征，word embedding可提供更丰富的信息</p></li><li><p>展望：可解释性、更多声学特征、其它attention机制</p></li></ul><h2 id="其它">1.3. 其它</h2><ul><li>Chen L, Tao J, Ghaffarzadegan S, et al. End-to-end neural network based automated speech scoring[C]&#x2F;&#x2F;2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018: 6234-6238.</li><li>Yu Z, Ramanarayanan V, Suendermann-Oeft D, et al. Using bidirectional LSTM recurrent neural networks to learn high-level abstractions of sequential features for automated scoring of non-native spontaneous speech[C]&#x2F;&#x2F;2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015: 338-345.</li><li>Taghipour K, Ng H T. A neural approach to automated essay scoring[C]&#x2F;&#x2F;Proceedings of the 2016 conference on empirical methods in natural language processing. 2016: 1882-1891. CNN（提取局部上下文信息）+ RNN（提取长时信息）+ mean over time回归（利用全文信息）</li></ul><h1 id="2022-Chinese-English-Interpretation口语翻译">2. 2022 Chinese-English Interpretation口语翻译</h1><ul><li>作者：广东外语外贸大学</li></ul><h2 id="系统结构-1">2.1. 系统结构</h2><ul><li><p>流畅度分：语速</p></li><li><p><font color="red">关键字、内容、语法：采用Bert预训练模型、BiLSTM、attention机制</font></p><img src="https://note.youdao.com/yws/api/personal/file/WEB2a27b65f57024957f23441b7cf64123f?method=download&shareKey=d00680ca591a47f6ec40226486adbc31" width="50%"></li><li><p>采用随机森林回归器融合4个维度分计算总分</p></li></ul><h2 id="其它-1">2.2. 其它</h2><ul><li>Li X, Li X, Chen S, et al. Neural-based automatic scoring model for Chinese-English interpretation with a multi-indicator assessment[J]. Connection Science, 2022, 34(1): 1638-1653.</li></ul><h1 id="2022-Word-Scoring">3. 2022 Word Scoring</h1><ul><li>作者：字节跳动</li><li><font color="red">创新点<ul><li>数据增强：给定词典中的音素序列，从训练数据相应的音素级特征中随机抽样来伪造单词样本，单词分取音素GOP均值</li><li>采用MFCC、ASR AM deep feature进行评分</li></ul></font></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe00917195f9a8da3492d09eff12bc9f6?method=download&shareKey=55db844dfc0db67bba44ab887f6bea6b" alt="数据增强"></p><h2 id="其它-2">3.1. 其它</h2><ul><li>Fu K, Gao S, Wang K, et al. Improving Non-native Word-level Pronunciation Scoring with Phone-level Mixup Data Augmentation and Multi-source Information[J]. arXiv preprint arXiv:2203.01826, 2022.</li></ul><h1 id="【弃】2020-Automated-chinese-language-proficiency-scoring-by-utilizing-siamese-convolutional-neural-network-and-fusion-based-approach">4. 【弃】2020 Automated chinese language proficiency scoring by utilizing siamese convolutional neural network and fusion based approach</h1><ul><li>论文质量较差，弃</li><li>自制数据集</li></ul><h2 id="系统结构-2">4.1. 系统结构</h2><ul><li>native speakers’ key points、测试者语音，提取100*300维向量 -&gt; 分别送入权重共享的卷积层 -&gt; pooling层 -&gt; 计算cosine相似度 -&gt; 线性层输出分数</li><li>人工设计的特征：详见SpeechRater v5.0。提取tf-idf特征，计算测试语音、同一单词人工分4分的训练语音的cosine 相似度</li></ul><h2 id="其它-3">4.2. 其它</h2><ul><li>Kwong A, Muzamal J H, Zhang P Y, et al. Automated chinese language proficiency scoring by utilizing siamese convolutional neural network and fusion based approach[C]&#x2F;&#x2F;2020 International Conference on Engineering and Emerging Technologies (ICEET). IEEE, 2020: 1-6.</li><li>语音评测系统质量控制[37, 39-42]</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>端到端语音评分</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/duan-dao-duan/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen/duan-dao-duan/</url>
      
        <content type="html"><![CDATA[<h1 id="2022-Self-Supervised">1. 2022 Self-Supervised</h1><ul><li>作者：AI Lab, Kakao Enterprise（韩国公司）</li><li><font color="red">创新点：采用自监督学习模型提取深层声学表示</font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><div style="display: inline-block; width: 60%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4422119d06867cb6f80ed36536834260?method=download&shareKey=74b198b7631782b93ae09c5a823fa71e" alt="系统结构"></p></div><div style="display: inline-block; vertical-align: bottom; width: 39%;">*GAP：global average pooling<ul><li>用L2数据+CTC fine-tune 自监督学习模型（wav2vec 2.0、HuBERT）</li><li>取各transformer层的上下文表示的均值</li><li>输入文本字符序列，采用BLSTM评分</li></ul></div><h2 id="评价">1.2. 评价</h2><ul><li>数据集：KESL（韩国英语二语学习者，根据发音、准确度、重音、停顿、语调等评总分）、Speechocean762</li><li>基线<ul><li>自研DNN-HMM AM</li><li>Agg: SpeechRater time-aggregated特征。Seq: 时间序列特征，eGeMAPS set[33]（包含MFCC、响度、音调、jitter、shimmer）的均值、方差，各特征均值方差归一化，采用OpenSmile提取</li><li>评分模型：2层全连接</li></ul></li><li>自监督学习模型：采用Fairseq上的，finetune 150k步，batch size&#x3D;8</li><li>评分模型：embedding 64维，BLSTM 1层，128维</li><li><img src="https://note.youdao.com/yws/api/personal/file/WEB7540a07fff0e50902ae1f03f916265d2?method=download&shareKey=a683224dcdddbd810b26b9422f1bc3ee" width="383px"></li><li>实验结论<ul><li>相较于传统评分模型，本文提出的模型在各测试集上的评分效果有一致的提升。其中finetune之后的HuBERT Large模型效果最好</li><li>相较于采用自监督学习模型的CNN层特征、较高层的特征表示，采用各transformer层的上下文表示的均值，效果较好</li><li>相较于线性回归、MLP，BLSTM模型评分效果较好，可以学习时间序列特征</li></ul></li><li><font color="green">存在的问题：无Speechocean762 准确度、完整度、总分评分效果</font></li></ul><h2 id="其它">1.3. 其它</h2><ul><li>Kim E, Jeon J J, Seo H, et al. Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning[J]. arXiv preprint arXiv:2204.03863, 2022.</li></ul><h1 id="2021-Multi-Encoder">2. 2021 Multi-Encoder</h1><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：ICASSP 2021</li><li>被引用次数：2</li><li><font color="red">创新点：端到端朗读评分，输出句子级、单词级分数</font></li></ul><h2 id="系统结构-1">2.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa24e448d76d182c1bf2d2df35649895b?method=download&shareKey=161bc7c16a5682e139a3565471ea8c50" alt="系统结构"></p></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>音频encoder</p><ul><li>输入：80维Mel-filterbank + 1阶差分 + 2阶差分 + pitch + 位置embedding</li><li>CNN（提取局部特征、降采样） + transformer（每一层transformer由1层multi-head attention + 1层position-wise全连接组成），12层</li></ul></li><li><p>文本encoder</p><ul><li>输入：采用sentencepiece生成的子词序列 + 位置embedding</li><li>vanilla transformer encoder结构，2层</li></ul></li><li><p>词级表示 $h1_{word\left ( i \right ) }&#x3D;Attention \left ( h_{word\left ( i \right ) }, h_{audio}, h_{audio} \right ) + h_{word\left ( i \right ) } $</p></li><li><p>单词评分：1层全连接 + sigmoid</p></li><li><p>句子评分</p><ul><li>输入：所有单词表示的average pooling，拼接所有单词的平均分</li><li>1层全连接 + ReLU + 1层全连接 + sigmoid</li></ul></li></ul></div><li><p>训练流程</p><ul><li>audio encoder + decoder，ASR训练。训练集：960h LibriSpeech + 1000h L2</li><li>冻结audio encoder，预训练。将上述训练集中的词随机替换40%为口语考试中的高频词，不匹配的单词label为0，否则为1。</li><li>采用专家标注数据finetune，采用multi-task学习（单词评分、句子评分）。训练集：10000句、15000词；测试集：1000句、1000词。3位专家打分，1-5，归一化为0-1</li></ul></li><h2 id="实验结果">2.2. 实验结果</h2><ul><li>基线<ul><li>GBT，输入特征为各音素的平均GOP分、各音素在句中的平均位置（B-1，I-2，E-3，S-4）、各音素的数量、句中元音时长的平均差</li><li>BD-LSTM with attention，声学特征为音素GOP、元音时长的平均差，文本特征为音素embedding、音素位置embedding、预训练的GloVe word embedding</li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB3897f724e308dd3f11f1d5bd91998a10?method=download&shareKey=43276c4d04b452c03fa73d45cb868411" width="393px"><h2 id="其它-1">2.3. 其它</h2><ul><li>Lin B, Wang L. Attention-Based Multi-Encoder Automatic Pronunciation Assessment[C]&#x2F;&#x2F;ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 7743-7747.</li></ul><h1 id="2021-Transfer-Learning">3. 2021 Transfer Learning</h1><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：Interspeech 2021</li><li>被引用次数：6</li><li><font color="red">创新点：当前基于DNN-HMM的GOP计算存在许多局限性。相较于传统方案采用ASR AM计算GOP等评分特征，提出直接采用AM提取的深层特征进行评分。迁移学习：采用ASR预训练模型，用评分任务fine-tune。相较于AM参数量，可采用特定的评分任务快速fine tune评分模型</font></li></ul><h2 id="系统结构-2">3.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEBe06a9dd97f1e8da8052a976b113ce875?method=download&shareKey=398ca0c53b023a081355e6265d4e5e4c"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>输入：ASR AM提取的深层特征、<font color="red">强制对齐信息</font></p></li><li><p>基于强制对齐，对于各音素，对应帧的深层特征取均值，+ phone embedding</p></li><li><p>multi-head self-attention + 非线性变换，得到词级表示</p></li><li><p>self-attention + 非线性变换，得到句级表示</p></li><li><p>sigmoid，输出句级评分</p></li></ul></div><li><p>训练流程</p><ul><li>预训练11层TDNN-HMM ASR声学模型，深层特征256维，非线性变换降为32维</li><li>采用大量合成数据，用基于GOP的评分模型打分（low-quality），预训练评分模型</li><li>采用少量指定任务的专家标注数据fine-tune</li></ul></li><h2 id="评价-1">3.2. 评价</h2><ul><li><p>数据集</p><ul><li>ASR：960h LibriSpeech + 1000h L2</li><li>合成数据：50000句</li><li>fine tune数据<ul><li>11000句（含1000句测试集），平均词数13，3位专家打分1-5</li><li>Speechocean762（50%用作测试集，分数缩放至0-1）</li></ul></li></ul></li><li><p>模型</p><ul><li>LayerNorm</li><li>参数量：AM 8165047；评分模型 20289</li></ul></li><li><p>实验结果</p><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>对比基于GOP的SOTA：<br>输入音素GOP、embedding，2层BLSTM+MLP；<br>输入音素GOP均值、各音素的平均位置（BIES）、句子音素总数，GBT</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2584d9889200577d4e05c1c40b773378?method=download&shareKey=2ff0f42e16e7e3bd70ff3579ca55fa1d" alt="table2"></td><td></td></tr><tr><td>对比输入特征<br>STPs：考虑转移概率</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB41fd8d0028b57c6ac522716778d9bf76?method=download&shareKey=640db70f420eb058d02bf9acf6e4736c" alt="table3"></td><td><font color="green">采用音素GOP特征，当前模型效果差于2BLSTM+MLP？</font></td></tr><tr><td>冻结AM参数、3阶段训练</td><td></td><td>始终冻结AM参数+第2阶段预训练+finetune效果最好。评分模型参数量约为AM的0.2%，finetune高效</td></tr><tr><td>self-attention机制：对比替换为平均</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB448228dfdb5c057f5ff3834eb5266409?method=download&shareKey=a776ada79534cc24c131ec6d40c2d626" alt="table6"></td><td></td></tr></tbody></table></li></ul><h2 id="其它-2">3.3. 其它</h2><ul><li>Lin B, Wang L. Deep Feature Transfer Learning for Automatic Pronunciation Assessment[C]&#x2F;&#x2F;Interspeech. 2021: 4438-4442.</li></ul><h1 id="2020-Multi-Modal">4. 2020 Multi Modal</h1><ul><li>作者：印度、哥伦比亚大学、SLTI公司</li><li>被引用次数：9</li><li><font color="red">创新点：提出了基于注意力融合的多模态端到端自发语音评分</font></li></ul><h2 id="系统结构-3">4.1. 系统结构</h2><img src="https://note.youdao.com/yws/api/personal/file/WEB4b48b1cf6ca4f879e1ddd50f955e5c96?method=download&shareKey=b833b0104a23cae2ad5f1e60d0e4287a" width="740px"><ul><li>音频<ul><li>输入：log-scaled mel谱，采样频率16000Hz，帧长2048、hop size 512，mel band 128个，归一化，切分为固定帧数（128）的段</li><li>CNN+BLSTM，每组(2*Conv1D+MaxPooling)后，filters数目double</li></ul></li><li>文本<ul><li>non-native ASR，word embedding：300维glove embedding初始化，OOV词初始化为全0向量，模型训练时优化</li></ul></li><li>拼接音频、文本表示<font color="red">（识别时需要输出单词起止时间）</font>，$h^{m}&#x3D;[h^{a}, h^{t}]$，attention<br>$$e_{t}&#x3D;h_{t}^{m} w_{a} ; a_{t}^{m}&#x3D;\frac{\exp \left(e_{t}\right)}{\sum_{i&#x3D;1}^{T} \exp \left(e_{i}\right)} ; c^{m}&#x3D;\sum_{t&#x3D;1}^{T} a_{t}^{m} h_{t}^{m}$$</li></ul><h2 id="实验结果-1">4.2. 实验结果</h2><ul><li><p>数据集：题目难度、评分标准遵循Common European Framework of Reference (CEFR)标准，英语口语等级A2-C1（根据题目难度确定可以得到的最高分），人工分包含5个等级：A2、LB1、HB1、LB2、HB2，其中L、H分别表示low、high</p></li><li><p>为每个题目分别训练模型</p></li><li><p>评价指标：Quadratic Weighted Kappa (QWK)，衡量两个序列间的一致性。通常取值范围为0-1，若两者一致性低于随机分布，也可以为负数。</p><p>$$\kappa&#x3D;1-\frac{\sum_{i, j} W_{i, j} O_{i, j}}{\sum_{i, j} W_{i, j} E_{i, j}}$$</p><p>其中，$W$、$O$、$E$ 均为$N\times N$的矩阵，$N$为评分类别数，$W_{i, j}&#x3D;\frac{(i-j)^{2}}{(N-1)^{2}}$，$O_{i, j}$表示评分为$i$、人工分为$j$的样本数，<font color="green"> $E_{i, j}$表示分数期望的histogram矩阵（计算方式：人工分的histogram向量与机器分的histogram向量的外积，归一化使其与矩阵$O$有相同的和）。QWK不适用于类别不均衡的数据，对样本数少的类特别敏感</font>。</p></li><li><p>采用hyperopt包优化每个分数之间round的阈值，以最大化QWK</p></li><li><p>实验结果<br><img src="https://note.youdao.com/yws/api/personal/file/WEB17d08d0c67c81e666db7ed3145e80ade?method=download&shareKey=0f1ddebcfd19bc8468516e55f866ff3e" alt="实验结果"><br>*MMAF: Multi-modal with Attention Fusion； BDRCNNAttn [A]、BDLSTMAttn  [T]：单模态基线，attention权重仅赋予音频或文本</p></li><li><p>实验结论：多模态注意力融合比单模态网络效果更好</p></li><li><p>存在的问题：未与SOTA基线对比</p></li></ul><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>attention分布、与题目难度的关系</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc6463effb413ddf4d7584db5ddd33cd9?method=download&shareKey=52862b06a3026ef3a7ef93d9430d3785" alt="attention分布1"></td><td>文本:音频attention权重比约为85:15 <br> 低难度题目（prompt 1），attention权重向音频特征倾斜；高难度题目（prompt 4），attention权重向识别文本特征倾斜 <br> 评分越高，attention权重越向识别文本特征倾斜</td></tr><tr><td>attention分布定性分析</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfafd3a21834490651a22fd68420efd64?method=download&shareKey=8958497f6fba8feaa1a0a1dd1eee2f86" alt="attention分布2"></td><td>对含uh、um等填充词，false starts，停顿的片段，attention权重偏向音频，对含关键信息的流利片段，attention权重偏向文本，如图2（prompt 4主题为smoking）</td></tr></tbody></table><h2 id="其它-3">4.3. 其它</h2><ul><li>Grover M S, Kumar Y, Sarin S, et al. Multi-modal automated speech scoring using attention fusion[J]. arXiv preprint arXiv:2005.08182, 2020.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>多维度、多粒度语音评分</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/</url>
      
        <content type="html"><![CDATA[<h1 id="2022-Cambridge-PhD-Thesis">1. 2022 Cambridge PhD Thesis</h1><ul><li>发表信息：剑桥大学博士论文</li><li><font color="red">创新点：由于大量、可靠的多维度人工评分较难获取，采用总分标注数据，通过限定输入特征进行端到端多维度评分。相较于特征提取-评分两阶段模型，端到端模型能更好地拟合人工分，且对不同的数据集、不同的任务，泛化性能更好</font></li><li><font color="red">存在的问题：需要强制对齐信息；仅自制数据集上的评分效果；由于没有维度分人工标注，采用总分统计维度分评分效果 </font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><h3 id="发音评分">1.1.1. 发音评分</h3><ul><li>采用音素距离特征，与发音人属性（音色、性别等）不相关</li><li>传统方案：采用单高斯模型建模各音素的发音，计算各音素模型间的对称KL散度，拼接为$\frac{1}{2} p\left ( p-1 \right )$维的向量，并取$\log\left ( d+1 \right )$。对于短语音，包含缺失音素的KL散度设为-1。输入一层全连接进行评分。<ul><li>缺点：每个说话者需要大量数据训练高斯模型；可能丢弃发音相关的信息；未考虑音素的发音过程、同一音素在不同上下文中发音可能不同、音素对于评分的重要性取决于上下文；存在错误的识别、强制对齐</li></ul></li><li>端到端方案：将各音素片段的帧序列编码为固定长度的embedding，对应同一音素的音素片段采用attention（学习忽略对齐错误的音素片段、关注发音错误的片段）加权平均得到音素embedding，计算音素embedding间的欧式距离。输入一层全连接进行评分。<ul><li>各模块初始化<ul><li>双胞胎双向RNN：+sigmoid，判断两音素是否一致，预训练</li><li>评分全连接层：采用基线模型初始化</li></ul></li><li>端到端fine-tuning</li><li>为了模型收敛，损失函数需要加attention权重熵的惩罚项，$C \left( \lambda, S_{train} \right)&#x3D;MSE_{train} -\beta \sum_{m&#x3D;1}^{M} \sum_{n&#x3D;1}^{N_{m}} \alpha_{n m} \log \alpha_{n m}$，其中$M$为音素总数，$N_m$ 为该音素的音素片段数</li><li><font color="green">存在的问题：音素片段数目不确定，attention如何实现？</font></li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB7176bacfcf429dccb31d01b15693550f?method=download&shareKey=ec352fbca4d4bd034b6e439a23d35347" width="709px"><h3 id="韵律评分">1.1.2. 韵律评分</h3><ul><li><p>英语的重音等时性</p></li><li><p>传统方案</p><ul><li>评分特征<ul><li>相邻元音音程时长的平均差 $ rPVI_V&#x3D;\frac{1}{K_{V}-1} \sum_{k&#x3D;1}^{K_{V}-1}\left|d\left(\tau_{k}^{(V)}\right)-d\left(\tau_{k+1}^{(V)}\right)\right| $ ，其中，$d\left(\tau_{k}\right)$ 为第k个元音音程的时长，$K_V$ 为元音音程总数</li><li>相邻非元音音程时长的平均差 $rPVI_C$</li><li>$ CCI_V&#x3D;\frac{1}{K_{V}-1} \sum_{k&#x3D;1}^{K_{V}-1}\left|\frac{d\left(\tau_{k}\right)}{l_{k}}-\frac{d\left(\tau_{k+1}\right)}{l_{k+1}}\right| $ ，其中 $l_{k}$ 为第k个元音音程中的元音个数，$\frac{d\left(\tau_{k}\right)}{l_{k}}$ 表示第k个元音音程中各元音的时长均值</li><li>$\mathrm{CCI}_{C}$</li></ul></li><li>音程示例<img src="https://note.youdao.com/yws/api/personal/file/WEB62a1368e558eb842e642101204135967?method=download&shareKey=e218a8dfbcec2c97cdde02a678016c95" width="710px"></li></ul></li><li><p>端到端方案</p><ul><li>采用attention学习音程中不同子段的重要性（学习忽略对齐错误导致的时长异常的子段），与该音程时长拼接 $ x_k&#x3D;\left[\sum_{m&#x3D;1}^{M^{(k)}} \alpha_{m} d\left(v_{m}^{(k)}\right), d\left(\tau_{k}\right)\right] $</li><li>采用序列模型（BLSTM或transformer）学习元音音程特征序列 $x_{1: K_{V}}^{(V)}$ 、非元音音程特征序列。</li><li>分别在元音音程深层特征序列$h_{1: K_{V}}^{(V)}$、非元音音程深层特征序列上加attention。</li><li>拼接 $\tilde{\boldsymbol{h}} &#x3D; \left[\tilde{\boldsymbol{h}}^{(V)}, \tilde{\boldsymbol{h}}^{(C)}\right]$，采用1层全连接评分。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB1662f02ff97ca28c83a9ac353cebcb29?method=download&shareKey=20178e325c8829035111c33474cf80cf" width="711px"></li></ul><h3 id="语调评分">1.1.3. 语调评分</h3><ul><li>重读单词基频高</li><li>传统方案<ul><li>方案一：浊音区域的基频的均值、中位数、最大值、1&#x2F;4分位数、3&#x2F;4分位数，采用1层全连接评分</li><li>方案二：考虑清音音素和静音，对于各音素，分别采用基频、浊音概率计算上述统计值并拼接，采用sequence-to-vector模型评分</li><li>方案三：采用最小二乘cosine拟合基频包络（DCT），清音区域插值，提取相应的系数向量采用DNN评分</li></ul></li><li>端到端方案<ul><li>输入：基频、浊音概率序列、position embedding。multi-head sequence-to-vector attention</li><li>考虑到长音频不适合用帧序列特征，采用sequence-to-vector模型学习各音素的特征表示，再采用sequence-to-vector模型预测分数</li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEBab93c416e54a88463f04bac213c506cf?method=download&shareKey=61fe567f298dc2bbc0a50a246d336ac2" width="500px"><h3 id="文本评分">1.1.4. 文本评分</h3><p>采用BERT提取word embedding，采用LSTM with attention评分[223]</p><h3 id="系统结构-1">1.1.5. 系统结构</h3><img src="https://note.youdao.com/yws/api/personal/file/WEBb09fd6a33bd460fc514eb7c6d99d211a?method=download&shareKey=b60236ca8e8979ca0069a830a9f487d2" width="706px"><h2 id="总分">1.2. 总分</h2><ul><li>方案一：各维度分均值</li><li>方案二：拼接各维度分打分器倒数第二层的输入表示，输入全连接网络评总分</li><li>方案三：各维度分的加权和：采用维度分打分器的中间表示、全连接网络计算attention系数</li></ul><h2 id="评价">1.3. 评价</h2><ul><li><p>数据集<br>英语水平测试数据，测试含简答、读8句话、根据提示自由表述，对每个说话人评总分0-6。</p></li><li><p>基线：Gaussian Process评分器[154]</p></li><li><p>实验结果</p><ul><li><font color="red">*由于维度分没有人工标注，采用总分进行近似</font></li><li>*由于模型对随机初始化较敏感，统计5次训练的模型的均值、标准差</li></ul><table><thead><tr><th>实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>发音</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3913cedf044c88366820bbf5c4d5c58e?method=download&shareKey=84242511a73adfa55e575ec8c8744835"></td><td>输入特征：MFCC vs PLP，性能差异较小；<br> 音素片段的sequence-to-vector模型，BLSTM + 最后一层输出additive attention，性能最好；<br> 在训练集-测试集 匹配&#x2F;不匹配的配置下，端到端模型性能均最好，一方面端到端模型可以学习更有表征能力的特征，另一方面泛化能力更好；<br>观察人工分-机器分散点图，端到端模型存在低分打高，但对特定人工分，机器分分布更集中；<br> tunability：分别采用音素距离KL散度、采用说话人分类任务训练得到的x-vector、采用评分任务训练得到的deep 音素距离特征，后者在L1分类任务上性能最好</td></tr><tr><td>韵律</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfc777e01dd42196937bed56940b389d7?method=download&shareKey=df673e504d499f435ecd21407b97e996"></td><td>sequence-to-vector模型：BLSTM + 最后一层输出additive attention；<br> 存在明显的低分打高</td></tr><tr><td>语调</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3270f41bcc6d831669450c9f25110614?method=download&shareKey=ce5ceff13afb5b18939ea898aff4de27"></td><td>相较于multi-head attention，attention LSTM评分效果更好；<br> 不存在明显的打分偏移，基于基频统计特征的DNN存在明显的低分打高</td></tr><tr><td>发音分-音素距离-音素片段attention分布</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB34df75f218c23024b00dc4e3f3b088f3?method=download&shareKey=02a7f78a56973a438be39d7228dfea90"></td><td>大多数情况下，attention权重接近均匀分布，仅排除少量异常值；<br> 余下的大部分只关注一个或少量具有代表性的音素片段；<br> 剩下的取中间值的较少，可能是随机现象，也可能是关注发音特别错误或特别好的音素片段</td></tr><tr><td>维度分相关性、互补性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBad03336b0f416faef0441aabed50706e?method=download&shareKey=2853e4e6318acb5b4860f2242397b24a"></td><td>发音评分与文本评分有一定的相关性：发音中缺失的音素与说话长度、单词的丰富程度等有关；<br> 各维度分互补</td></tr><tr><td>评分模型系统偏差</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4ef2e51c6d3e8128e12d0f861d029b1e?method=download&shareKey=9a212a7e25487cd248ad343aba51cb85"></td><td><font color="red">韵律分：高水平数据中才与总分相关，无法区分中低水平说话人的韵律水平；<br> 发音和文本特征较难区分高水平数据；<br> 发音分低分打低：可能由于ASR错误率更高</font></td></tr><tr><td>总分</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB28847721cde7c6b5b25325a1baf7b812?method=download&shareKey=ba4b6bf4eae0cde16e8ed2c6d66dc33f"></td><td>各维度分的加权和（采用attention计算加权系数）效果最好</td></tr></tbody></table></li></ul><h2 id="其它">1.4. 其它</h2><ul><li>Kyriakopoulos K. Deep Learning for Automatic Assessment and Feedback of Spoken English[D]. University of Cambridge, 2022.<ul><li>5.1节：理论推导了采用总分标注数据，训练多维度评分模型的可行性</li></ul></li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2018). A deep learning approach to assessing non-native pronunciation of english using phone distances. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 2018, pages 1626–1630.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2019). A deep learning approach to automatic characterisation of rhythm in non-native english speech. In INTERSPEECH, pages 1836–1840.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2020). Automatic detection of accent and lexical pronunciation errors in spontaneous non-native english speech. Interspeech.</li></ul><h1 id="2022-Multi-Aspect-Multi-Granularity">2. 2022 Multi-Aspect Multi-Granularity</h1><ul><li>作者：MIT 人工智能实验室（CSAIL）、平安科技研究院（PAII Inc.）</li><li>发表信息：ICASSP 2022</li><li><font color="red">代码：<a href="https://github.com/YuanGongND/gopt">https://github.com/YuanGongND/gopt</a></font> (Goodness Of Pronunciation feature-based Transformer)</li><li><font color="red">创新点<ul><li>联合训练音素、单词、句子级各维度分及总分</li><li>采用BERT风格非层级的标准Transformer 架构</li></ul></font></li></ul><h2 id="系统结构-2">2.1. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB915a9e1bc3fbd6be8224b3a8122dbfbd?method=download&shareKey=6c0e39f6c5b19fa1a9892f30ad805242" alt="系统结构"></p><ul><li>声学模型<ul><li>TDNN-F，训练集：960h Librispeech，用Kaldi Librispeech S5 recipe训练</li><li>PAII-A：自研AM，452h L1 + 1696h L2</li><li>PAII-B：995h L1 + 6591h L2</li></ul></li><li>输入<ul><li>GOP特征：84维（42个音素，log phone posterior、log posterior ratio），经过1层线性层降维至24维</li><li>正确发音phone embedding，24维。<ul><li>音素序列填充5个[cls] token，对应句子级各维度分、总分</li></ul></li><li>位置embedding，24维，可训练</li></ul></li><li>采用标准Transformer encoder结构，但减为3层，embedding 24维</li><li>评分：各个评分分别采用1层24*1的线性层，layer normalization。<font color="green">单词分：训练时反向传播至该单词的各个音素，推断时取其各个音素的输出的均值。</font></li></ul><h2 id="评价-1">2.2. 评价</h2><ul><li>数据集：speechocean762（类别不均衡，主要为高分），单词、句子评分缩放至0-2，与音素一致</li><li>评价指标：主要为PCC（Pearson相关系数）</li><li>基线：speechocean762实现的RF（随机森林）、SVR（支持向量回归），[21]transfer learning、LSTM（模型深度、维度等与GOPT一致，LSTM最后一个token的输出作为句子表示）</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe86bc0edfdbde5c5f9c1462cd7bc2797?method=download&shareKey=a13ff3549e54e584ec7ebb674f8d68f3" alt="实验结果"><br>*用不同的随机种子重复5次实验，统计均值、标准差</p><img src="https://note.youdao.com/yws/api/personal/file/WEBe04116ccf077c827de0f320a76641997?method=download&shareKey=7c437dbe136f2f972ca8c513e65879df" width="50%"><ul><li>实验结论<ul><li><font color="red">GOPT除单词重音、句子完整度评分性能较差（可能与speechocean762训练集中句子完整度分布不均有关）外，其它任务可提供SOTA效果</font></li><li>采用PAII-A，音素、单词评分性能提升，但句子评分性能下降</li><li>联合训练音素、单词、句子评分模型，相对于分别训练，各模型性能都有提升。</li><li>正确发音phone embedding对提升模型性能有帮助</li><li>继续加宽或加深模型结构，性能无提升（训练集较小）</li><li>采用PAII-A、PAII-B，评分性能相当</li></ul></li></ul><h2 id="其它-1">2.3. 其它</h2><ul><li>Gong Y, Chen Z, Chu I H, et al. Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment[C]&#x2F;&#x2F;ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022: 7262-7266.</li></ul><h1 id="2020-Multi-Granularity">3. 2020 Multi-Granularity</h1><ul><li>作者：腾讯智能平台产品部、北京语言大学</li><li>发表信息：Interspeech 2020</li><li><font color="red">创新点<ul><li>考虑音素、单词、句子评分间的层次关系和上下文，提出一种分层网络结构，联合评分</li><li>采用半监督训练，利用无标注数据训练音素检错</li></ul></font></li></ul><h2 id="系统结构-3">3.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEB15d8915a312dd0f51572a305759e4465?method=download&shareKey=06c0c6e07038fab09d0b18a838356915" width="782px"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>音素检错</p><ul><li>输入：GOP（通过强制对齐计算）、音素embedding、位置embedding（B、I、E、S分别表示单词开头、中间、末尾、单音素词。<font color="red">音素发音因其在单词中的位置而异</font>）、类别embedding（C、V分别表示辅音、元音。<font color="red">单词中元音和辅音的重要性不同</font>）</li><li>模型结构：BLSTM。半监督学习</li></ul></li><li><p>单词评分<br><font color="red">单词中每个音素对最终单词得分的贡献不同，采用attention机制。</font><br>$U_{p}&#x3D;\tanh \left(w * O_{p}+b\right)$, $\alpha_{p}&#x3D;\frac{\exp \left(U_{p}^{T} U_{w}\right)}{\sum_{q \in w} \exp \left(U_{q}^{T} U_{w}\right)}$, $S_{w}&#x3D;\sum_{p \in w} \alpha_{p} O_{p}$<br><font color="green"> 其中, $O_p$ 为音素$p$的评分, $U_w$是随机初始化的向量, 可以作为单词上下文的记忆单元。</font></p></li><li><p>句子评分</p><ul><li><font color="red">不同属性（如词性、音素个数）的单词对句子得分贡献不同。</font></li><li>输入：word层输出、词性、单词长度</li><li>模型结构：BLSTM+MLP，sigmoid回归。</li></ul></li><li><p>multitask：$L_{total}&#x3D;(1-w)\times L_{sent}+ w\times L_{phoneme}$，$L_{sent}$其中为句子评分的均方误差损失，$L_{phoneme}$为PUNU损失。</p></li></ul></div><li><p>半监督 - PUNU (positive unlabeled and negative unlabeled) learning</p><ul><li><p>正样本：native发音；负样本：GOP较低的L2学习者发音；unlabeled数据：剩下的L2发音。</p></li><li><p>损失函数如下：</p><p>$R_{\mathrm{PUNU}}^{\gamma}(g)&#x3D;(1-\gamma) R_{\mathrm{PU}}(g)+\gamma R_{\mathrm{NU}}(g)$<br>$R_{\mathrm{PU}}(g)&#x3D;\theta_{\mathrm{P}} E_{\mathrm{P}}[l(g(x), 1)]+E_{\mathrm{U}}[l(g(x),-1)]-\theta_{\mathrm{P}} E_{\mathrm{P}}[l(g(x),-1)]$<br>$R_{\mathrm{NU}}(g)&#x3D;\theta_{\mathrm{N}} E_{\mathrm{N}}[l(g(x),-1)]+E_{\mathrm{U}}[\mathrm{l}(g(x), 1)]-\theta_{\mathrm{N}} E_{\mathrm{N}}[l(g(x), 1)]$</p><p>其中, $g$ 为任意决策函数, $l$ 为 loss 函数, $\theta_P$、$\theta_N$ 为正负样本的先验概率, $E_U$、$E_P$、$E_N$ 分别表示未标记数据、正类、负类（边际）的损失期望。</p></li></ul></li><h2 id="评价-2">3.2. 评价</h2><ul><li><p>数据集</p><ul><li>Timit + 22998英语句子，1000中国说话人，16-20岁。句子评分、单词评分、音素检错标注量分别为8998、4000、10000句。句子平均单词数为13。标注音素量99568。</li><li>1-5分，3人评分取均值。3人评一致性：计算某一评分员的评分与剩余评分员的平均分之间的PCC，句子、词级分别为0.78、0.76。</li><li>检错3人投票，<font color="green"> 3人评一致性：随机挑选1000句，计算任意两标注员的Kappa系数，平均0.65，95%置信度区间(0.647, 0.653)，p-value小于0.1%，一致性较高。</font></li></ul></li><li><p>训练集：7998句non-native数据，有评分。5000句native数据，无评分。</p></li><li><p>测试集：4000句，标注了39808个音素、1000词、1000句。错误音素占比约14%。</p><table><thead><tr><th>音素错误分布</th><th>单词分分布</th><th>句子分分布</th></tr></thead><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB85b5a6de565f9c520658835b95786ba8?method=download&shareKey=8632f81f22963f5f07fd6cb3e9eef9d2"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2d0fd1ee7ade8549ac8bcab6226f37e1?method=download&shareKey=2f0a145baa086763a420d45b143c03a9"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4d0c8cbceb3537c0867f876ff2f2f3e7?method=download&shareKey=6e744f4ae6c8a59b0bb97a0674605191"></td></tr></tbody></table></li><li><p>实验结果</p><table><thead><tr><th>实验</th><th>基线</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>句子评分</td><td>2BLSTM+MLP，后一BLSTM的输入为音素BLSTM最后一个隐含单元的输出拼接、词性、单词长度</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa5476cf9e1f6ac5f1ee3c5cdf4c56b69?method=download&shareKey=1c586ae1cc1ff426900c1b9e0fccfd67"> <br> *STL：single task learning</td><td>单词层attention、multitask学习可提升评分性能</td></tr><tr><td>单词评分</td><td>BLSTM+MLP：去掉上述句子评分BLSTM。SL：用3000个单词评分标注数据训练</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB30a1747796429bff94888d09a7eb6ce1?method=download&shareKey=fd537dcca8a48cdd697826af548cdbd4"></td><td>对比前两行：attention机制有收益；<br>最后一行：PCC较高，仅用句子、音素级标注信息，仍能学到单词分信息</td></tr><tr><td>音素检错</td><td>SL：用59760个音素检错标注训练</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB8f8bcea9b4f939f430dd5b5985a7b779?method=download&shareKey=767fa915db6e5fe6e9c036fb5461cd0b"></td><td>半监督学习未召回略差于有监督学习，虚警相差较小。<font color="green">加无标注数据效果变差？</font></td></tr></tbody></table></li></ul><h2 id="其它-2">3.3. 其它</h2><ul><li>Lin B, Wang L, Feng X, et al. Automatic scoring at multi-granularity for L2 pronunciation[J]. Proc. Interspeech 2020, 2020: 3022-3026.</li></ul><h1 id="2019-ETS-monologue-and-dialogue">4. 2019 ETS monologue and dialogue</h1><ul><li>作者：ETS</li><li>发表信息：ICASSP 2019</li><li><font color="red">创新点<ul><li>采用基于attention的BLSTM对自由表述的3个维度评分：内容（话题相关度、得体性）、 <del>组织（语篇结构和连贯性）、</del> 语用（词汇、语法）、delivery（发音、重音、流利度、语调）</li><li>采用BLSTM或MemN2N（端到端记忆网络）编码提示文本或多轮对话的历史信息</li></ul></font></li></ul><h2 id="系统结构-4">4.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEBb737aee697fb953d540bee20a2fa9f48?method=download&shareKey=bd923154f67ce2976488da77c835431c"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>内容</p><ul><li>word embedding层：用Google’s Word2Vec初始化，模型训练时优化</li><li>采用BLSTM将提示文本的词序列编码为固定长度的向量$v^{p}$，与回答中各个词的词向量$e_{t}^{r}$拼接</li></ul></li><li><p>语用特征：POS：词性one-hot向量；DEP：句法依存标签，如主语、宾语；Morph（形态）。采用spaCy提取，分别19、51、248维</p></li><li><p>发音：采用non-native ASR模型识别，native ASR模型<font color="red">强制对齐</font>。8维特征：时长、音调、强度、静音或停顿时长、non-native ASR模型后验概率、native ASR模型后验概率、识别结果LM分、ASR置信度分，取各帧平均（实验对比音素、音节、词级特征）</p></li><li><p>评分模型</p><ul><li>维度分：<font color="green">feed-forward attention层输出向量的均值。</font>多轮对话：对每个回答评维度分，整个对话的维度分取多轮对话的均值</li><li>总分：3个维度分拼接，经过1层全连接层</li></ul></li></ul></div><li><p>采用MemN2N（端到端记忆网络）编码多轮对话的历史信息</p><img src="https://note.youdao.com/yws/api/personal/file/WEB7bf9ff8d8f680293f1103d2ad9247e0d?method=download&shareKey=e1ee2a6611af41883a37a25ee02708ba" width="414px"><p>拼接$e_{t}^{r}$、$v^{p}$、$a^{p} \cdot v_{h}^{p}$、$a^{r} \cdot v_{h}^{r}$，其中，$v_{h}^{p}$、$v_{h}^{r}$分别表示历史提示、历史回答，$a^{p}$、$a^{r}$分别表示对应的attention向量</p></li><li><p>语用特征示例</p><img src="https://note.youdao.com/yws/api/personal/file/WEB8b9848c574cb881e11ef9f4ba799a455?method=download&shareKey=c9480e8e26bb1c13902d745c6f37e3d5" width="410px"></li><h2 id="评价-3">4.2. 评价</h2><ul><li><p>数据集</p><ul><li><img src="https://note.youdao.com/yws/api/personal/file/WEB4c9a9144977d69a7f73545ff490b5f49?method=download&shareKey=ecb0b2fae6f26aa5092f69916a3ce29c" width="414px"></li><li>monologue：delivery、内容、语用，0-4分</li><li>对话：整个对话的总分，考虑熟练程度和任务完成情况</li></ul></li><li><p>声学模型</p><ul><li>识别模型：基于iVector的BLSTM，960h non-native数据。LM用提示文本自适应</li><li>强制对齐：960h LibriSpeech</li><li>提取语用、内容特征时过滤filler words、重复的partial words</li></ul></li><li><p>超参：BLSTM 128维。dropout&#x3D;0.5。100 epochs、batch size 64。MemN2N：记忆前10轮提示与回答，memory size 20</p></li><li><p>基线</p><ul><li>评分特征：SpeechRater，超过100个</li><li>回归模型：Logistic回归、AdaBoost、决策树、Gradient Boost、SVM、随机森林等。其中，随机森林效果最好。</li></ul></li><li><p>实验结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEBda84d306717e91a49a5487ae38050729?method=download&shareKey=687b24e9378b54aaa8e828c1589709e4" width="417px"><p>*预测的维度分与总分计算相关度，无人工标注</p></li><li><p>实验结论</p><ul><li>内容：结合提示信息，评分效果较好。采用MemN2N可进一步提高多轮对话内容评分与人工分的相关度</li><li>发音采用音节级特征较好（音素级或音节级LM分：采用所属单词的LM分）</li><li>输入所有特征计算总分，效果更好，神经网络可以学习各维度特征间的关系</li></ul></li></ul><h2 id="其它-3">4.3. 其它</h2><ul><li>Qian Y, Lange P, Evanini K, et al. Neural approaches to automated speech scoring of monologue and dialogue responses[C]&#x2F;&#x2F;ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2019: 8112-8116.</li><li>展望：可解释性、诊断</li></ul><h1 id="2018-ETS-prompt-aware">5. 2018 ETS prompt-aware</h1><ul><li><p>方案同上</p></li><li><p>基线：SVR评分模型。采用C-rater系统提取特征：</p><ul><li>2-5阶 character n-gram</li><li>词级 1-2阶 n-gram</li><li>回答的字符数</li><li>句法依赖：采用Zpar dependency parser提取</li><li>Prompt bias</li></ul></li><li><p>LM：口语测试转写文本（超过5百万词）训练的LM、提示文本训练的LM 线性插值</p></li><li><p>实验结果</p><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>prompt-aware<br>Siamese LSTM: 用于评分前先进行离题检测，分类准确度97.3%。Manhattan distance</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa24523c00cf49bcef30e9181fffc1655?method=download&shareKey=ad09059e7e16a6f1de9fd0f54b8d4e64" alt="模型结构对比"></td><td>prompt-encoder可以学到离题信息</td></tr><tr><td>模型结构</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB724c8edd670b9460906fb9935b2c7a9d?method=download&shareKey=69cae0a1c815483183f70678579416b5" alt="模型结构对比"></td><td>fine-tune word embedding、attention机制、prompt encoder都有收益</td></tr><tr><td>对比传统模型</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf1396007c43b6dd584ac28e18b137e3e?method=download&shareKey=baf3230953f5f4a36a8581b2f325123a" alt="对比基线"></td><td>prompt bias特征收益较小；<br>相较于采用人工转写，采用ASR识别结果时集外题相对集内题的效果下降更显著；<br>prompt encoder（最后两列）有收益，特别是在集外题上</td></tr></tbody></table></li></ul><h2 id="其它-4">5.1. 其它</h2><ul><li>Qian Y, Ubale R, Mulholland M, et al. A prompt-aware neural network approach to content-based scoring of non-native spontaneous speech[C]&#x2F;&#x2F;2018 IEEE spoken language technology workshop (SLT). IEEE, 2018: 979-986.</li><li>内容分<ul><li>LSA (Latent Semantic Analysis)： 对各任务分别训练LSA模型，计算识别的词序列与训练集中高分数据的cosine相似度，SVD（奇异值分解）降维</li><li>CVA (Content Vector Analysis)：按人工分将训练集分组，计算cosine相似度。</li><li>考虑识别单词的置信度分，使模型对识别错误更鲁棒</li><li>multi-task训练BLSTM：打分+word embedding，指定分数的词向量更有区分性<blockquote><p>Alikaniotis D, Yannakoudakis H, Rei M. Automatic text scoring using neural networks[J]. arXiv preprint arXiv:1606.04289, 2016.</p></blockquote></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeNet论文</title>
      <link href="/blog/yu-yin/gong-ju-bao/wenet/lun-wen/"/>
      <url>/blog/yu-yin/gong-ju-bao/wenet/lun-wen/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>cleanup_segmentation</title>
      <link href="/blog/yu-yin/gong-ju-bao/kaldi/cleanup-segmentation.mm/"/>
      <url>/blog/yu-yin/gong-ju-bao/kaldi/cleanup-segmentation.mm/</url>
      
        <content type="html"><![CDATA[<div class="markmap-container" style="height:800px">  <svg data="{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;run_cleanup_segmentation.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;clean_and_segment_data.sh &lt;br/&gt; 采用转写文本构建语言模型，选取和转写文本编辑距离最小的解码路径；&lt;br/&gt;若识别为连续的重复，non-scored words间的错误，与sil、fix、OOV相邻的删除错误，修正转写文本；&lt;br/&gt; 挑选识别正确的片段，并加一系列限定条件 &lt;br/&gt; &lt;font style=background:green&gt; 输入&amp;lt;srcdir&amp;gt;：SAT GMM模型目录，或fMLLR对齐结果&lt;/font&gt; &lt;br/&gt; clean_and_segment_data_nnet3.sh 区别：功能一致，用NNET3模型解码&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;make_biased_lm_graphs.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;取转写文本中top_n_words（默认值100）个高频词，unigram概率=频次/高频词总频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;make_biased_lms.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;根据min-words-per-graph（默认值100）将音频文本分组&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;make_one_biased_lm.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;统计第n阶的count&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;CompletelyDiscountLowCountStates&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;GetHistToTotalCount：统计各历史/前缀（长度&amp;gt;=2）出现的频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;对第n至3阶，若历史/前缀出现的频次&amp;lt;min_count（默认值10），删除该项，并回退&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;ApplyBackoff：对第2至n阶，各项频次折扣discounting-constant（默认值0.3），累加给backoff_symbol，相应的低1阶的频次+1&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;AddTopWords：添加高频词unigram，频次为unigram总频次*概率&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;PrintAsFst：ngram概率=折扣后的ngram概率+折扣的概率*低1阶的概率，打印FST&quot;}]}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;compile-train-graphs-fsts：生成group HCLG&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;decode_segmentation.sh：gmm-latgen-faster&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;analyze_lats.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;lattice-depth-per-frame：lattice中经过各帧的弧数&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[22,23]},&quot;v&quot;:&quot;lattice-best-path：获取 1best 识别和对齐&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[23,24]},&quot;v&quot;:&quot;ali-to-phones --write-lengths=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[24,25]},&quot;v&quot;:&quot;analyze_phone_length_stats.py 统计各音素时长分布&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[25,26]},&quot;v&quot;:&quot;ali-to-phones --per-frame=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[26,27]},&quot;v&quot;:&quot;analyze_lattice_depth_stats.py 统计各音素lattice depth分布&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[28,29]},&quot;v&quot;:&quot;lattice_oracle_align.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[29,30]},&quot;v&quot;:&quot;lattice-oracle：获取和转写文本编辑距离最小的解码路径&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[30,31]},&quot;v&quot;:&quot;get_ctm&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[31,32]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-align-words-lexicon 对齐词边界&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[32,33]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-1best 获取最优路径（消歧符可能导致上述lattice有多条路径）&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[33,34]},&quot;v&quot;:&quot;nbest-to-ctm 打印utt_id, channel, start, dur, word_id&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[34,35]},&quot;v&quot;:&quot;align-text&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[35,36]},&quot;v&quot;:&quot;wer_per_utt_details.pl：打印utt-id、编辑距离、转写词数、解码结果、转写&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[36,37]},&quot;v&quot;:&quot;wer_per_spk_details.pl&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[37,38]},&quot;v&quot;:&quot;wer_ops_details.pl：打印各个词识别为正确、插入、删除、替换的频次&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[38,39]},&quot;v&quot;:&quot;get_ctm_edits.py：打印utt_id, channel, start, dur, 识别的单词, 置信度(始终为1），转写的单词, 编辑类型 &lt;br/&gt;（其中，sil表示无对应的转写单词且识别为&amp;lt;eps&amp;gt; sil；若转写单词不在词典中且识别为&amp;lt;unk&amp;gt;则编辑类型为cor）&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[40,41]},&quot;v&quot;:&quot;modify_ctm_edits.py：若识别为non-scored words（!sil, &amp;lt;eps&amp;gt;, &amp;lt;spoken_noise&amp;gt;, &amp;lt;unk&amp;gt;）间的替换、插入、删除，&lt;br/&gt;或者识别为连续的重复（如转写文本为a，识别为a a；或转写文本为a b，识别为a b a b），&lt;br/&gt;将转写文本中的词替换为识别文本中的，前者编辑类型改为fix，后者编辑类型改为cor&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[42,43]},&quot;v&quot;:&quot;taint_ctm_edits.py：识别错误前后连续相邻的sil、fix、OOV识别为&amp;lt;unk&amp;gt; 标记为tainted；若该识别错误为删除且前/后有tainted，删除该行&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[44,45]},&quot;v&quot;:&quot;segment_ctm_edits.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[45,46]},&quot;v&quot;:&quot;默认参数&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[46,47]},&quot;v&quot;:&quot;片段最短时长0.5s、新片段最短时长1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[47,48]},&quot;v&quot;:&quot;tainted词最大时长0.05s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[48,49]},&quot;v&quot;:&quot;片段首尾静音最大时长0.5s（若导致不满足片段最短时长或新片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[49,50]},&quot;v&quot;:&quot;片段首尾non-scored word最大时长0.5s（若导致不满足片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[50,51]},&quot;v&quot;:&quot;片段内单个静音段最大时长2s、片段内单个non-scored word&lt;font style=background:green&gt;（除OOV，实现未考虑）&lt;/font&gt;最大时长2s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[51,52]},&quot;v&quot;:&quot;如果片段首尾靠近识别错误，填充0.05s &amp;lt;unk&amp;gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[52,53]},&quot;v&quot;:&quot;tainted词+填充的unk占片段时长的最大比例0.1&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[53,54]},&quot;v&quot;:&quot;根据上一条规则分段时，分割点静音段或non-scored word的最短时长0.1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[54,55]},&quot;v&quot;:&quot;合并重叠或相邻的片段时，若片段间删除的转写词数&amp;lt;=1，保留&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[55,56]},&quot;v&quot;:&quot;GetSegmentsForUtterance&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[56,57]},&quot;v&quot;:&quot;ComputeSegmentCores：挑选仅含cor、fix、sil的片段，至少有一个词识别为cor且不为OOV，不包含tainted&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[57,58]},&quot;v&quot;:&quot;PossiblyAddTaintedLines：若边界识别为cor并且单词非non-scored word，前后相邻的1个词为tainted，扩充该词&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[58,59]},&quot;v&quot;:&quot;PossiblySplitSegment：根据片段内静音最大时长、片段内non-scored word最大时长分段，将识别为sil或转写文本为non-scored word的词均分&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[59,60]},&quot;v&quot;:&quot;PossiblyTruncateBoundaries：根据片段首尾静音最大时长、片段首尾non-scored word最大时长 截断片段首尾sil或non-scored word&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[60,61]},&quot;v&quot;:&quot;RelaxBoundaryTruncation：片段首尾非tainted截断：若满足片段时长&amp;gt;=片段最短时长、新片段最短时长，不撤销；若全撤销后仍不满足，全撤销；否则放宽截断比例至正好满足条件。&lt;br/&gt;令b=1-a，则length_with_truncation + (length_with_relaxed_boundaries - length_with_truncation) * b = length_cutoff &lt;br/&gt; start_keep_proportion = orig_start_keep_proportion + (1-orig_start_keep_proportion) * b&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[61,62]},&quot;v&quot;:&quot;PossiblyAddUnkPadding：若片段首尾识别为cor并且非non-scored word，填充unk_padding：不超过音频起止时刻；若填充时长&amp;lt; 0.5*unk_padding，不填充&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[62,63]},&quot;v&quot;:&quot;删除不满足新片段最短时长、片段最短时长的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[63,64]},&quot;v&quot;:&quot;PossiblyTruncateStartForJunkProportion：若片段起始 (unk_padding+tainted词时长)/(时长&amp;gt;min_split_point_duration，第一个识别为sil或识别为cor的non_scored_word前的时长) &amp;gt;= max_junk_proportion，删除该词前的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[64,65]},&quot;v&quot;:&quot;PossiblyTruncateEndForJunkProportion：同上，处理片段末尾&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[65,66]},&quot;v&quot;:&quot;ContainsAtLeastOneScoredNonOovWord：片段包含至少一个识别为cor、非OOV的scored_word，否则删除&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[66,67]},&quot;v&quot;:&quot;若片段首尾(unk_padding+tainted词)时长占比&amp;gt;max_junk_proportion，删除该片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[67,68]},&quot;v&quot;:&quot;合并重叠或相连的片段：若重叠片段包含的识别为del的词数&amp;gt;合并时保留的最大删除词数，则text不包含这些词&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[68,69]},&quot;v&quot;:&quot;AccWordStatsForUtterance：统计转写文本中各个词的词频、不被包含在分段中的比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[69,70]},&quot;v&quot;:&quot;WriteSegmentsForUtterance：写分段转写文本text和segment文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[70,71]},&quot;v&quot;:&quot;PrintDebugInfoForUtterance：写ctm文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[71,72]},&quot;v&quot;:&quot;PrintSegmentStats：打印音频总数，被完全丢弃的音频数，总时长，每一步处理后segment数目、相对于原始数据的时长比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[72,73]},&quot;v&quot;:&quot;PrintWordStats&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[74,75]},&quot;v&quot;:&quot;创建数据文件夹：统计padding=训练集（音频时长-特征时长）最高频的值，segment结束时刻+=padding &lt;br/&gt; 主要处理feats.scp、vad.scp，cmvn.scp需要重新生成&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[76,77]},&quot;v&quot;:&quot;重新计算CMVN&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[78,79]},&quot;v&quot;:&quot;对齐清洗后数据&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[80,81]},&quot;v&quot;:&quot;重训GMM模型&quot;}]}"/></div><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> Kaldi </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>评分标准</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/ping-fen-biao-zhun/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen/ping-fen-biao-zhun/</url>
      
        <content type="html"><![CDATA[<h1 id="考纲">1. 考纲</h1><ul><li><a href="http://www.neea.edu.cn/res/Home/1901/d15ec0514666ac280810099f9595b557.pdf">普通高等学校招生全国统一考试大纲-2019</a><ul><li>语音项目<ul><li>基本读音：辅音连缀、成音节、单词重音</li><li>连读、失去爆破、弱读、同化</li><li>意群与停顿、语调、句子重音、节奏</li></ul></li></ul></li><li>英语口语等级考试三级考试大纲-2018（湖北省教育考试院）<a href="http://www.hbea.edu.cn/html/2018-11/12162.html">^湖北SETS</a><ul><li>短文朗读：语音语调（权重0.6）、流利程度（权重0.4）</li><li>情景提问：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>情景应答：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>连续表达：语音语调（权重0.2）、语法词汇（权重0.3）、流利程度（0.2）、交际能力（权重0.3）</li><li>分档：4档，百分制 &gt;&#x3D;85、&gt;&#x3D;75、&gt;&#x3D;60、&lt;60</li></ul></li><li>广东高考-2011：模仿朗读、情景问答、故事复述<a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></li><li>广西高考-2021 <a href="%5B%E5%B9%BF%E8%A5%BF%E9%AB%98%E8%80%83-2021%EF%BC%88%E6%8B%9B%E7%94%9F%E8%80%83%E8%AF%95%E9%99%A2%EF%BC%89%5D(https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf)">^广西高考</a><ul><li>模仿朗读：语音语调（权重0.5）、流利度（权重0.3）、完整度（权重0.2）</li><li>口头表达：内容（权重0.5）、语法（0.27）、语音语调+流利度（0.23）</li></ul></li><li>2019年江苏省初中英语听力口语自动化考试纲要：朗读短文（4档）、情景问答（2档）、话题简述（4档）<a href="https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf">^江苏中考</a></li><li>宁波市2022年初中学业水平考试英语听力口语自动化考试说明：朗读短文（4档）、情景问答（3档）、话题简述（4档）<a href="http://nbeea.nbedu.net.cn/ckfile/files/%E5%AE%81%E6%B3%A2%E5%B8%822022%E5%B9%B4%E5%88%9D%E4%B8%AD%E5%AD%A6%E4%B8%9A%E6%B0%B4%E5%B9%B3%E8%80%83%E8%AF%95%E8%8B%B1%E8%AF%AD%E5%90%AC%E5%8A%9B%E5%8F%A3%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%8C%96%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E.pdf">^宁波中考</a></li><li><a href="http://jyj.panjin.gov.cn/2019_09/24_00/content-65069.html">盘锦中考-2019</a>：朗读（4档）、情景问答（3档）</li><li><a href="http://edu.wenzhou.gov.cn/art/2022/3/3/art_1341152_59021518.html">温州中考-2022</a>：篇章朗读、情景问答、说话 （无评分标准介绍）</li></ul><h1 id="维度分考察点">2. 维度分考察点</h1><table><thead><tr><th>维度分</th><th>考察点</th><th>备注</th><th>分档描述</th></tr></thead><tbody><tr><td>发音准确度</td><td>语音、声调&#x2F;单词重音</td><td>与完整度无关，自由表述题中与正确答案无关[^先声]</td><td>5档：<br>语音、语调清晰、准确；<br>有错误，但不影响理解；<br>有错误，且有时影响理解；<br>有多处错误，且影响理解；<br>表现出较严重发音困难，且严重影响理解[^广西高考]</td></tr><tr><td>流畅度</td><td>语速、停顿次数、重复[^讯飞]</td><td>与朗读的内容无关[^先声] <font style="background: green">（回读）</font></td><td>5档：<br>朗读自然流利，语速适中，有节奏感[^宁波中考]无语流中断，停顿和反复现象很少[^湖北SETS]；<br>基本流畅；<br>部分话语不够流畅；<br> 话语大部分不流畅; <br> 不流畅[^广西高考]</td></tr><tr><td>标准度&#x2F;韵律</td><td>无中式口音，能灵活地运用连读、重读、失去爆破等发音技巧，节奏良好，感情充沛[^讯飞]<br>意群停顿、升降调、句子重音[^先声]</td><td></td><td></td></tr><tr><td>完整度</td><td>朗读题：已读内容占提示文本的比例<br>自由表述题：要点覆盖率</td><td></td><td>5档：<br>内容丰富，完整、连贯；<br>内容基本完整、偶尔不够连贯；<br>有部分陈述不够完整，有时不连贯；<br>大部分陈述不完整，或不连贯；严重缺乏完整性和连贯性[^广西高考]</td></tr><tr><td>语法</td><td>人称、单复数、时态、语态、动词的及物性；<br>词汇、短语、语法结构使用[^广西高考]</td><td></td><td>5档：<br>能用合适的词汇、短语、语法结构组织话语；<br>个别地方出现错误；<br>少量错误；<br>大部分不正确；<br>不能正确使用[^广西高考]</td></tr></tbody></table><h1 id="音标朗读">3. 音标朗读</h1><h1 id="单词朗读">4. 单词朗读</h1><h1 id="短文朗读">5. 短文朗读</h1><h2 id="总分与维度分">5.1. 总分与维度分</h2><blockquote><p>成人句子：total_score &#x3D; (0.6*accuracy_score + fluency_score*0.3 + standard_score*0.1)* integrity_score&#x2F;100</p><p>成人篇章：total_score &#x3D; (0.5*accuracy_score + fluency_score*0.3 +standard_score*0.2)* integrity_score&#x2F;100 <a href="https://www.xfyun.cn/doc/Ise/IseAPI.html">^讯飞</a></p></blockquote><h1 id="单项选择">6. 单项选择</h1><p>用户只能按事先设定的固定答案作答；只有读正确答案并且发音正确、完整，才有得分；用户回答多个选项，以后面的回答为准。<a href="https://open.singsound.com/doc/engine?type=engine-en-en.sent.score">^先声</a></p><h1 id="情景问答">7. 情景问答</h1><h2 id="题型说明">7.1. 题型说明</h2><p>先描述一段场景，然后从描述的场景中提出一个问题，让回答者根据听到的场景回答问题<a href="https://open.singsound.com/doc/engine?type=engine-en-en.sent.score">^先声</a></p><h2 id="示例">7.2. 示例</h2><p><a href="https://www.chivox.com/opendoc/#/ChineseDoc/coreEn">^先声</a></p><pre class="line-numbers language-none"><code class="language-none">&quot;para&quot;（描述情景的文本）: &quot;It&#39;s unbelievable. He looks stupid, but in fact, he is such a great and humorous actor. What&#39;s going on? You know what? Mr. Bean graduate from Oxford University. Exactly, I am also very crazy about Mr. Bean. He is really a funny guy and he does have a great sense of humor. In my eyes, he is a genius. I really admire him. I couldn&#39;t agree more, and it&#39;s his giftedness and hard works that make him succeed. After seeing his interesting films, I feel cheerful and excited, he brings happiness to us. Yes, I hope we can bring laughter to people too, just like Mr. Bean. I can&#39;t wait to see more his films after class. But first thing first, let&#39;s get our homework done.&quot;,&quot;quest_ans&quot;（提问问题的文本）: &quot;What makes Mr. Bean so successful?&quot;,&quot;lm&quot;（可能的正确回答；每个text表示一种正确的回答）: [    &#123;&quot;text&quot;: &quot;It&#39;s his giftedness and hard works that make him succeed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;is talent and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works makes Mr. Bean so successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His talent and hard work makes him successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His body language is so funny, he makes people laugh, feel happy and relaxed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;Hard work and giftedness.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and he is very hard working.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His gift and hard works.&quot;&#125;],&quot;key&quot;（关键点可能的表述方式；关键点对打分的影响很大）:    [[&quot;giftedness&quot;, &quot;gift&quot;, &quot;talent&quot;], &quot;hard work&quot;],&quot;unkey&quot;（错误答案，用户发音命中其中任一错误答案，对得分影响很大，得分会较低）:[&quot;no&quot;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="评分标准">7.3. 评分标准</h2><h3 id="广东高考">7.3.1. 广东高考</h3><p><a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></p><table><thead><tr><th></th><th>权重</th><th>分档描述</th></tr></thead><tbody><tr><td>信息</td><td>75%</td><td>1.5分：按照要求传递了信息；<br>1分：基本按照要求传递信息（漏了一、两点次要信息、或添加了无关信息）；<br>0分：不能按照要求传递信息</td></tr><tr><td>语言</td><td>25%</td><td>0.5分：不影响理解所表达的信息；<br>0分：导致不能理解所表达的信息</td></tr></tbody></table><h3 id="宁波中考">7.3.2. 宁波中考</h3><p><a href="http://nbeea.nbedu.net.cn/ckfile/files/%E5%AE%81%E6%B3%A2%E5%B8%822022%E5%B9%B4%E5%88%9D%E4%B8%AD%E5%AD%A6%E4%B8%9A%E6%B0%B4%E5%B9%B3%E8%80%83%E8%AF%95%E8%8B%B1%E8%AF%AD%E5%90%AC%E5%8A%9B%E5%8F%A3%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%8C%96%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E.pdf">^宁波中考</a></p><table><thead><tr><th>分值</th><th>评分标准</th></tr></thead><tbody><tr><td>1</td><td>意思明白，表达清楚，语音、语调正确，词语、语法合乎规范。</td></tr><tr><td>0.5</td><td>意思基本明白，表达基本清楚，语音、语调基本正确，词语、语法有错误。</td></tr><tr><td>0</td><td>答非所问，或错误很多，不能达意。</td></tr></tbody></table><h3 id="其它版本">7.3.3. 其它版本</h3><ul><li>江苏中考（2档）<a href="https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf">^江苏中考</a>、深圳中考、衢州中考</li></ul><h3 id="维度分">7.3.4. 维度分</h3><p>驰声：内容、语法、发音、流利度</p><p>先声：完整度、发音、流利度，只建议展示总分</p><h1 id="故事复述">8. 故事复述</h1><p><a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></p><h2 id="题型说明-1">8.1. 题型说明</h2><p>Retelling(故事复述)，要求考生先听一段大约两分钟的独白，录音播放两遍。考生准备一分钟之后开始复述所听的内容。要求考生尽可能使用自己的语言复述，而且复述内容应涵盖尽可能多的原文信息点。选取的独白其体裁主要以记述文和议论文为主。</p><h2 id="示例-1">8.2. 示例</h2><p><font style="background: yellow">（300词左右）</font></p><p><strong>A Young Man’s Present</strong></p><p>A young man who lived in London was in love with a beautiful girl. Soon she became his girlfriend. The man was very poor while the girl was rich. The young man wanted to give her a present on her birthday. He wanted to buy something beautiful for her, but he had no idea how to do it, as he had very little money. The next morning he went to a shop. There were many fine things: rings, gold watches, diamonds — but all these things were too expensive. There was one thing he could not take his eyes off. It was a beautiful vase. That was a suitable present for his girlfriend. He had been looking at the vase for half an hour when the manager of the shop noticed him. The young man looked so pale, sad and unhappy that the manager asked what had happened to him.</p><p>The young man told him everything, The manager felt sorry for him and decided to help him. He came up with a good idea. The manager pointed to the corner of the shop. To his great surprise the young man saw a vase broken into many pieces. The manager said: “I can help you. I shall order my worker to pack it and take it to your girlfriend. When he enters the room, he will drop it.”</p><p>On the birthday of his girlfriend the young man was very excited.</p><p>Everything happened as had been planned. The worker brought in the vase, and as he entered the room, he dropped it. There was horror on everybody’s face. When the vase was unpacked the guests saw that each piece was packed separately.</p><h2 id="评分标准-1">8.3. 评分标准</h2><table><thead><tr><th></th><th>内容</th><th>语言</th><th>流利</th><th>语音</th></tr></thead><tbody><tr><td>权重</td><td>50%</td><td>16.7%</td><td>20.8%</td><td>12.5%</td></tr><tr><td>考察点</td><td>原文信息点被覆盖的比例</td><td>语法</td><td></td><td>语音语调</td></tr></tbody></table><p>考生不按话题规定内容表述或套背内容毫不相干的范文：0分<a href="%5B%E5%B9%BF%E8%A5%BF%E9%AB%98%E8%80%83-2021%EF%BC%88%E6%8B%9B%E7%94%9F%E8%80%83%E8%AF%95%E9%99%A2%EF%BC%89%5D(https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf)">^广西高考</a></p><h1 id="话题简述">9. 话题简述</h1><h1 id="信度、效度">10. 信度、效度</h1><ul><li>信度：多位专家打分，分数是否一致<ul><li>人工评分平均相关度、平均误差：计算评测员1与其它评测员的平均分的相关度、平均误差，作为评测员1的评分性能；以此类推；取多名评测员的平均作为人工评分性能</li></ul></li><li>效度：分数能否真实反映学生水平</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>英文口语评测功能</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/</url>
      
        <content type="html"><![CDATA[<h1 id="技术文档">1. 技术文档</h1><ul><li>科大讯飞：<a href="https://www.xfyun.cn/doc/Ise/IseAPI.html#%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E">https://www.xfyun.cn/doc/Ise/IseAPI.html#接口说明</a></li><li>驰声：<a href="https://www.chivox.com/opendoc/#/ChineseDoc/coreEn/">https://www.chivox.com/opendoc/#/ChineseDoc/coreEn/</a></li><li>先声：<a href="https://open.singsound.com/doc/engine?type=engine-en-en.word.score">https://open.singsound.com/doc/engine?type=engine-en-en.word.score</a></li></ul><h1 id="题型">2. 题型</h1><table><thead><tr><th>题型</th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音标朗读</td><td></td><td>√ 自定义文本音标</td><td>√</td></tr><tr><td>单词朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>单词纠音（音素识别）</td><td></td><td>√</td><td>√</td></tr><tr><td>句子朗读</td><td>√</td><td>√</td><td>√ 支持音频对比，返回音量、语调、语速相关度得分</td></tr><tr><td>句子纠音</td><td></td><td>√</td><td></td></tr><tr><td>篇章朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>背诵</td><td></td><td>√ 篇章朗读题型背诵模式，要求严格按顺序朗读</td><td>√</td></tr><tr><td>句子选读</td><td></td><td></td><td>√ 返回实际朗读的是第几个句子</td></tr><tr><td>自然拼读</td><td></td><td></td><td>√ <font style="background: green">（文本形式不明确）</font></td></tr><tr><td>选择</td><td>√</td><td>√ 支持单选、多选</td><td>√ <br>支持扩展选择题：用户可以在事先设定的固定答案基础上扩展表述；引擎检测到读的更像哪个答案，就会有对应的得分。支持设置错误关键词。<br>支持设置解码网络：常规、精简（解码速度快，但效果下降；当参考文本较多时，可以设置）</td></tr><tr><td>问答</td><td>√</td><td>√</td><td>√ 支持设置错误关键词</td></tr><tr><td>复述、口头翻译、看图说话、口头作文</td><td>√</td><td>√</td><td>√</td></tr><tr><td>自由识别</td><td></td><td>√</td><td></td></tr></tbody></table><h1 id="功能配置">3. 功能配置</h1><table><thead><tr><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>区分英美式发音</td><td></td><td>支持K12词汇</td><td>√</td></tr><tr><td>自定义音标</td><td>√ 支持指定数字的读法</td><td>√</td><td>√</td></tr><tr><td>集外词</td><td></td><td>√</td><td>√</td></tr><tr><td>人群定制</td><td>传入group（成人、中学、小学）、grade（年级，junior、middle、senior）</td><td>自适应少儿、成人群体</td><td>儿童单词、句子为单独的题型；看图说话、复述支持设置，影响打分松紧度</td></tr><tr><td>松紧调节</td><td>仅中文评测支持，3档</td><td>线性调节</td><td>朗读题 5档；问答题 0.8-1.5线性调节</td></tr><tr><td>实时评测</td><td></td><td>√</td><td>√</td></tr></tbody></table><h1 id="评测结果">4. 评测结果</h1><table><thead><tr><th></th><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音频级</td><td>总分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>发音准确度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>流畅度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>标准度&#x2F;韵律分</td><td>√</td><td>意群（sense）、重读、升降调</td><td>√ 意群、重读、升降调占比分别为50%、25%、25%</td></tr><tr><td></td><td>完整度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>语法分</td><td></td><td>√</td><td></td></tr><tr><td></td><td>识别文本</td><td>自由表述题</td><td>句子纠音：若错读、增读的单词在参考文本内，正常识别，否则标记为unk。<br>自由识别：识别文本带标点符号，支持逗号、句号、问号、感叹号</td><td></td></tr><tr><td></td><td>关键词&#x2F;要点命中</td><td></td><td>√</td><td>√</td></tr><tr><td>句子级</td><td>句末升降调检错</td><td></td><td>√</td><td>√</td></tr><tr><td>词级</td><td>评分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td>√ 不区分回读</td><td>正确、漏读、回读</td></tr><tr><td></td><td>浊化</td><td></td><td>√</td><td></td></tr><tr><td></td><td>连读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>失去爆破</td><td></td><td>√</td><td></td></tr><tr><td></td><td>重读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>意群停顿</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>字母-音素对应</td><td></td><td>√</td><td>√</td></tr><tr><td>音节级</td><td>评分</td><td>√</td><td></td><td>√</td></tr><tr><td></td><td>发音检错</td><td>√</td><td></td><td></td></tr><tr><td></td><td>重音检错</td><td>√ 检测重读音节是否重读</td><td>√ 检测音节是否重读</td><td>√</td></tr><tr><td>音素级</td><td>评分</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音检错</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音诊断</td><td></td><td>√ 音素识别</td><td></td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td></td><td></td></tr></tbody></table><ul><li>音标、单词朗读仅发音准确度维度</li><li>标准度&#x2F;韵律分：科大讯飞：文本单词数&gt;&#x3D;5时才有</li><li>语法分：仅自由表述题有</li><li>驰声<ul><li>选择题、AITalk：返回置信度得分，可由应用层根据题目难易设置阈值（通常为75）判断结果是否正确</li></ul></li><li>先声<ul><li>句子朗读：统计各音素出现的次数、平均发音得分</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>小妇人</title>
      <link href="/blog/books/xiao-fu-ren/"/>
      <url>/blog/books/xiao-fu-ren/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 书籍 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>和平战士</title>
      <link href="/blog/videos/he-ping-zhan-shi/"/>
      <url>/blog/videos/he-ping-zhan-shi/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 视频 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>定风波</title>
      <link href="/blog/words/ding-feng-bo/"/>
      <url>/blog/words/ding-feng-bo/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 文字 </category>
          
          <category> 诗词 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch</title>
      <link href="/blog/ji-qi-xue-xi/kuang-jia/pytorch/"/>
      <url>/blog/ji-qi-xue-xi/kuang-jia/pytorch/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 框架 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CTC</title>
      <link href="/blog/ji-qi-xue-xi/loss/ctc/"/>
      <url>/blog/ji-qi-xue-xi/loss/ctc/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> loss </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>音标</title>
      <link href="/blog/yu-yin/yu-yin-xue/yin-biao/"/>
      <url>/blog/yu-yin/yu-yin-xue/yin-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="符号表">1. 符号表</h1><h2 id="辅音">1.1. 辅音</h2><table><thead><tr><th>辅音</th><th>单词</th><th>音标</th></tr></thead><tbody><tr><td>p</td><td>pen</td><td>&#x2F;pen&#x2F;</td></tr><tr><td>b</td><td>bad</td><td>&#x2F;bæd&#x2F;</td></tr><tr><td>t</td><td>tea</td><td>&#x2F;tiː&#x2F;</td></tr><tr><td>d</td><td>did</td><td>&#x2F;dɪd&#x2F;</td></tr><tr><td>k</td><td>cat</td><td>&#x2F;kæt&#x2F;</td></tr><tr><td>ɡ</td><td>get</td><td>&#x2F;ɡet&#x2F;</td></tr><tr><td>tʃ</td><td>chain</td><td>&#x2F;tʃeɪn&#x2F;</td></tr><tr><td>dʒ</td><td>jam</td><td>&#x2F;dʒæm&#x2F;</td></tr><tr><td>f</td><td>fall</td><td>&#x2F;fɔːl&#x2F;</td></tr><tr><td>v</td><td>van</td><td>&#x2F;væn&#x2F;</td></tr><tr><td>θ</td><td>thin</td><td>&#x2F;θɪn&#x2F;</td></tr><tr><td>ð</td><td>this</td><td>&#x2F;ðɪs&#x2F;</td></tr><tr><td>s</td><td>see</td><td>&#x2F;siː&#x2F;</td></tr><tr><td>z</td><td>zoo</td><td>&#x2F;zuː&#x2F;</td></tr><tr><td>ʃ</td><td>shoe</td><td>&#x2F;ʃuː&#x2F;</td></tr><tr><td>ʒ</td><td>vision</td><td>&#x2F;ˈvɪʒn&#x2F;</td></tr><tr><td>h</td><td>hat</td><td>&#x2F;hæt&#x2F;</td></tr><tr><td>m</td><td>man</td><td>&#x2F;mæn&#x2F;</td></tr><tr><td>n</td><td>now</td><td>&#x2F;naʊ&#x2F;</td></tr><tr><td>ŋ</td><td>sing</td><td>&#x2F;sɪŋ&#x2F;</td></tr><tr><td>l</td><td>leg</td><td>&#x2F;leɡ&#x2F;</td></tr><tr><td>r</td><td>red</td><td>&#x2F;red&#x2F;</td></tr><tr><td>j</td><td>yes</td><td>&#x2F;jes&#x2F;</td></tr><tr><td>w</td><td>wet</td><td>&#x2F;wet&#x2F;</td></tr></tbody></table><h2 id="元音">1.2. 元音</h2><table><thead><tr><th>牛津</th><th>单词</th><th>音标</th><th>备注</th><th>朗文</th><th>-</th><th>-</th><th>-</th><th>cambridge</th><th>-</th><th>-</th><th>-</th><th>-</th></tr></thead><tbody><tr><td>ʌ</td><td>cup</td><td>&#x2F;kʌp&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɑː</td><td>father</td><td>&#x2F;ˈfɑːðə(r)&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɒ</td><td>got</td><td>&#x2F;ɡɒt&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔː</td><td>saw</td><td>&#x2F;sɔː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ə</td><td>about</td><td>&#x2F;əˈbaʊt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɜː</td><td>fur</td><td>&#x2F;fɜː(r)&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪ</td><td>sit</td><td>&#x2F;sɪt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>i</td><td>happy</td><td>&#x2F;ˈhæpi&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>iː</td><td>see</td><td>&#x2F;siː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊ</td><td>put</td><td>&#x2F;pʊt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>u</td><td>actual</td><td>&#x2F;ˈæktʃuəl&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>uː</td><td>too</td><td>&#x2F;tuː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>e</td><td>bed</td><td>&#x2F;bed&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>æ</td><td>cat</td><td>&#x2F;kæt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aɪ</td><td>my</td><td>&#x2F;maɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eɪ</td><td>say</td><td>&#x2F;seɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔɪ</td><td>boy</td><td>&#x2F;bɔɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aʊ</td><td>now</td><td>&#x2F;naʊ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>əʊ</td><td>go</td><td>&#x2F;ɡəʊ&#x2F;</td><td></td><td>British English</td><td>oʊ</td><td>note</td><td>American English</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪə</td><td>near</td><td>&#x2F;nɪə(r)&#x2F;</td><td>British English</td><td><del>British English</del></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eə</td><td>hair</td><td>&#x2F;heə(r)&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊə</td><td>pure</td><td>&#x2F;pjʊə(r)&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>ɒː</td><td>dog</td><td>American English</td><td>ɚ</td><td>mother</td><td></td><td>American English，轻音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>iə</td><td>peculiar</td><td></td><td>ɝ</td><td>worm</td><td></td><td>American English，重读音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>t̬</td><td>butter</td><td>&#x2F;ˈbʌt̬.ɚ&#x2F;</td><td>American English</td><td>[ɾ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>l̩</td><td>little</td><td>&#x2F;ˈlɪt.l̩&#x2F;</td><td></td><td>[ɫ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>ə</sup>l,<sup>ə</sup>m,<sup>ə</sup>n</td><td></td><td>&#x2F;leɪb.<sup>ə</sup>l&#x2F;</td><td></td><td>(ə)l …</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>r</sup></td><td>four apples</td><td>fɔː<sup>r</sup> + ˈæp.l ̩z &#x3D; fɔːˈræp.l ̩z</td><td>British English，在元音前时才发音</td><td>(r)</td></tr></tbody></table><h2 id="备注">1.3. 备注</h2><ul><li>&#x2F;i&#x2F;可以发成&#x2F;iː&#x2F;或&#x2F;ɪ&#x2F;或两者之间折中的音；&#x2F;u&#x2F;代表&#x2F;uː&#x2F;和&#x2F;ʊ&#x2F;之间的弱元音<ul><li>存在单词，含有两个仅&#x2F;iː&#x2F;、&#x2F;ɪ&#x2F;差异（或仅&#x2F;uː&#x2F;、&#x2F;ʊ&#x2F;差异）的音标，但不合并为&#x2F;i&#x2F;（或&#x2F;u&#x2F;），如 real &#x2F;ˈriːəl&#x2F;、&#x2F;rɪəl&#x2F; （room &#x2F;ruːm&#x2F; &#x2F;rʊm&#x2F;）</li><li>&#x2F;iə&#x2F;、&#x2F;uə&#x2F;：区别于&#x2F;ɪə&#x2F;、&#x2F;ʊə&#x2F;，如 alien &#x2F;ˈeɪ-li-ən&#x2F;、video &#x2F;ˈvɪ-di-əʊ&#x2F;，actual &#x2F;ˈæk-tʃu-əl&#x2F;、continuo &#x2F;kən-ˈtɪn-ju-əʊ&#x2F;，<a href="https://english.stackexchange.com/questions/433103/what-is-i%c9%99-in-english#">https://english.stackexchange.com/questions/433103/what-is-i%c9%99-in-english#</a></li></ul></li><li>&#x2F;ɒ&#x2F;只出现在英式英语中，美式英语的发音通常是&#x2F;ɔː&#x2F;或&#x2F;ɑː&#x2F;</li><li>(r)：只有紧跟的是下一个单词开头的元音时，英式发音才会出现&#x2F;r&#x2F;，如far away；否则省略&#x2F;r&#x2F;。对于美式英语来说，所有的&#x2F;r&#x2F;都应该发音。</li><li>&#x2F;l̩&#x2F; 、&#x2F;n̩&#x2F;、(&#x2F;m̩)&#x2F;：成音节，如final &#x2F;ˈfaɪnl&#x2F;，发音为[<sup>ə</sup>l] or [<sup>ə</sup>n]</li><li>音位变体<ul><li><p>&#x2F;t&#x2F;音素还包含闪音[ɾ]和glottal stop [ʔ]。</p><p>[ɾ] 发音像快速的&#x2F;d&#x2F;，美式发音，在很多拼写为-t-或-tt-的单词中，在元音或&#x2F;r&#x2F;之后、非重读元音或音节&#x2F;l&#x2F;之前，如city &#x2F;ˈsɪt̮ɪ &#x2F;; parting &#x2F;ˈpɑrt̮ɪŋ &#x2F;; little &#x2F;ˈlɪt̮l &#x2F;；</p><p>英美式发音有时会用glottal stop [ʔ] (声带短暂的闭合)来表达&#x2F;t&#x2F;，比如football &#x2F;ˈfʊtbɔːl&#x2F;和button &#x2F;ˈbʌtn&#x2F;。</p></li><li><p>&#x2F;l&#x2F;在元音之前或中间时（如 like）与在其他位置时（如 full [ɫ] ）发音不同</p></li><li><p>&#x2F;r&#x2F; [ɹ]如red</p></li></ul></li><li>&#x2F;x&#x2F;：摩擦音，如苏格兰的loch、爱尔兰的lough &#x2F;lɒx&#x2F;</li><li>˜：鼻元音，可能在某些源自法语的单词中保留，如penchant &#x2F;ˈpɒ̃ʃɒ̃&#x2F;</li><li>有的发音标注了强、弱形式，给出的第一个发音通常代表最常用的；但是当一个单词被强调时，应该采用strong form；当单词位于句子末尾时，也通常使用strong form。如can &#x2F;kən&#x2F;, strong form &#x2F;kæn&#x2F;</li><li>重读音节相对loud、持续时间长、发音清晰，并且可以通过音调被注意到。重读音节通常不包含弱元音&#x2F;ə&#x2F;、&#x2F;i&#x2F;或&#x2F;u&#x2F;。</li><li>英语音素 48 vs 44<blockquote><p>传统语音学认为：英语有48个音素。一个音素对应一个音标，所以共有48个国际音标。这也是我们国内更加熟知的一种音标体系。</p><p>现代语音学认为：英语有44个音素。因为现代语音学认为 &#x2F;tr&#x2F; &#x2F;dr&#x2F; &#x2F;ts&#x2F; &#x2F;dz&#x2F; 这四个不是独立的音素，而是辅音连缀。</p><p><a href="https://zhuanlan.zhihu.com/p/31484071">https://zhuanlan.zhihu.com/p/31484071</a></p></blockquote><blockquote><p>&#x2F;ts&#x2F;、&#x2F;dz&#x2F;、&#x2F;tr&#x2F;、&#x2F;dr&#x2F; 如果作为辅音连缀简单地连读，影响正确发音</p><p><a href="https://www.hjenglish.com/yinbiao/p776611/">https://www.hjenglish.com/yinbiao/p776611/</a></p></blockquote><ul><li>&#x2F;tr&#x2F;、&#x2F;dr&#x2F;<ul><li>分开读，连读影响重音：如 outrun &#x2F;ˌaʊtˈrʌn&#x2F;</li><li>连读：如 restroom &#x2F;ˈrestruːm&#x2F;、outrage &#x2F;ˈaʊtreɪdʒ&#x2F;、bedroom &#x2F;ˈbedruːm&#x2F;、handwriting &#x2F;ˈhændraɪtɪŋ&#x2F;</li></ul></li><li>&#x2F;ts&#x2F;、&#x2F;dz&#x2F;<ul><li>分开读，在不同音节：如 itself &#x2F;ɪtˈself&#x2F;、outside&#x2F;ˌaʊtˈsaɪd&#x2F;、outset &#x2F;ˈaʊtset&#x2F;、Watson &#x2F;ˈwɑːtsən&#x2F;、curtsy &#x2F;ˈkɜːtsi&#x2F;</li><li>连读，在同一音节：如 Pittsburg &#x2F;ˈpɪtsˌbərg&#x2F;、sportsman &#x2F;ˈspɔːtsmən&#x2F;、swordsman &#x2F;ˈsɔːdzmən&#x2F;</li><li>yangtze &#x2F;ˈjæŋtsi&#x2F;、Lindsey &#x2F;ˈlɪndzi&#x2F;</li></ul></li></ul></li></ul><h2 id="参考">1.4. 参考</h2><ul><li><a href="https://www.oxfordlearnersdictionaries.com/about/english/pronunciation_english">https://www.oxfordlearnersdictionaries.com/about/english/pronunciation_english</a></li><li><a href="https://www.oxfordlearnersdictionaries.com/about/pronunciation/_american_english">https://www.oxfordlearnersdictionaries.com/about/pronunciation\_american_english</a></li><li><a href="https://www.ldoceonline.com/howtouse.html">https://www.ldoceonline.com/howtouse.html</a></li><li><a href="https://dictionary.cambridge.org/help/phonetics.html">https://dictionary.cambridge.org/help/phonetics.html</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/%E8%8B%B1%E8%AA%9E%E5%9C%8B%E9%9A%9B%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/英語國際音標</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/DJ%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/DJ音標</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/KK%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/KK音標</a></li><li>音素、音标、国际音标（IPA）、DJ音标、KK音标、宽式标音等 <a href="https://en-yinbiao.xiao84.com/study">https://en-yinbiao.xiao84.com/study</a></li><li>音标特殊字符unicode编码：<a href="http://www.fmddlmyy.cn/text65.html">http://www.fmddlmyy.cn/text65.html</a> （注意ɡ、ː、ˈ、ˌ）</li></ul><h1 id="词典">2. 词典</h1><ul><li><p>主流英语词典：朗文、牛津、剑桥、柯林斯、韦式 <a href="https://www.jianshu.com/p/26d12a32f048">https://www.jianshu.com/p/26d12a32f048</a></p></li><li><p>朗文交际9000词 <a href="https://github.com/MuhammadYaseenKhan/Longman-Communication">https://github.com/MuhammadYaseenKhan/Longman-Communication</a></p></li><li><p>韦式词典</p><p><a href="https://www.merriam-webster.com/assets/mw/static/pdf/help/guide-to-pronunciation.pdf">https://www.merriam-webster.com/assets/mw/static/pdf/help/guide-to-pronunciation.pdf</a></p><p><a href="https://mdx.mdict.org/%E5%85%AD%E5%A4%A7%E7%9F%A5%E5%90%8D%E8%AF%8D%E5%85%B8/%E9%9F%A6%E6%B0%8F_Merriam-Webster/Merriam-Webster/Merriam-Webster/">https://mdx.mdict.org/六大知名词典/韦氏_Merriam-Webster/Merriam-Webster/Merriam-Webster/</a></p></li><li><p>数据堂英语发音词典：<a href="https://m.datatang.com/news/info/aboutus/451">https://m.datatang.com/news/info/aboutus/451</a></p></li></ul><h1 id="ARPAbet音素集">3. ARPAbet音素集</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3fda55097eb7eb054863211dc53ea73d?method=download&shareKey=fecd6d9867afa9c0ab9874d271e9ce46" alt="wiki"></p><p>元音区分是否重读，重音标记：0表示非重音，1表示主重音，2表示次重音</p><h2 id="arpabet-to-ipa">3.1. arpabet-to-ipa</h2><p><a href="https://github.com/wwesantos/arpabet-to-ipa/blob/master/src/App.php">https://github.com/wwesantos/arpabet-to-ipa/blob/master/src/App.php</a></p><h2 id="与48个音素的区别">3.2. 与48个音素的区别</h2><ul><li>无短元音&#x2F;ɒ&#x2F;，美式发音中多为&#x2F;ɑː&#x2F;或&#x2F;ɔː&#x2F;，如lot、long</li><li>无双元音&#x2F;ɪə&#x2F;、&#x2F;eə&#x2F;、&#x2F;ʊə&#x2F;，用两个单元音表示</li><li>无4个辅音连缀，用两个音标表示</li><li>&#x2F;ʌ&#x2F;、&#x2F;ə&#x2F;共用符号AH，美式英语中常为多发音，如but</li></ul><h2 id="CMU-Carnegie-Mellon-University-词典">3.3. CMU(Carnegie Mellon University)词典</h2><p><a href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict/">http://www.speech.cs.cmu.edu/cgi-bin/cmudict/</a></p><p>CMU词典用于北美英语，采用ARPAbet音素集，元音区分是否重读</p><p>39个音素：</p><p>元音(15)：AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW</p><p>辅音(24)：B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, P, R, S, SH, T, TH, V, W, Y, Z, ZH</p><h1 id="发音学习视频">4. 发音学习视频</h1><ul><li>《BBC音标教程》Alex <a href="https://www.bilibili.com/video/BV127411n7nj">https://www.bilibili.com/video/BV127411n7nj</a></li><li>《英语语音》屠蓓 <a href="https://www.bilibili.com/video/BV1EP4y1p7p3">https://www.bilibili.com/video/BV1EP4y1p7p3</a></li><li>《美语从头学：美语音标》赖世雄  <a href="https://www.bilibili.com/video/BV1qo4y1S7tY/">https://www.bilibili.com/video/BV1qo4y1S7tY/</a></li><li>英语兔 <a href="https://space.bilibili.com/483162496/channel/series">https://space.bilibili.com/483162496/channel/series</a></li></ul><h1 id="频谱">5. 频谱</h1><p><a href="https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html">https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GOP</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/gop/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/gop/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>序列区分性训练</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>解码器</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/jie-ma-qi/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/jie-ma-qi/</url>
      
        <content type="html"><![CDATA[<h1 id="参数">1. 参数</h1><ul><li>beam: 每一帧保留的token的cost 与当前帧最优cost的最大允许差值。越大，解码越慢，内存占用越大，但越准确。默认值16.0<font color="green">（即路径概率约为1.1e-07 &#x3D; 1&#x2F;exp(16) 倍）</font>。</li><li>max_active: 每一帧可以激活的最大状态数。越大，解码越慢，内存占用越大，但越准确。默认值7000。</li><li>min_active: 每一帧需要激活的最小状态数。默认值200。</li><li><a href="#lattice_beam">lattice_beam</a>: Lattice Decoder中，跳转弧到达当前最后一帧active token A的最优路径的cost与到达A的最优路径的cost的最大允许差值（对于当前最后一帧的多个active tokens，其一满足即可）。默认值6.0<font color="green">（即路径概率约为0.0025 &#x3D; 1&#x2F;exp(6) 倍）</font>。</li><li><a href="#beam_delta">beam_delta</a>: FasterDecoder中，用于根据前一帧估计当前帧的<code>adaptive_beam</code>。默认值0.5。</li><li>prune_interval: Lattice Decoder中，每隔多少帧裁剪过去帧的tokens。默认值20。</li><li><a href="#prune_scale">prune_scale</a>: Lattice Decoder中，若token的extra cost变大（假设上一次PruneActiveTokens时，该token计算extra cost对应的是当时最后一帧的token A，当A不在到达当前最后一帧的最优路径上时，该token的extra cost就会变大）（变化量，而非值）超过<code>lattice_beam * prune_scale</code>，则需要判断之前的帧是否需要裁剪。越大，则向前回溯得越少（内存与解码速度的均衡）。默认值0.1。</li><li><a href="#hash_ratio">hash_ratio</a>：FasterDecoder中，用于设置存储tokens的哈希表的容量。默认值2。</li><li>length_penalty：LatticeFasterDecoder中，每跳转到一个新的状态（排除了状态自旋、epsilon弧），graph cost加上length_penalty。&gt;0则倾向于短句子，&lt;0时倾向于长句子。默认值0.0。</li></ul><h1 id="数据结构">2. 数据结构</h1><ul><li>DecodableInterface: 为了最小化解码器和声学模型代码之间的交互，创建了该基类。DecodableInterface 对象可以视为LogLikelihood矩阵。<ul><li>BaseFloat LogLikelihood(int32 frame, int32 index): 返回索引(frame, index)对应的声学log-likelihood。若已计算，则直接返回值；否则调用声学模型计算。可能还包含减log_priors、乘acoustic_scale等处理。<ul><li>acoustic_scale：默认值0.1。</li></ul></li><li>int32 NumFramesReady(): 根据已计算的声学特征，可得到的LogLikelihood帧数（声学模型可能包含降采样、特征上下文等处理）。</li><li>bool IsLastFrame(int32 frame): 是否为最后一帧（若为online模式，则须输入结束）。</li></ul></li><li>Decoder<ul><li>构造decoder需要传入fst和相关配置参数。</li></ul></li><li>Token（以下主要以Lattice Decoder为例）<ul><li>每一帧、每个激活的状态各由一个Token表示。</li><li>BaseFloat tot_cost: 从句子开头到当前状态的AM + LM cost (ac_cost + graph_cost)。<ul><li>vector&lt;BaseFloat&gt; cost_offsets_: 每一帧，AM log-likelihoods 加的 offset，使其接近0，减小roundoff的误差。</li></ul></li><li>BaseFloat extra_cost: 认为当前最后一帧的各active token都有可能在最终的最优路径上，extra_cost均为0；其它帧token的extra cost为：经过该token到达当前最后一帧active token（计为A）的最优路径，与到达A的最优路径的cost差值；对于当前最后一帧的多个active tokens，取这些cost差值的最小值。在PruneForwardLinks(Final)中计算。</li><li>ForwardLink<stdtoken> *links: 该token到下一帧的跳转弧（或跳转到当前帧的epsilon弧）的单链表<ul><li>fst::StdArc::Label ilabel</li><li>fst::StdArc::Label olabel</li><li>BaseFloat acoustic_cost: cost_offset - logLikelihood</li><li>BaseFloat graph_cost: arc.weight，包含状态转移模型概率、词典发音概率、静音概率、LM概率等</li><li>Token *next_tok：跳转到的下一个token</li><li>ForwardLink *next：单链表结构</li></ul></stdtoken></li><li>Token *backpointer: 到达当前token的最优路径上的上一token。用于LatticeFasterOnlineDecoder中高效地GetBestPath。</li><li>Token *next: 该帧的下一token</li><li>非Lattice Decoder中还包含：<ul><li>int32 ref_count_: 该token跳转到的token的数量+1。当该token不再在到达当前最后一帧的各active token的最优路径上时（ref_count&#x3D;&#x3D;1），删除到达该token的路径上的所有无用token（TokenDelete）。</li><li>Token *prev_: 路径中的上一个token，唯一，用于回溯。<ul><li>Lattice Decoder记录路径中的下一token，链表，多个。<font color="green">更高效</font>。GetBestPath中，先顺序遍历每一帧的active tokens 生成lattice，再采用 openfst 的 ShortestPath 返回最优路径。</li></ul></li><li>arc_.nextstate 当前状态</li></ul></li></ul></li></ul><h1 id="解码器类型">3. 解码器类型</h1><h2 id="SimpleDecoder">3.1. SimpleDecoder</h2><ul><li>采用<code>unordered_map&lt;StateId, Token*&gt; cur_toks_, prev_toks_</code> 分别存储当前帧和前一帧的active tokens。</li><li>Viterbi beam search。</li></ul><p>核心流程如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (num_frames_decoded_ &lt; target_frames_decoded) &#123;    &#x2F;&#x2F; note: ProcessEmitting() increments num_frames_decoded_    ClearToks(prev_toks_);  &#x2F;&#x2F; 删除历史无用路径，并清空prev_toks_    cur_toks_.swap(prev_toks_);    ProcessEmitting(decodable);  &#x2F;&#x2F; 处理发射弧，将token从上一帧传播到当前帧    ProcessNonemitting();  &#x2F;&#x2F; 处理当前帧token的epsilon弧    PruneToks(beam_, &amp;cur_toks_);  &#x2F;&#x2F; 再次裁剪（因为ProcessEmitting中的cutoff是动态更新的）&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="FasterDecoder">3.2. FasterDecoder</h2><ul><li>采用<code>HashList&lt;StateId, Token*&gt; toks_</code> 存储tokens，其元素可以通过哈希表访问，也可以通过单链表访问。token传播时，先清空哈希表，前一帧的active tokens仅可以通过单链表访问，当前帧的active tokens还可以通过哈希表访问。<ul><li>创建新token时，需要查询对应的状态是否已存在token，相较于unordered_map的遍历查找，采用哈希表查找速度更快。</li></ul></li><li>采用<code>max-active</code>、<code>min-active</code> 调整裁剪。</li></ul><p>核心流程如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (num_frames_decoded_ &lt; target_frames_decoded) &#123;    double weight_cutoff &#x3D; ProcessEmitting(decodable);    ProcessNonemitting(weight_cutoff);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="BiglmFasterDecoder">3.3. BiglmFasterDecoder</h2><ul><li>大语言模型：弧数&gt;百万。由于内存限制，很难构建解码图。</li><li><font color="red"> 两种处理方式 </font><ul><li>先采用小LM生成lattice，再用大LM对lattice进行rescore。（小、大指ngram阶数）</li><li>采用“biglm” decoder：先采用小LM <code>G</code> 构建HCLG，解码时动态地compose 大LM $G^{\prime}$ 与小LM的差。$HCLG \circ G^{-} \circ G^{\prime}$，其中 $G^{-}$ 为对<code>G</code>中的权重取负数得到。<ul><li>实现：未采用通用的compose算法，而是采用 $(HCLG中的状态, G^{-} \circ G^{\prime}中的状态)$ 状态空间，查找指定状态上具有指定输入标签的弧。</li><li>缺点：解码较慢。<font color="green"> 相较于beam相同、但没有biglm部分的解码器，biglm decoder 耗时几乎为2倍 (疑问：对比时，普通解码器LM大小与biglm decoder中的<code>G</code>相当，还是与$G^{\prime}$相当？)</font>。</li></ul></li></ul></li><li>效果：大语言模型的HCLG &gt; biglm decoder &gt; rescore。<ul><li>rescore：采用小LM生成lattice时最优路径有可能被裁剪掉。</li><li>biglm decoder：<font color="green"> only updates the ‘good’ language model score every time it crosses a word（ChatGPT: 只有在完整的单词被解码时才会更新语言模型分，可以通过减少语言模型需要被评估的次数来加速解码，但是，与在每个声学帧或每个子单元上更新语言模型分相比，可能会导致稍微不太准确的解码结果。）</font></li></ul></li></ul><h2 id="Lattice-Decoder">3.4. Lattice Decoder</h2><ul><li>对比<ul><li>one-best解码：只保留到达active状态的最优路径。</li><li>lattice解码：若存在多条可行路径到达同一active状态，均保留。</li></ul></li><li>采用<code>vector&lt;TokenList&gt; active_toks_</code>记录每一帧的active tokens链表头节点。</li><li>采用lattice beam进一步裁剪。</li></ul><p>核心流程如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (NumFramesDecoded() &lt; target_frames_decoded) &#123;    if (NumFramesDecoded() % config_.prune_interval &#x3D;&#x3D; 0)      PruneActiveTokens(config_.lattice_beam * config_.prune_scale);    double weight_cutoff &#x3D; ProcessEmitting(decodable);    ProcessNonemitting(weight_cutoff);&#125;FinalizeDecoding();  &#x2F;&#x2F; 可选，采用lattice beam进行裁剪<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="LatticeFasterOnlineDecoder">3.5. LatticeFasterOnlineDecoder</h2><ul><li>支持在使用endpointing或其它需要频繁获取最佳路径的场景下，利用backpointer高效计算GetBestPath()。</li></ul><h2 id="LatticeIncrementalDecoder">3.6. LatticeIncrementalDecoder</h2><ul><li>普通解码器LatticeFasterDecoder，用于长语音实时应用时，每次获取lattice时的lattice确定化可能会花费相当多的时间，从而引入延迟。LatticeIncrementalDecoder将lattice确定化分散到整个解码过程中。</li><li><font color="green">TODO: 实现</font></li><li>术语<ul><li>chunk：对 chunk 帧原始lattice分别进行确定化。长度至少为1个单词，如20帧。</li></ul></li></ul><h1 id="代码解析">4. 代码解析</h1><h2 id="Decodable-NumFramesReady">4.1. Decodable::NumFramesReady()</h2><p>由于声学模型输入特征可能包含上下文，音频中间和末尾的NumFramesReady计算方式可能不一样。</p><h2 id="cost-cutoff-x3D-ProcessEmitting-decodable">4.2. cost_cutoff &#x3D; ProcessEmitting(decodable)</h2><p>将令牌从前一帧传播到当前帧。</p><ul><li>GetCutoff：计算上一帧的cutoff <font color="red">(cur_cutoff)</font>。<ul><li>返回值：同时满足<code>beam</code>、<code>max_active</code>、<code>min_active</code>约束的cost临界值。</li><li>adaptive_beam: 自适应估计的有效beam width。若根据<code>max_active</code>或<code>min_active</code>调整了beam，则为同时满足上述约束的$beam^{\prime}$ + <span id="beam_delta"> <code>beam_delta</code></span>，否则为<code>beam</code>。该值通常 &gt;&#x3D; 实际有效的beam width，认为采用beam_delta&#x3D;0.5（默认值）时 该估计值比实际值更严格的情况不会经常发生。</li></ul></li><li><span id="hash_ratio">PossiblyResizeHash(tok_cnt)：若上一帧的token数<code>tok_cnt</code> <code>* hash_ratio &gt; toks_.Size()</code>，<code>toks_</code>扩容。</span></li><li>初始化当前帧的cutoff <font color="red">(next_cutoff)</font>：取上一帧的best state，对其在fst上的各发射弧，计算cost，cutoff取上述cost的最小值 + adaptive_beam。<ul><li>cost_offsets_[frame] &#x3D; - best_elem-&gt;val-&gt;tot_cost (best_elem: 上一帧的)</li></ul></li><li>对于前一帧的各active token (tok)，若满足不被裁剪的条件 <font color="red">（cur_cutoff）</font>，处理其对应状态的各发射弧（即输入标签非0 - epsilon），若满足不被裁剪的条件 <font color="red">（next_cutoff）</font>，则FindOrAddToken，并将新ForwardLink添加至tok-&gt;links头部。动态更新当前帧cutoff。<ul><li>FindOrAddToken：若当前帧已存在与该状态关联的token，判断是否更新tot_cost、backpointer。</li></ul></li></ul><h2 id="ProcessNonemitting-cost-cutoff">4.3. ProcessNonemitting(cost_cutoff)</h2><ul><li>对于当前帧满足cost_cutoff约束的各token，处理其对应状态的各非发射弧（即输入标签为0 - epsilon）。若仍满足cost_cutoff约束，则FindOrAddToken，并将新ForwardLink添加至tok-&gt;links头部。</li><li>采用队列（queue）遍历含epsilon弧的token。若FindOrAddToken中新添加的token 或 更新了tot_cost的token 也含Epsilon弧，也加入queue。<ul><li>DeleteForwardLinks(tok)：若 tok 存在 ForwardLinks，则一定为 epsilon 弧，且再次在queue中，则 tot_cost 减小了，因此DeleteForwardLinks，并重新处理其所有的 epsilon 弧。</li></ul></li></ul><h2 id="PruneActiveTokens">4.4. PruneActiveTokens</h2><p>逆序遍历每一帧（不包含当前帧），如果该帧还未被该函数访问过(new TokenList)，或者下一帧存在token extra_cost变化超过<span id="prune_scale"> delta （&#x3D; config_.lattice_beam * config_.prune_scale）</span>，对该帧token的forward link进行裁剪（若 link_extra_cost &gt; <span id="lattice_beam"> config_.lattice_beam</span>），并删除下一帧 forward link 均被裁剪了的 token。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">&#x2F;&#x2F; delta &#x3D; config_.lattice_beam * config_.prune_scalevoid PruneActiveTokens(BaseFloat delta) &#123;    int32 cur_frame_plus_one &#x3D; NumFramesDecoded();    &#x2F;&#x2F; 保留当前最后一帧的所有token，认为其均有可能走到最后(extra cost均为0)。    for (int32 f &#x3D; cur_frame_plus_one - 1; f &gt;&#x3D; 0; f--) &#123;        &#x2F;&#x2F; TokenList初始化时must_prune_forward_links默认为true        &#x2F;&#x2F; 或者，下一帧存在token extra_costs_changed        if (active_toks_[f].must_prune_forward_links) &#123;            bool extra_costs_changed &#x3D; false, links_pruned &#x3D; false;            PruneForwardLinks(f, &amp;extra_costs_changed, &amp;links_pruned, delta);            if (extra_costs_changed &amp;&amp; f &gt; 0)  &#x2F;&#x2F; 存在token extra_cost变化超过delta                active_toks_[f-1].must_prune_forward_links &#x3D; true;            if (links_pruned)  &#x2F;&#x2F; 存在link被裁剪                active_toks_[f].must_prune_tokens &#x3D; true;            active_toks_[f].must_prune_forward_links &#x3D; false;        &#125;        &#x2F;&#x2F; 必须先裁剪掉前一帧的forward links，再裁剪token，从而避免“悬挂”的forward links        if (f+1 &lt; cur_frame_plus_one &amp;&amp; active_toks_[f+1].must_prune_tokens) &#123;            &#x2F;&#x2F; 删除extra_cost为无穷的token            PruneTokensForFrame(f+1);            active_toks_[f+1].must_prune_tokens &#x3D; false;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>PruneForwardLinks:</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">&#x2F;&#x2F; 遍历该帧所有token，若extra_cost **变化** 均不超过delta，循环停止。&#x2F;&#x2F; 由于存在epsilon弧，而链表不保证拓扑顺序，所以需要while循环。while (changed) &#123;    changed &#x3D; false;    &#x2F;&#x2F; 对该帧的所有token    for (Token *tok &#x3D; active_toks_[frame_plus_one].toks; tok !&#x3D; NULL; tok &#x3D; tok-&gt;next) &#123;        ForwardLinkT *link, *prev_link &#x3D; NULL;        BaseFloat tok_extra_cost &#x3D; std::numeric_limits&lt;BaseFloat&gt;::infinity();        &#x2F;&#x2F; 对该token的所有link        for (link &#x3D; tok-&gt;links; link !&#x3D; NULL; ) &#123;            Token *next_tok &#x3D; link-&gt;next_tok;            &#x2F;&#x2F; a &#x3D; tok-&gt;tot_cost + link-&gt;acoustic_cost + link-&gt;graph_cost 经过该link到达next_tok的tot_cost            &#x2F;&#x2F; b &#x3D; a - next_tok-&gt;tot_cost 上述tot_cost与到达next_tok的最优路径的cost差值（不会随着后续帧变化）            &#x2F;&#x2F; link_extra_cost &#x3D; next_tok-&gt;extra_cost + b 经过该link到达当前最后一帧active token（计为A）的最优路径，与到达A的最优路径的cost差值            BaseFloat link_extra_cost &#x3D; next_tok-&gt;extra_cost + ((tok-&gt;tot_cost + link-&gt;acoustic_cost + link-&gt;graph_cost) - next_tok-&gt;tot_cost);            &#x2F;&#x2F; if ... 则删除该link            if (link_extra_cost &gt; config_.lattice_beam) &#123;                ForwardLinkT *next_link &#x3D; link-&gt;next;                if (prev_link !&#x3D; NULL) prev_link-&gt;next &#x3D; next_link;                else tok-&gt;links &#x3D; next_link;                delete link;                link &#x3D; next_link;                *links_pruned &#x3D; true;            &#125; else &#123;                if (link_extra_cost &lt; tok_extra_cost)                    tok_extra_cost &#x3D; link_extra_cost;                prev_link &#x3D; link;                link &#x3D; link-&gt;next;            &#125;        &#125;        &#x2F;&#x2F; tok_extra_cost: 若该token的所有forward link均被裁剪，则为无穷大；否则为link_extra_cost的最小值        &#x2F;&#x2F; 若存在token extra_cost变化量超过delta，则changed为true        if (fabs(tok_extra_cost - tok-&gt;extra_cost) &gt; delta)            changed &#x3D; true;        tok-&gt;extra_cost &#x3D; tok_extra_cost;    &#125;    if (changed) *extra_costs_changed &#x3D; true;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>通常每一帧有数千个active tokens，因此定期（prune_interval）采用lattice_beam 进行裁剪，而不是等到话语结束才裁剪，否则会OOM。</li><li>裁剪算法的时间复杂度接近线性，而不是$O(T^2)$。对于非常长的语句，通常仅向后裁剪1-2s。另外，绝大多数token在该算法第一次访问时就被删除了。</li><li><a href="https://kaldi-asr.org/doc/lattices.html#lattices_generation">https://kaldi-asr.org/doc/lattices.html#lattices_generation</a></li></ul></blockquote><h2 id="FinalizeDecoding">4.5. FinalizeDecoding</h2><p>采用 lattice beam 进行裁剪。与 PruneActiveTokens 的区别：</p><ul><li>PruneForwardLinksFinal：最后一帧特殊处理。<ul><li><font color="red">若存在token在final状态，则不在final状态的token extra_cost 为无穷大；否则，不考虑final权重。</font></li><li>若 link_extra_cost（epsilon弧） &gt; lattice_beam，则删除该link。</li><li>token 的 extra cost 为 经过该token的最优路径 与 全局最优路径 的cost差值。若 &gt; lattice_beam，则设为无穷大。</li></ul></li><li>逆序遍历、处理所有帧，不需考虑 delta、must_prune_tokens 等条件。</li></ul><h2 id="GetBestPath">4.6. GetBestPath</h2><ul><li><font color="red">参数：use_final_probs。若为true，且存在token在final状态，则仅考虑所有在final状态的token；否则，不考虑final权重。</font></li><li>非lattice解码器：从cost最小的token开始回溯，返回线性FST，<font color="green">最后调用RemoveEpsLocal</font>。</li><li>lattice 解码器<ul><li>GetRawLattice：根据active_toks_中每一帧的active tokens，及token间的forwardlink，生成FST。<ul><li><font color="green">TopSortTokens：对一帧的token进行拓扑排序，使得生成的fst上，epsilon弧的起始状态的id &lt; 结束状态的id。</font></li></ul></li><li>ShortestPath：调用 OpenFst 的 ShortestPath 算法。</li><li>由于epsilon弧，弧数可能比帧数多。</li></ul></li></ul><h2 id="GetLattice">4.7. GetLattice</h2><ul><li><font color="green">TODO: 实现</font></li><li>lattice 确定化：只保留每个单词序列的最优路径。确定化前的lattice可用于声学rescore，很大；确定化后的lattice 可用于计算句子级的置信度分、LM rescore。</li></ul><h1 id="参考资源">5. 参考资源</h1><ul><li><a href="https://kaldi-asr.org/doc/decoders.html">https://kaldi-asr.org/doc/decoders.html</a></li><li><a href="https://kaldi-asr.org/doc/lattices.html">https://kaldi-asr.org/doc/lattices.html</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/yu-yan-mo-xing/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/yu-yan-mo-xing/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
