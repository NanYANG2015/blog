<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>语音搜题</title>
      <link href="/blog/llm/yu-yin-dui-hua/ye-wu/yu-yin-sou-ti/"/>
      <url>/blog/llm/yu-yin-dui-hua/ye-wu/yu-yin-sou-ti/</url>
      
        <content type="html"><![CDATA[<ul><li>LLM 直接处理音频输入的好处<ul><li>缓解 ASR 错误传播，相较于 ASR 模型小的 decoder，LLM语义理解能力更强，且音频输入的训练可使得 LLM适应联想发音相近的词，在隐空间自动纠偏。</li><li>对听感模糊的关键词要求澄清。</li><li>该场景副语言信息用处？</li></ul></li><li>LLM 直接处理音频输出的好处<ul><li>TTS 模块需针对学科特殊符号进行优化，且需要适应 LLM的输出文本形式。可直接端到端优化，简化流程。</li><li>注意：同时需要文本输出，满足多感官学习需求。可进一步扩展图像、动画、视频输出。</li></ul></li><li>模型训练<ul><li>声学表示对齐：儿童语音识别。</li><li>线上数据微调：适应业务音频、用户口语不连贯的表达、回复应简短易懂、鼓励式的对话风格。</li><li>强化学习：优化 badcase，对齐用户偏好。</li></ul></li><li>评价指标：准确率、对话轮次、平均解决时长、用户满意度、安全性。<ul><li>由于为多轮对话形式，难以采用固定的用户输入与回复形成连贯的对话，来评价系统性能。</li><li>产品侧可利用奖励手段鼓励用户参与反馈，如是否解决了你的问题，解决后即开启新对话，便于统计平均解决时长、准确率和满意度。（回复有反问时不触发）</li><li>ASRWER：可作为参考，但无法完全对齐产品体验，因为非关键词的识别对回复准确率可能影响不大。</li></ul></li><li>年龄段差异化设计：<ul><li>低龄儿童（6-9岁）：优先卡通音色+游戏化激励</li><li>高龄学生（10+岁）：侧重精准答疑+学习资源推荐</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 业务 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spectron</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/spectron/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/spectron/</url>
      
        <content type="html"><![CDATA[<ul><li>Nachmani E, Levkovitch A, Hirsch R, et al. Spoken Question Answeringand Speech Continuation Using Spectrogram-Powered LLM[J]. arXiv preprintarXiv:2305.15255, 2023.</li><li>作者来自谷歌等</li><li><a href="https://michelleramanovich.github.io/spectron/spectron/" class="uri">https://michelleramanovich.github.io/spectron/spectron/</a></li><li><font color="red">创新点：提出 speech encoder + projector + LLM +projector（输出频谱） + vocoder 的端到端 spoken languagemodel，直接输入、输出声谱图。通过给定 3s的频谱前缀，生成文本转写、文本延续、频谱延续的方式训练，在单次生成中实现跨模态的思维链。文本部分采用交叉熵损失，不需要语音-文本时间对齐；语音部分采用<span class="math inline">\(\ell_1\)</span>、<span class="math inline">\(\ell_2\)</span>重建损失，且考虑声谱图时间、频率维度的 K阶差分（出发点：差分提供了信号形状的丰富、长时的信息）。</font></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5edb5261b19f594b8413aa3d6f92a1df?method=download&shareKey=2f9becac71cc69706e99c0ee5c6c4069" width="1202"></p><ul><li>speech encoder：Google USM Conformer encoder，600M，在 1200wh数据集上预训练。</li><li>LLM：PaLM 2，350M 或1B，前者用于语义和声学质量评价，后者用于语音问答评价。</li><li>Pre-net、Post-net 均为两层 MLP。Pre-net用于频谱维度压缩，<font color="green">有助于防止重复生成。</font></li><li>vocoder：WaveFit，冻结。</li></ul><h1 id="训练">2. 训练</h1><p><span class="math display">\[\begin{aligned}\Delta_k^{\text {time }}(z) &amp;= z_{[1: T-k,:]}-z_{[k: T,:]},\Delta_k^{\text {feat }}(z)=z_{[:, 1: F-k]}-z_{[:, k: F]} \\\mathcal{L}_{1+2}\left(z, z^{\prime}\right) &amp;=\left\|z-z^{\prime}\right\|_1+\left\|z-z^{\prime}\right\|_2^2 \\\mathcal{L}_{\mathrm{s}}\left(x_c, \hat{x}_c\right) &amp;=\mathcal{L}_{1+2}\left(x_c, \hat{x}_c\right), \\\mathcal{L}_{\mathrm{f}}\left(x_c, \hat{x}_c\right) &amp;=\mathcal{L}_{1+2}\left(\Delta_1^{\mathrm{feat}}\left(x_c\right),\Delta_1^{\text {feat }}\left(\hat{x}_c\right)\right) \\\mathcal{L}_{\mathrm{t}}\left(x_c, \hat{x}_c\right) &amp; =\sum_{k=1}^K\mathcal{L}_{1+2}\left(\Delta_k^{\text {time }}\left(x_c\right),\Delta_k^{\text {time }}\left(\hat{x}_c\right)\right) \\\mathcal{L}_{\text {Recon. }}\left(x_c, \hat{x}_c\right)&amp;=\mathcal{L}_{\mathrm{s}}\left(x_c,\hat{x}_c\right)+\mathcal{L}_{\mathrm{f}}\left(x_c,\hat{x}_c\right)+\mathcal{L}_{\mathrm{t}}\left(x_c, \hat{x}_c\right) \\\mathcal{L}_{\text {total }}(x, y) &amp;= \mathcal{L}_{\mathrm{CE}}(y,\hat{y})+\lambda_r \mathcal{L}_{\text {Recon. }}\left(x_c,\hat{x}_c\right)\end{aligned}\]</span> 其中 <span class="math inline">\(z\)</span>为维度为 <span class="math inline">\(T \times F\)</span> 的 tensor。</p><ul><li>训练集：Libri-Light，采用 NST 模型转写。</li><li>文本转写+文本延续首尾添加 <code>&lt;sos&gt;</code> 和<code>&lt;eos&gt;</code>。</li><li>除 vocoder 冻结外，所有参数都被更新，包括语音编码器。</li></ul><h1 id="评价">3. 评价</h1><ul><li><p>语义连贯性：采用 Google USM 转写语音延续，采用 GPT-2 medium 和transformers 库计算 log-perplexity。</p></li><li><p>Naturalness Mean Opinion Score (N-MOS)：人工标注</p></li><li><p>说话人一致性：PnG-NAT TTS 模型说话人 embedding余弦相似度。</p></li><li><p>语音问答</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBad20fa30cda1fb33c24c55438e05e99a?method=download&shareKey=2f9becac71cc69706e99c0ee5c6c4069" width="510"></p></li><li><p>消融实验</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9526985e3ccbb53e3dc322aba4cbf6a7?method=download&shareKey=2f9becac71cc69706e99c0ee5c6c4069" width="381"></p></li><li><p><font color="red">局限性</font></p><ul><li><font color="red">由于频谱帧率为12.5ms，生成的时间和空间复杂度较高。可能的解决方案：同时生成多帧频谱。</font></li><li><font color="red">文本和频谱生成未并行，引入了延迟。</font></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FSQ</title>
      <link href="/blog/ji-qi-xue-xi/xiang-liang-liang-hua/fsq/"/>
      <url>/blog/ji-qi-xue-xi/xiang-liang-liang-hua/fsq/</url>
      
        <content type="html"><![CDATA[<ul><li><p>Mentzer F, Minnen D, Agustsson E, et al. Finite scalarquantization: Vq-vae made simple[J]. arXiv preprint arXiv:2309.15505,2023.</p></li><li><p>作者来自谷歌</p></li><li><p><font color="red">创新点：有限标量量化（Finite ScalarQuantization,FSQ）通过将高维表示投影到低维空间，对低维表示的每一维应用有界函数并四舍五入取整，得到量化后的表示。在大多数模型中实现了接近100％ 的码本利用率，而 VQ 存在大型码本利用率低的问题。尽管 FSQ的设计更为简单，但其性能与 VQ相当，且不需要复杂的辅助损失和训练机制。</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb62614282a3711177d5be8a7568294db?method=download&shareKey=34c601e9d2c4078f842027de60a83d1c" width="447"></p></li><li><p>超参数：通道数 <span class="math inline">\(d\)</span>、每个通道的级别数 <span class="math inline">\(L_i\)</span>，有界函数如 <span class="math inline">\(\lfloor L / 2\rfloor \tanh\left(z_i\right)\)</span>。码本大小为 <span class="math inline">\(|\mathcal{C}|=\prod_{i=1}^d L_i\)</span>。推荐采用<span class="math inline">\(L_i \geq 5 \foralli\)</span>。无需学习的参数。</p><p><font color="red">推荐配置</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB12915bcb0f92e98ab7d9f86bbbcfc52f?method=download&shareKey=34c601e9d2c4078f842027de60a83d1c" width="785"></p></li><li><p>码本扩展带来的收益递减：采用FSQ，模型性能随码本大小扩展而改善，但有上限，会趋于饱和，随码本扩展建模难度增加。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe759f4bae5e9cdb90b70762894a20512?method=download&shareKey=34c601e9d2c4078f842027de60a83d1c" width="542"></p></li><li><p><font color="green">存疑：论文附录 A.3 码本可视化，单个 code不学习抽象的概念，生成内容由解码器决定？</font></p></li><li><p><a href="https://github.com/google-research/google-research/tree/master/fsq">JAX实现</a>、<a href="https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py">PyTorch实现</a></p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def bound(self, z, eps: float &#x3D; 1e-3):    # 对于奇数 level，取整后取值范围为 [-(level - 1)&#x2F;2, (level - 1)&#x2F;2]，如[-2, -1, 0, 1, 2]    # 对于偶数 level，取整后取值范围为 [-level&#x2F;2, level&#x2F;2 - 1]，如[-3, -2, -1, 0, 1, 2]    half_l &#x3D; (self._levels - 1) * (1 + eps) &#x2F; 2  # eps 的作用?    offset &#x3D; torch.where(self._levels % 2 &#x3D;&#x3D; 0, 0.5, 0.0)  # 对于奇数 level，offset&#x3D;0；对于偶数 level，offset&#x3D;0.5    shift &#x3D; (offset &#x2F; half_l).atanh()  # 对于奇数 level，shift&#x3D;0；对于偶数 level，确定 bound 取值范围时 shift 不起作用？    return (z + shift).tanh() * half_l - offset  # 第一项取值范围 (-(level - 1)&#x2F;2, (level - 1)&#x2F;2)，偶数 level 向下 offset 0.5def quantize(self, z):    # case: _levels &#x3D; [5,6,6]    half_width &#x3D; self._levels &#x2F;&#x2F; 2  # [2,3,3]    quantized &#x3D; round_ste(self.bound(z)) &#x2F; half_width  # 除以 half_width: 将量化表示各维的值重新归一化到 [-1,1]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>梯度传播：采用 straight-through estimator (STE，直通估计器) stopgradient operation</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def round_ste(z: Tensor) -&gt; Tensor:    &quot;&quot;&quot;Round with straight through gradients.&quot;&quot;&quot;    zhat &#x3D; z.round()    return z + (zhat - z).detach()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>其它</p><ul><li>量化的作用：数据压缩和语义化</li><li>VQ 训练通常采用的辅助机制<ul><li>commitment losses:量化前后表示的距离，用于鼓励原始表示与量化表示接近。</li><li>entropy loss:鼓励增加码本的熵、码本元素均匀分布，以提高利用率。</li><li>codebook reset: 将低频使用的码本元素替换为新的随机向量。</li><li>codebook splitting:如训练中检测到低频使用的码字后，将最常用的码字加噪分裂为两个新的码字。</li></ul></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 向量量化 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>位置编码</title>
      <link href="/blog/ji-qi-xue-xi/transformer/wei-zhi-bian-ma/"/>
      <url>/blog/ji-qi-xue-xi/transformer/wei-zhi-bian-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="正弦位置编码">1. 正弦位置编码</h1><ul><li>提出：Vaswani A. Attention is all you need[J]. Advances in NeuralInformation Processing Systems, 2017.</li></ul><p><span class="math display">\[ \begin{aligned}P E_{(pos, 2 i)} &amp; =\sin \left(pos / 10000^{2 i / d_{\text{model}}}\right) \\P E_{(pos, 2 i+1)} &amp; =\cos \left(pos / 10000^{2 i / d_{\text{model}}}\right)\end{aligned} \]</span> 每一维的位置编码为正/余弦函数。周期为 <span class="math inline">\(2 \pi \cdot 10000^{2 i / d_{\text{model}}}\)</span>，从 <span class="math inline">\(2 \pi\)</span> 增长到<span class="math inline">\(2 \pi \cdot 10000\)</span>。</p><p>示例: <span class="math inline">\(d_{\text {model}}=512\)</span><span class="math display">\[\overrightarrow{\mathrm{PE}}_{\mathrm{pos}}=\left[\begin{array}{c}\sin \left(\operatorname{pos} / 10000^{2 \times \frac{0}{d_{\text{model}}}}\right) \\\cos \left(\operatorname{pos} / 10000^{2 \times \frac{0}{d_{\text{model}}}}\right) \\\vdots \\\sin \left(\operatorname{pos} / 10000^{2 \times \frac{d_{\text{model}}/2-1}{d_{\text {model}}}}\right) \\\cos \left(\operatorname{pos} / 10000^{2 \times \frac{d_{\text{model}}/2-1}{d_{\text {model}}}}\right)\end{array}\right]_{\mathrm{d}_{\text {model }} \times 1}\approx\left[\begin{array}{c}\sin (\mathrm{pos}) \\\cos (\mathrm{pos}) \\\sin (0.964662 \cdot \operatorname{pos}) \\\cos (0.964662 \cdot \operatorname{pos}) \\\vdots \\\sin (0.000104 \cdot \operatorname{pos}) \\\cos (0.000104 \cdot \operatorname{pos})\end{array}\right]_{\mathrm{d}_{\text {model }} \times 1}\]</span> &gt; 注意：每一维的周期约为 [<span class="math inline">\(2\pi\)</span>, <span class="math inline">\(2 \pi \cdot 1.037\)</span>,<span class="math inline">\(2 \pi \cdot 1.075\)</span>, ...]，不为 <span class="math inline">\(2 \pi\)</span>的整数倍，所以正弦位置编码的有效范围不是 <span class="math inline">\(2\pi \cdot 10000\)</span>，约为 62.8k。</p><ul><li><p>相对位置信息：<font color="green"><a href="https://arxiv.org/abs/1706.03762">we hypothesized it would allowthe model to easily learn to attend by relative positions, since for anyfixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linearfunction of <span class="math inline">\(PE_{pos}\)</span>.</a></font></p><blockquote><p><font color="green">如何在 attention score中体现相对位置的影响？</font></p></blockquote></li><li><p>长度外推：<font color="green"><a href="https://arxiv.org/abs/1706.03762">We chose the sinusoidal versionbecause it may allow the model to extrapolate to sequence lengths longerthan the ones encountered during training.</a></font></p><blockquote><p><font color="green">由于正/余弦函数的周期性、平滑性，使得模型能够适应与推断？是否有更直接的解释？</font></p></blockquote></li><li><p>实验对比了可学习的位置编码，模型性能几乎相同。</p></li></ul><h1 id="相对位置-aware-self-attention">2. 相对位置 awareself-attention</h1><ul><li><p>提出：Shaw P, Uszkoreit J, Vaswani A. Self-attention withrelative position representations[J]. arXiv preprint arXiv:1803.02155,2018.</p></li><li><p>原理</p><p><font color="red">通过在 K、V 表示上加可学习的相对位置表示，在attention计算中引入相对位置信息，提高模型性能。</font>并将相对位置限制为 [-k,k]，假设超过一定距离后，精确的相对位置信息是无用的；还使模型能够泛化到训练期间未见过的序列长度。通过跨头、跨序列共享相对位置表示，降低增加的空间复杂度。</p><p><span class="math display">\[ \begin{aligned}e_{i j}=\frac{\left(x_i W^Q\right)\left(x_j W^K\right)^T}{\sqrt{d_z}}&amp;=&gt; e_{i j}=\frac{x_i W^Q\left(x_j W^K+a_{ij}^K\right)^T}{\sqrt{d_z}} \\\alpha_{i j}&amp;=\frac{\exp e_{i j}}{\sum_{k=1}^n \exp e_{i k}} \\z_i=\sum_{j=1}^n \alpha_{i j}\left(x_j W^V\right) &amp;=&gt;z_i=\sum_{j=1}^n \alpha_{i j}\left(x_j W^V+a_{i j}^V\right) \\a_{i j}^K &amp; =w_{\operatorname{clip}(j-i, k)}^K \\a_{i j}^V &amp; =w_{\operatorname{clip}(j-i, k)}^V \\\operatorname{clip}(x, k) &amp; =\max (-k, \min (k, x))\end{aligned}\]</span></p></li></ul><h1 id="rope">3. RoPE</h1><ul><li><p>提出：Su J, Ahmed M, Lu Y, et al. Roformer: Enhanced transformerwith rotary position embedding[J]. 2021.</p></li><li><p>作者苏剑林的博客 <a href="https://kexue.fm/archives/8265" class="uri">https://kexue.fm/archives/8265</a></p></li><li><p>原理：找到一个关于向量 <span class="math inline">\(q\)</span>及其位置 <span class="math inline">\(m\)</span> 的位置编码函数 <span class="math inline">\(f(\mathbf{q}, m)\)</span>，使得 <span class="math inline">\(f(\mathbf{q}, m)\)</span> 与 <span class="math inline">\(f(\mathbf{k}, n)\)</span> 的内积仅与 <span class="math inline">\(\mathbf{q}\)</span>、<span class="math inline">\(\mathbf{k}\)</span>、<span class="math inline">\(m-n\)</span> 有关。 <span class="math display">\[\boldsymbol{f}(\boldsymbol{q}, m)=\boldsymbol{q}e^{\mathrm{i} m \theta}\]</span></p><details><summary><p>推导</p></summary><p><font color="red">初始化 <span class="math inline">\(\boldsymbol{f}(\boldsymbol{q},0)=\boldsymbol{q}\)</span></font></p><p>借助复数形式来求解，将函数表示为 <span class="math inline">\(\boldsymbol{f}(\boldsymbol{q},m)=R_f(\boldsymbol{q}, m) e^{\mathrm{i} \Theta_f(\boldsymbol{q},m)}\)</span></p><p>根据复数内积公式 <span class="math inline">\(\left\langle z_1,z_2\right\rangle=r_1 r_2e^{i\left(\theta_1-\theta_2\right)}\)</span></p><p>令 <span class="math inline">\(n=m\)</span>，则</p><p><span class="math inline">\(R_f(\boldsymbol{q}, m)R_f(\boldsymbol{k}, m)=R_f(\boldsymbol{q}, 0) R_f(\boldsymbol{k},0)=\|\boldsymbol{q}\|\|\boldsymbol{k}\|\)</span>。<font color="red">令<span class="math inline">\(R_f(\boldsymbol{q},m)=\|\boldsymbol{q}\|\)</span>。</font></p><p><span class="math inline">\(\Theta_f(\boldsymbol{q},m)-\Theta_f(\boldsymbol{k}, m)=\Theta_f(\boldsymbol{q},0)-\Theta_f(\boldsymbol{k},0)=\Theta(\boldsymbol{q})-\Theta(\boldsymbol{k})\)</span>。则 <span class="math inline">\(\Theta_f(\boldsymbol{q},m)-\Theta(\boldsymbol{q})=\Theta_f(\boldsymbol{k},m)-\Theta(\boldsymbol{k})\)</span>。所以 <span class="math inline">\(\Theta_f(\boldsymbol{q},m)-\Theta(\boldsymbol{q})\)</span> 应该是一个只与 <span class="math inline">\(m\)</span> 相关、与 <span class="math inline">\(\boldsymbol{q}\)</span> 无关的函数，记为 <span class="math inline">\(\varphi(m)\)</span>。</p><p><span class="math inline">\(\varphi(m) - \varphi(m-1) =\Theta_f(\boldsymbol{q}, m) - \Theta_f(\boldsymbol{k}, m-1) -\Theta(\boldsymbol{q}) +\Theta(\boldsymbol{k})\)</span>。根据期望的特性，前两项的结果仅与 <span class="math inline">\(\boldsymbol{q}\)</span>、<span class="math inline">\(\boldsymbol{k}\)</span>、1相关，后两项为常量，因此 <span class="math inline">\(\{\varphi(m)\}\)</span>为等差数列。<font color="red">令 <span class="math inline">\(\varphi(m)= m \theta\)</span>。</font></p><p><span class="math display">\[\boldsymbol{f}(\boldsymbol{q}, m)=\|q\|e^{\mathrm{i}(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q}e^{\mathrm{i} m \theta}\]</span></p></details><p><span class="math display">\[ \left(\begin{array}{c}q_0 \\q_1 \\q_2 \\q_3 \\\vdots \\q_{d-2} \\q_{d-1}\end{array}\right) \otimes\left(\begin{array}{c}\cos m \theta_0 \\\cos m \theta_0 \\\cos m \theta_1 \\\cos m \theta_1 \\\vdots \\\cos m \theta_{d / 2-1} \\\cos m \theta_{d / 2-1}\end{array}\right)+\left(\begin{array}{c}-q_1 \\q_0 \\-q_3 \\q_2 \\\vdots \\-q_{d-1} \\q_{d-2}\end{array}\right) \otimes\left(\begin{array}{c}\sin m \theta_0 \\\sin m \theta_0 \\\sin m \theta_1 \\\sin m \theta_1 \\\vdots \\\sin m \theta_{d / 2-1} \\\sin m \theta_{d / 2-1}\end{array}\right) \]</span></p><p><span class="math display">\[\theta_i =10000^{-2 i / d}, i \in[0,1,\ldots, d / 2-1]\]</span></p><details><summary><p>rotate_half</p></summary><p><span class="math display">\[ \left(\begin{array}{c}q_0 \\q_2 \\\vdots \\q_{d-2} \\q_1 \\q_3 \\\vdots \\q_{d-1}\end{array}\right) \otimes\left(\begin{array}{c}\cos m \theta_0 \\\cos m \theta_1 \\\vdots \\\cos m \theta_{d / 2-1} \\\cos m \theta_0 \\\cos m \theta_1 \\\vdots \\\cos m \theta_{d / 2-1}\end{array}\right)+\left(\begin{array}{c}-q_1 \\-q_3 \\\vdots \\-q_{d-1} \\q_0 \\q_2 \\\vdots \\q_{d-2}\end{array}\right) \otimes\left(\begin{array}{c}\sin m \theta_0 \\\sin m \theta_1 \\\vdots \\\sin m \theta_{d / 2-1} \\\sin m \theta_0 \\\sin m \theta_1 \\\vdots \\\sin m \theta_{d / 2-1}\end{array}\right) \]</span></p></details><h2 id="远程衰减">3.1. 远程衰减</h2><p>随着相对距离增加，<span class="math inline">\(\mathbf{q}\)</span>、<span class="math inline">\(\mathbf{k}\)</span> 的内积震荡衰减。</p></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6f4d4847e72369309380937443c06ebe?method=download&shareKey=084284ff0628e98fe5754563476382e7" width="320"></p><details><summary>python 代码</summary><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">import numpy as npimport matplotlib.pyplot as pltdef rotate(v, theta):    &quot;&quot;&quot;    Rotate vector v by theta radians.    &quot;&quot;&quot;    rotation_matrix &#x3D; np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])    return np.dot(rotation_matrix, v)def calculate_similarity(q, k, position_q, position_k):    &quot;&quot;&quot;    Calculate the similarity of RoPE-encoded vectors.    &quot;&quot;&quot;    theta_q &#x3D; np.array([10000**(-2 * i &#x2F; dim) * position_q for i in range(len(q))])    theta_k &#x3D; np.array([10000**(-2 * i &#x2F; dim) * position_k for i in range(len(k))])    q_rope &#x3D; [rotate(q[i], theta_q[i]) for i in range(len(q))]    k_rope &#x3D; [rotate(k[i], theta_k[i]) for i in range(len(k))]    q_rope &#x3D; np.concatenate(q_rope, axis&#x3D;0)    k_rope &#x3D; np.concatenate(k_rope, axis&#x3D;0)    return np.dot(q_rope, k_rope.T)dim &#x3D; 128q &#x3D; np.random.rand(int(dim &#x2F; 2), 2)  # dim-dimension vector divided into dim &#x2F; 2 2D sub-vectorsk &#x3D; np.random.rand(int(dim &#x2F; 2), 2)# Calculate similarity for different positionsposition_q &#x3D; 128positions_k &#x3D; range(256)similarities &#x3D; [calculate_similarity(q, k, position_q, position_k) for position_k in positions_k]plt.plot([x - position_q for x in positions_k], similarities)plt.xlabel(&#39;position_k - position_q&#39;)plt.ylabel(&#39;attention score&#39;)plt.title(&#39;Similarity decay for RoPE&#39;)plt.show()plt.savefig(&#39;rope.png&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></details><h1 id="alibi">4. ALiBi</h1><ul><li><p>提出：Press O, Smith N A, Lewis M. Train short, test long:Attention with linear biases enables input length extrapolation[J].arXiv preprint arXiv:2108.12409, 2021.</p></li><li><p>目标：<font color="red">长度外推</font>。训练时采用短序列，推理时对更长的序列进行有效推断。</p></li><li><p>原理：不向 word embedding 加 position embedding，而是在 QKattention score 上加一个与距离成比例的线性惩罚来实现外推。 <span class="math display">\[\operatorname{softmax}\left(q_i K^{\top}+m\cdot[-(i-1), \ldots,-2,-1,0]\right)\]</span> 其中，<span class="math inline">\(m\)</span> 为头特定的斜率超参数。</p></li><li><p>斜率选择：对于 <span class="math inline">\(n\)</span>个头的模型，斜率值以 <span class="math inline">\(2^{\frac{-8}{n}}\)</span>为起始值、ratio。如8个头的模型，采用 <span class="math inline">\(\frac{1}{2^1}, \frac{1}{2^2}, \ldots,\frac{1}{2^8}\)</span>；16个头的模型，采用 <span class="math inline">\(\frac{1}{2^{0.5}}, \frac{1}{2^1},\frac{1}{2^{1.5}}, \ldots, \frac{1}{2^8}\)</span>。</p></li><li><p>相关工作：<font color="red">正弦位置 embedding、RoPE长度外推能力有限；相对位置编码计算复杂度高、需要额外的内存</font>。效果对比详见论文表2-4。且 ALiBi 无额外的运行时开销。</p></li><li><p>效果：可以实现与直接在长序列上训练、采用正弦位置编码的模型相当的困惑度，训练速度更快，内存需求更小。长度外推性能在训练序列长度两倍左右达到峰值，详见论文图6。不进行长度外推时，可实现更低或相似的困惑度。</p></li><li><p>采用滑动窗口测试，外推到更长的序列时性能保持平稳或下降（区别于非重叠窗口测试），表明ALiBi 并不能利用更长的上下文进行更准确的预测。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2986d813a7847865bfc3874eca2abedb?method=download&shareKey=14d81391df804606315fe7ac557f1f23" width="536"></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Attention</title>
      <link href="/blog/ji-qi-xue-xi/transformer/attention/"/>
      <url>/blog/ji-qi-xue-xi/transformer/attention/</url>
      
        <content type="html"><![CDATA[<p>学习序列中不同位置的依赖关系，从而更好地表征语义。</p><h1 id="attention-算法">1. attention 算法</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB42e0cef18c1cf57104225b9e90e0b9c7?method=download&shareKey=eccd0c8d4b74eba9a35497e62f8f2cc4" width="500"></p><h2 id="scaled-dot-product-attention">1.1. Scaled Dot-Product Attention</h2><p><span class="math display">\[\operatorname{Attention}(Q, K,V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)V\]</span></p><ul><li>提出：Vaswani A. Attention is all you need[J]. Advances in NeuralInformation Processing Systems, 2017.</li><li>出发点：<font color="red">优化高维 <span class="math inline">\(q\)</span>、<span class="math inline">\(k\)</span>下的数值稳定性、训练稳定性。</font><span class="math inline">\(q \cdotk=\sum_{i=1}^{d_k} q_i k_i\)</span>，对于较大的 <span class="math inline">\(d_k\)</span>，点积得到的值可能较大（假设 <span class="math inline">\(q\)</span>、<span class="math inline">\(k\)</span>的元素都是均值为 0、方差为 1 的独立随机变量，则 <span class="math inline">\(q \cdot k\)</span> 均值为 0，方差为 <span class="math inline">\(d_k\)</span>），导致 softmax落入梯度极小的区域，训练不稳定。</li></ul><h2 id="multi-head-attention">1.2. Multi-Head Attention</h2><p><span class="math display">\[\begin{aligned}\operatorname{MultiHead}(Q, K, V) &amp;=\operatorname{Concat}\left(\operatorname{head}_1, \ldots,\operatorname{head}_{\mathrm{h}}\right) W^O \\\text { where head } &amp; =\operatorname{Attention}\left(Q W_i^Q, KW_i^K, V W_i^V\right)\end{aligned}\]</span></p><ul><li><p>提出：Vaswani A. Attention is all you need[J]. Advances in NeuralInformation Processing Systems, 2017.</p></li><li><p>出发点：通过多个 attention head，允许模型 attend不同表示子空间（<span class="math inline">\(d_k=d_v=d_{\text {model }} /h\)</span>，可以理解为被映射到了子空间）的信息，<font color="red">提高了模型的表征能力</font>。</p></li><li><p>计算量</p><p>全维度单头 attention <span class="math inline">\(O\left(d_{\text{model }}^2\right)\)</span></p><p>multi-head attention <span class="math inline">\(h\times\left(O\left(d_{\text {model }}^2 / h\right)+O\left(d_{\text{model }}^2 / h^2\right)\right)=O\left(d_{\text {model}}^2\right)+O\left(d_{\text {model }}^2 /h\right)\)</span>。随注意力头的数量增加，计算复杂度会减少。</p></li><li><p>相较于全维度单头attention，<font color="red">内存访问效率更高，可以实现更高的计算吞吐量</font>。</p><blockquote><ul><li>多个 head 并行计算，能够充分利用硬件的SIMD（单指令多数据）能力，减小单次计算的延迟。</li><li>单个注意力计算的数据块更小，缓存更友好，尤其是在现代硬件（如 GPU 或TPU）中，可以更高效地利用寄存器和 L1/L2 缓存。</li><li>矩阵切分后的更小子矩阵可以更好地适应硬件的块状存储和预取机制。</li></ul></blockquote></li></ul><h1 id="计算效率优化">2. 计算效率优化</h1><ul><li>原因：vanilla Transformer self-attention 的二次复杂度。</li><li>优化方向<ul><li>访存高效的注意力算法<ul><li><a href="#multi-head-attention">Multi-Head Attention</a></li><li><a href="#flash-attention">Flash Attention</a></li></ul></li><li><font color="green">Linear Attention</font><ul><li><a href="https://arxiv.org/abs/2401.04658">LightningAttention-2</a>，如 <a href="https://arxiv.org/abs/2501.08313">MiniMax-01</a></li></ul></li><li>部分注意力<ul><li>Local (i.e., sliding window) attention。如 Mistral 7B。</li><li>alternating local-global attention。如 <a href="https://research.character.ai/optimizing-inference/">Character.AI</a>。</li><li>sparse attention。如 <a href="https://qwen2.org/qwen2-5-turbo/">Qwen2.5-Turbo</a><ul><li><a href="#star-attention">Star Attention</a></li></ul></li></ul></li><li>KV<ul><li>KV 共享<ul><li><a href="#muti-query-attention">Muti-Query Attention</a></li><li><a href="#group-query-attention">Group-Query Attention(GQA)</a></li><li><a href="#cross-layer-attention">Cross-Layer Attention</a></li><li><a href="#paged-attention">Paged Attention</a></li></ul></li><li>KV Cache 压缩</li><li>……</li></ul></li><li>多个主机并行计算<ul><li><a href="#ring-attention">Ring Attention</a></li><li><a href="#star-attention">Star Attention</a></li></ul></li><li><a href="https://www.aussieai.com/research/head-pruning">attentionhead 剪枝</a></li><li>……</li></ul></li></ul><h2 id="flash-attention">2.1. Flash attention</h2><ul><li><p>提出：Dao T, Fu D, Ermon S, et al. Flashattention: Fast andmemory-efficient exact attention with io-awareness[J]. Advances inNeural Information Processing Systems, 2022, 35: 16344-16359.</p><p>Dao T. Flashattention-2: Faster attention with better parallelism andwork partitioning[J]. arXiv preprint arXiv:2307.08691, 2023.</p></li><li><p><a href="https://github.com/Dao-AILab/flash-attention" class="uri">https://github.com/Dao-AILab/flash-attention</a></p></li><li><p><font color="red">显著优化了 attention计算的速度和内存占用</font>，可以以相同的成本训练更长上下文的模型。</p></li><li><p><a href="https://arxiv.org/abs/2407.08608">FlashAttention-3</a>针对 Hopper GPUs（H100/H800）进行了优化。</p></li></ul><h2 id="muti-query-attention">2.2. Muti-Query Attention</h2><ul><li>提出：Shazeer N. Fast transformer decoding: One write-head is allyou need[J]. arXiv preprint arXiv:1911.02150, 2019.</li><li>所有 Query 共享同一个 Key-Value对，<font color="red">显著减小了计算量和 KV cache的存储开销，特别适用于长上下文任务、资源受限设备、实时推理任务。</font></li><li>应用：如 <a href="https://arxiv.org/pdf/2406.12793">ChatGLM2</a>、<a href="https://research.character.ai/optimizing-inference/">Character.AI</a></li></ul><h2 id="group-query-attention">2.3. Group-Query Attention</h2><ul><li>提出：Ainslie J, Lee-Thorp J, de Jong M, et al. Gqa: Traininggeneralized multi-query transformer models from multi-headcheckpoints[J]. arXiv preprint arXiv:2305.13245, 2023.</li><li>将 query heads 分成 G 个组，每个组内的 query heads 共享一个Key-Value 对。模型性能和推理速度介于 multi-head attention、muti-queryattention 之间。</li><li>应用：如 <a href="https://arxiv.org/pdf/2406.12793">GLM-4</a>、<a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md">Llama3.3</a>、<a href="https://arxiv.org/pdf/2310.06825">Mistral 7B</a></li></ul><h2 id="cross-layer-attention">2.4. Cross-Layer Attention</h2><ul><li><p>跨层共享 KV cache。相较于传统的 Multi-Query Attention没有性能退化，进一步优化了内存占用。</p></li><li><p>应用：如 <a href="https://research.character.ai/optimizing-inference/">Character.AI</a></p><p>Brandon W, Mishra M, Nrusimha A, et al. Reducing TransformerKey-Value Cache Size with Cross-Layer Attention[J]. arXiv preprintarXiv:2405.12981, 2024.</p></li></ul><h2 id="paged-attention">2.5. Paged attention</h2><ul><li>提出：Kwon W, Li Z, Zhuang S, et al. Efficient memory management forlarge language model serving with pagedattention[C]//Proceedings of the29th Symposium on Operating Systems Principles. 2023: 611-626.</li><li><a href="https://github.com/vllm-project/vllm" class="uri">https://github.com/vllm-project/vllm</a></li><li>原理<ul><li><p><font color="red">将 KV cache划分为多个块</font>，每个块包含固定数量 token 的 KV向量，<font color="red">缓解内存碎片化问题</font>。</p><blockquote><ul><li>避免了为每个请求预先分配一大块连续内存，可以按需动态分配和释放，也避免了新生成token 时的内存拷贝。</li><li>各 KV 块不需要连续存储。</li></ul></blockquote></li><li><p>vLLM在前缀一致时，支持请求内（如并行采样（一个请求生成多个答案，如创意写作）、beamsearch 解码）和跨请求（如具体应用中 prompt较固定，如机器翻译）<font color="red">KV vache共享</font>。减少了重复计算与内存占用。</p></li><li><p>支持更大的 batch size，缩短了请求延迟，增大了吞吐量。</p></li></ul></li></ul><h2 id="ring-attention">2.6. Ring attention</h2><ul><li>提出：Liu H, Zaharia M, Abbeel P. Ring attention with blockwisetransformers for near-infinite context[J]. arXiv preprintarXiv:2310.01889, 2023.</li><li>作者来自 UC Berkeley。</li><li>原理<ul><li>采用 <a href="https://arxiv.org/pdf/2305.19370">Blockwise ParallelTransformer(BPT)</a>，将输入序列划分为多个<font color="red">块</font>，分别分发给<font color="red">多个主机</font>。每个主机<font color="red">blockwise 并行计算</font>其输入块的 self-attention 和FFN，显著减小了内存占用。</li><li><font color="red">环形通信</font>:所有主机形成环状结构，每个设备在计算 blockwise self-attention 和 FFN时，将其使用的 KV block 发送给下一个设备，并从上一个设备接收 KVblock。因此<font color="red">不需要存储完整序列的 KV值，进一步减小了内存占用</font>。且<font color="red">通信与计算重叠</font>，没有额外的通信开销。</li></ul></li><li>通信与计算重叠的条件：<span class="math inline">\(c &gt;=F/B\)</span>。其中，<span class="math inline">\(c\)</span>为块大小，<span class="math inline">\(F\)</span> 为FLOPs，<span class="math inline">\(B\)</span> 为带宽。推导详见论文第3节。</li><li><font color="red">允许上下文长度随设备数量线性扩展</font>。同时保持高计算利用率，几乎没有额外开销。</li></ul><h2 id="star-attention">2.7. Star attention</h2><ul><li><p>提出：Acharya S, Jia F, Ginsburg B. Star attention: Efficient llminference over long sequences[J]. arXiv preprint arXiv:2411.17116,2024.</p></li><li><p>作者来自 NVIDIA。</p></li><li><p>出发点：许多 LLM 长上下文任务，输入由 <font color="red">longcontext</font>、short query、short answer 组成。回答 query 通常只需要context 小部分内容的信息，因此 context tokens 只需关注附近的 tokens，而query tokens 需要关注之前所有的 tokens。</p><p>对应的 LLM 的 prompt 结构见论文附录 C.3.</p></li><li><p>两阶段计算</p><ul><li><p>Context Encoding：将 context划分成连续的<font color="red">块</font>，分别分发给<font color="red">多个主机</font>。每个主机计算<font color="red">anchor block</font>（第一个块）+ 其分配块的<font color="red">selfattention（blockwise-local/block-sparse）</font>。将 attention计算复杂度从二次降低到线性。相较于 Ring attention，star attention不与其他主机通信，最小化通信开销。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0636c6ed7ab3b15162fe66bec4841b6c?method=download&shareKey=b7f8adc605f5dc883ae27f5df5102396" width="501"></p><p>anchor block：将第一个 block 作为前缀，使得 blockwise-local attention的结果接近全局 attention 中存在的 <font color="green">attentionsink</font>。实验（论文表 3）表明没有 anchor block性能显著下降；且相较于采用前一 block，采用第一个 block 性能更好。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBab7b7aa11ce64369eca2bfff2e33967d?method=download&shareKey=b7f8adc605f5dc883ae27f5df5102396" width="504"></p></li><li><p>Query Encoding and Token Generation：各个主机计算 query、其 KVcache 的局部 attention 输出。<font color="red">query host聚合结果</font>，并更新 query tokens 和新生成 token 的 KVcache。<font color="red">query 和生成 token 能看到全部context。</font></p><p><span class="math display">\[ \begin{gathered}A_h=\left(\frac{\exp \left(\frac{QK_h^{T}}{\sqrt{d}}\right)}{\sum_{k=1}^{l_k} \exp \left(\frac{Q K_{h,k}^{T}}{\sqrt{d}}\right)}\right) V_h \\s_h=\sum_{k=1}^{l_k} \exp \left(\frac{Q K_{h,k}^{\top}}{\sqrt{d}}\right) \\A=\left[A_1, A_2, \ldots, A_H\right] \\s=\left[s_1, s_2, \ldots, s_H\right] \\s_{\text {global }}=\sum_{h=1}^H s_h \\A_{\text {global }}=\sum_{h=1}^H \frac{s_h}{s_{\text {global }}} A_h\end{gathered}\]</span></p></li><li><p>伪代码详见论文附录 A。</p></li></ul></li><li><p>block size 设置</p><p>block size 影响准确度、推理速度之间的均衡。建议设置为序列长度的1/4。取决于具体的用户需求，比如对于超过 128K 的序列长度，block size固定为 32K 以优先考虑速度；对于更大的模型，如 70B，block size 限制为16K。</p></li><li><p>效果</p><ul><li>相较于全局 attention，可保持 95-100% 的准确率。在 RULER aggregation任务（常见词汇提取、频繁词汇提取）上，甚至有显著的性能提升。</li><li>相较于 Ring (global) Attention，准确率下降控制在 3% 以内时，可以实现1.1-11 倍的加速。特别是在长序列、大模型上，速度提升更显著。</li></ul></li><li><p><a href="https://github.com/NVIDIA/Star-Attention" class="uri">https://github.com/NVIDIA/Star-Attention</a></p><p>兼容绝大多数基于全局注意力的 Transformer模型，<font color="red">无需微调</font>。</p></li></ul><h1 id="资源">3. 资源</h1><ul><li><a href="https://www.aussieai.com/research/attention">attention优化综述</a></li><li><a href="https://pytorch.org/blog/flexattention/">Flexattention</a>: pytorch attention 计算接口，支持多种 attention变体。</li></ul><h1 id="todo">4. TODO:</h1><ul><li>Linear Attention</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>MiniCPM-o 2.6</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/minicpm-o-2.6/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/minicpm-o-2.6/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://github.com/OpenBMB/MiniCPM-o" class="uri">https://github.com/OpenBMB/MiniCPM-o</a></li><li><a href="https://openbmb.notion.site/MiniCPM-o-2-6-GPT-4o-188ede1b7a558084b3aedd669cb80730">官方技术blog</a></li><li>作者来自：清华、面壁智能</li></ul><p><img src="https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/minicpm-o-26-framework.png" width="1049"></p><ul><li>功能：图像理解、语音理解与生成。接收连续的视频和音频流，支持配置<font color="red">VAD</font>阈值、是否允许用户打断，进行实时语音交互。</li><li>基于SigLip-400M、<font color="red">Whisper-medium-300M、ChatTTS-200M 和Qwen2.5-7B</font>, 共 8B 参数。通过优化视觉 token密度，优化模型的推理速度、首 token 延迟、内存占用和功耗。</li><li>时分复用：多模态数据交错。</li><li>语音理解任务<ul><li>中英文语音识别、翻译</li><li>情感识别，说话人性别、年龄范围、健康状态</li><li>音频字幕</li><li>声学场景标记</li></ul></li><li>benchmark: StreamingBench</li><li>存在的问题<ul><li>不稳定的语音输出。语音生成可能会受到背景噪音和无意义声音的影响，表现不稳定。</li><li>流式推理存在轻微的性能下降，因为音频编码并非全局的。</li><li>重复响应。当遇到连续相似的用户请求时，模型往往会重复相同的回答。</li></ul></li><li><font color="green">存疑</font><ul><li><font color="green">基于 VAD 丢弃用户静音片段、实现用户打断。强依赖VAD 划分语音 turn。嘈杂环境 VAD 是否能及时发送语音段？</font></li><li><font color="green">user query audio是啥？用户视频、语音均在视频流中了？</font></li><li><font color="green">speech embedding: 将 LLM 隐藏状态作为 chatTTS生成的 condition。</font></li></ul></li><li><font color="green">TODO: 官方技术 blog</font></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Mistral 7B</title>
      <link href="/blog/llm/mo-xing/mistral-7b/"/>
      <url>/blog/llm/mo-xing/mistral-7b/</url>
      
        <content type="html"><![CDATA[<ul><li><p>Jiang A Q, Sablayrolles A, Mensch A, et al. Mistral 7B[J]. arXivpreprint arXiv:2310.06825, 2023.</p></li><li><p><font color="red">出发点：保持高性能的同时提高推理效率</font>。</p></li><li><p>优化点</p><ul><li><p><font color="red">grouped-query attention</font>：减少了attention 的计算量和内存占用，允许更高的 batch size 和吞吐量。</p></li><li><p><font color="red">sliding windowattention</font>：支持任意长度的序列，减少计算成本、KV cache内存占用。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb9d7a9c49938b7fdf6e518f97f401771?method=download&shareKey=4eace57d6f3576091e9ddb3ad4087a51" width="798"></p><p><span class="math inline">\(W=4096\)</span>，n_layers=32，理论attention 跨度约为 131K tokens。</p></li><li><p><font color="red">Rolling Buffer Cache</font>：缓存大小固定为<span class="math inline">\(W\)</span>，时间步 <span class="math inline">\(i\)</span> 的 KV 存储在 <span class="math inline">\(i \bmod W\)</span>。<span class="math inline">\(i&gt;W\)</span>时，过去的值被覆盖。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBea3562ede5fb0a5881f87cd293e7dee4?method=download&shareKey=4eace57d6f3576091e9ddb3ad4087a51" width="799"></p></li><li><p>推理：对于非常长的提示，在预填充阶段，对序列分块以限制内存使用。</p></li></ul></li><li><p>其它：安全性，通过设计系统提示</p><p><code>Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.</code></p><p>self-reflection prompt：可以通过提示让模型将用户 prompt或生成的答案分类为可接受或 Illegal activities such as terrorism, childabuse or fraud; Hateful, harassing or violent content such asdiscrimination, self-harm or bullying; Unqualified advice for instancein legal, medical or financial domains.</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>building effective agents</title>
      <link href="/blog/llm/agent/building-effective-agents/"/>
      <url>/blog/llm/agent/building-effective-agents/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.anthropic.com/research/building-effective-agents" class="uri">https://www.anthropic.com/research/building-effective-agents</a></p><h1 id="agents-vs-workflows">1. agents vs workflows</h1><ul><li>workflows 有预定义的 code path，调用 LLMs 和 tools。</li><li>agents 由 LLMs 动态控制流程、工具使用。</li></ul><h1 id="何时以及何时不使用-agents">2. 何时（以及何时不）使用 agents</h1><ul><li>建议采用最简单的解决方案。当需要<font color="green">大规模的灵活性</font>、模型驱动的决策时，agents是更好的选择。</li><li>对于许多应用，通过<font color="red">检索、上下文示例</font>优化单次LLM 调用通常已经足够。</li></ul><h1 id="agent-框架">3. agent 框架</h1><ul><li>如 LangChain 的 <a href="https://langchain-ai.github.io/langgraph/">LangGraph</a>，AmazonBedrock 的 <a href="https://aws.amazon.com/bedrock/agents/">AI Agentframework</a>，<a href="https://rivet.ironcladapp.com/">Rivet</a>（构建LLM workflows 的拖拽式 GUI）,<a href="https://www.vellum.ai/">Vellum</a>（构建和测试复杂 workflows 的GUI）。</li><li>框架可能掩盖了底层的 prompts ​​and responses，更建议开发者直接使用 LLMAPIs，除非理解框架底层代码。</li><li><a href="https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents">常见agent workflows 最小实现示例代码</a></li></ul><h1 id="代理系统的常见模式">4. 代理系统的常见模式</h1><h2 id="building-block-the-augmented-llm">4.1. Building block: The augmentedLLM</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75"></p><p>anthropic 提供了 <a href="https://www.anthropic.com/news/model-context-protocol">AI系统连接内容库、商业工具、开发环境的标准协议</a> 以及 <a href="https://modelcontextprotocol.io/quickstart/client#building-mcp-clients">客户端实现</a>。</p><h2 id="workflow-prompt-chaining">4.2. Workflow: Prompt chaining</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75"></p><p>*Gate 用于检查输出确保流程正常进行。</p><ul><li>适用于可以轻松、清晰地分解为固定子任务的任务，主要目标是通过使每次LLM 调用都成为更简单的任务，以牺牲延迟换取更高的准确度。</li><li>示例<ul><li>生成文案，再翻译。</li><li>先写文档大纲、再检查是否符合特定标准，最后根据大纲撰写文档。</li></ul></li></ul><h2 id="workflow-routing">4.3. Workflow: Routing</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75"></p><ul><li>适用于可以对输入进行分类，且最好分别处理的任务。通过构建更具针对性的提示等方式专门处理。</li><li>示例<ul><li>将不同类型的客户服务查询（一般问题、退款请求、技术支持）路由到不同的下游流程、提示和工具。</li><li>将简单/常见问题路由到较小的模型，将困难/罕见的问题路由到更强大的模型，以优化成本和速度。</li></ul></li></ul><h2 id="workflow-parallelization">4.4. Workflow: Parallelization</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75"></p><ul><li>分为两类<ul><li>Sectioning：将任务分解为多个独立、可并行的子任务，以提高速度。</li><li>Voting：多次运行同一任务以获得多样化的、更高置信度的结果。</li></ul></li><li>示例<ul><li>Sectioning：一个实例处理用户请求；另一个实例进行安全guardrails。</li><li>Voting：评价内容是否不当，多个提示评估不同的方面，或采用不同的阈值来平衡false positives and negatives。</li></ul></li></ul><h2 id="workflow-orchestrator-workers">4.5. Workflow:Orchestrator-workers</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75"></p><ul><li>中央 LLM 动态分解任务、分派给 workerLLMs、合成结果。与并行的区别在于子任务不是预定义的。</li><li>示例：从多个来源收集、分析信息（源，查询、分析方式可能取决于具体任务）。</li></ul><h2 id="workflow-evaluator-optimizer">4.6. Workflow: Evaluator-optimizer</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75"></p><ul><li>适用于 LLM 可以对响应进行反馈、且反馈可以显著改进响应的任务。</li><li>示例<ul><li>翻译。</li><li>需要多轮搜索、分析的任务，评估器决定是否需要进一步搜索。</li></ul></li></ul><h2 id="agents">4.7. Agents</h2><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d53798dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75"></p><ul><li><p>适用于无法硬编码固定路径的开放式任务，且 LLM的决策有一定的可信度。</p></li><li><p>示例</p><p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75"></p></li></ul><h1 id="总结">5. 总结</h1><ul><li>实现代理的核心原则<ul><li>保持 agent 设计的简单性。</li><li>优先考虑透明度，显式地展示 agent 的规划步骤。</li><li>通过全面的工具文档、测试来仔细设计 agent-computer 接口。</li></ul></li></ul><h1 id="示例">6. 示例</h1><ul><li><p>agent为需要对话和行动、具有明确的成功标准、支持反馈循环、集成了有效的人工监督的任务带来了最大价值。</p></li><li><p>客户支持</p><p>聊天机器人界面，可以集成工具来提取客户数据、订单、知识库等；可以以编程的方式进行退款等操作，容易衡量是否成功。</p></li><li><p>coding</p><p>自动化测试、根据测试结果迭代优化，问题明确、容易衡量输出质量。</p></li></ul><h1 id="工具的提示工程">7. 工具的提示工程</h1><ul><li>对 LLM来说某些格式比其他格式更难编写。建议：接近互联网文本中的自然格式；没有格式化开销。</li><li>良好的工具定义通常包括示例用法、边界case、输入格式要求、<font color="green">与其他工具的明确界限</font>。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> Agent </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WavChat</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/wavchat/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/wavchat/</url>
      
        <content type="html"><![CDATA[<ul><li>Ji S, Chen Y, Fang M, et al. WavChat: A Survey of Spoken DialogueModels[J]. arXiv preprint arXiv:2411.13577, 2024.</li><li><a href="https://github.com/jishengpeng/WavChat" class="uri">https://github.com/jishengpeng/WavChat</a></li></ul><h1 id="相关研究">1. 相关研究</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB69e58a871dd44f74f79ad7c5dc63b59e?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="802"></p><p>Table 5: A comprehensive list of publicly available spoken dialoguemodels and their URL</p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>Model</th><th>URL</th></tr></thead><tbody><tr><td>AudioGPT</td><td>https://github.com/AIGC-Audio/AudioGPT</td></tr><tr><td>SpeechGPT</td><td>https://github.com/0nutation/SpeechGPT</td></tr><tr><td>Freeze-Omni</td><td>https://github.com/VITA-MLLM/Freeze-Omni</td></tr><tr><td>Baichuan-Omni</td><td>https://github.com/westlake-baichuan-mllm/bc-omni</td></tr><tr><td>GLM-4-Voice</td><td>https://github.com/THUDM/GLM-4-Voice</td></tr><tr><td>Mini-Omni</td><td>https://github.com/gpt-omni/mini-omni</td></tr><tr><td>Mini-Omni2</td><td>https://github.com/gpt-omni/mini-omni2</td></tr><tr><td>FunAudioLLM</td><td>https://github.com/FunAudioLLM</td></tr><tr><td>Qwen-Audio</td><td>https://github.com/QwenLM/Qwen-Audio</td></tr><tr><td>Qwen2-Audio</td><td>https://github.com/QwenLM/Qwen2-Audio</td></tr><tr><td>LLaMA3.1</td><td>https://www.llama.com</td></tr><tr><td>Audio Flamingo</td><td>https://github.com/NVIDIA/audio-flamingo</td></tr><tr><td>Spirit LM</td><td>https://github.com/facebookresearch/spiritlm</td></tr><tr><td>dGSLM</td><td>https://github.com/facebookresearch/fairseq/tree/main/examples/textless_nlp/dgslm</td></tr><tr><td>Spoken-LLM</td><td>https://arxiv.org/abs/2305.11000</td></tr><tr><td>LLaMA-Omni</td><td>https://github.com/ictnlp/LLaMA-Omni</td></tr><tr><td>Moshi</td><td>https://github.com/kyutai-labs/moshi</td></tr><tr><td>SALMONN</td><td>https://github.com/bytedance/SALMONN</td></tr><tr><td>LTU-AS</td><td>https://github.com/YuanGongND/ltu</td></tr><tr><td>VITA</td><td>https://github.com/VITA-MLLM/VITA</td></tr><tr><td>SpeechGPT-Gen</td><td>https://github.com/0nutation/SpeechGPT</td></tr><tr><td>Westlake-Omni</td><td>https://github.com/xinchen-ai/Westlake-Omni</td></tr><tr><td>MooER-Omni</td><td>https://github.com/MooreThreads/MooER</td></tr><tr><td>Hertz-dev</td><td>https://github.com/Standard-Intelligence/hertz-dev</td></tr><tr><td>Fish-Agent</td><td>https://github.com/fishaudio/fish-speech</td></tr><tr><td>SpeechGPT2</td><td>https://0nutation.github.io/SpeechGPT2.github.io/</td></tr></tbody></table><ul><li>本文提到的其它口语对话系统：Spectron, USDM, EMOVA, PSLM,IntrinsicVoice, OmniFlatten, SyncLLM, Parrot.</li><li>音视频口语对话系统<ul><li>Park S J, Kim C W, Rha H, et al. Let's Go Real Talk: Spoken DialogueModel for Face-to-Face Conversation[J]. arXiv preprint arXiv:2406.07867,2024.</li></ul></li></ul><h1 id="语音对话系统的能力">2. 语音对话系统的能力</h1><ul><li>理解音频中的语言、副语言、声学信息，并相应地生成响应。如 E-chat[227]中一个经典的示例，如果用户以愉快的语气问“我的手机无法开机，我该怎么办？”，系统可能会回答“看起来你对购买新手机很兴奋。你对什么类型的手机感兴趣？”相反，如果用户以悲伤的语气问同样的问题，系统可能会回答“很遗憾你的手机无法使用。如果你已经熟悉了维修政策，让我们继续下一步。”。支持多语种，进一步的，自动以用户使用的语言进行响应。</li><li>生成语音的可控性，如音色、风格、口音、情感、音高、语速等，或自动根据用户调整回复语音的风格。（音色克隆：提供该能力有安全性问题）</li><li>音频、音乐理解与生成。</li><li>长上下文多轮对话。</li><li>流式生成，低延迟。</li><li>全双工对话，支持 normal turnexchanges、打断、backchannel（不打断的反馈，如 OK、Isee）。具体的，用户说完后立即响应、正确处理用户语音中间的停顿、检测用户的打断、用户说话时的backchannel、<font color="green">必要时打断用户（是个真实需求吗？）</font>。</li><li>扩展更多模态与能力，如图像、视频等。</li><li>安全性。</li></ul><h1 id="系统结构">3. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBfc39b5dd45589a54031be773aef56582?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="603"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB681679cc19dcf3df708511dfd38733c0?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="719"></p><ul><li>模态 adapter：如 Window-Level Q-Former [121]。</li><li>交错生成文本、语音，如 <font color="green">Spirit-LM通过可视化表明采用这种数据格式训练，文本、语音特征的相似度更高</font>。</li><li>并行生成文本、语音，如 PSLM、Llama-Omni、Mini-Omni。</li></ul><h1 id="语音表示">4. 语音表示</h1><h2 id="semantic-vs-acoustic">4.1. Semantic vs Acoustic</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe62e830e638898aeb81f9220e345197b?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="634"></p><ul><li>语义<ul><li>自监督：如 HuBERT、Wav2Vec、XLS-R、WavLM、SPIRAL 等。</li><li>有监督：如 Whisper、supervised semantic speech(<span class="math inline">\(S^3\)</span>) tokenizer 等。</li></ul></li><li>声学：如 Encodec。</li><li>语义 + 声学：如 SpeechTokenizer（RVQ-1 蒸馏 HuBERT）、mimi（splitRVQ）。</li><li>音频生成<ul><li>语义 token<ul><li>如 HuBERT + HuBERT unit-based HiFi-GAN，<span class="math inline">\(S^3\)</span> tokenizer + flow matching +HiFi-GAN。</li><li>存在的问题：语音、声音事件、音乐统一建模。</li></ul></li><li>声学 token: vocoder</li><li>LLM 隐藏表示<ul><li>如 Freeze-Omni（LLM 隐藏表示 + 输出文本 token -&gt; NAR decoder-&gt; AR decoder -&gt; vocoder）、IntrinsicVoice (Group-Former)。</li></ul></li></ul></li><li>副语言信息的理解：语音表示中蕴含或提供额外的表示，如 speechembedding、副语言属性等。</li><li>展望：将语音表示对齐到 LLM 的文本表示空间，减轻 LLM的生成负担。</li></ul><h2 id="连续-vs-离散">4.2. 连续 vs 离散</h2><ul><li>CV 领域整合离散、连续表示的工作<ul><li>Zhou C, Yu L, Babu A, et al. Transfusion: Predict the next token anddiffuse images with one multi-modal model[J]. arXiv preprintarXiv:2408.11039, 2024.</li><li>Show-o: One single transformer to unify multimodal understanding andgeneration.</li></ul></li></ul><h2 id="量化器-单层-vs-多层">4.3. 量化器 单层 vs 多层</h2><ul><li>RVQ 多层表示：delay 并行生成、moshi depth transformer并行生成。</li><li>单层：如 WavTokenizer [90], Single-Codec [119], BigCodec[224]。</li></ul><h2 id="是否包含文本指导">4.4. 是否包含文本指导</h2><ul><li>在语音输入、语音输出中间插入文本任务，类似 chain-of-thought。</li></ul><h1 id="多阶段训练">5. 多阶段训练</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB51a3525b1b26c8611eec864884669e56?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="379"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb0a5c6f47baf60766ce4fb56ba39e456?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="492"></p><ul><li>对齐语音-文本模态以保证一致的理解与生成。</li><li>偏好优化（RL）<ul><li>DPO：如 Qwen-Audio 2、Align-SLM（semantic metrics）。</li><li>REINFORCE：如 <a href="https://nanyang2015.github.io/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/seed-tts/#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">Seed-TTS</a>。</li></ul></li></ul><h1 id="流式">6. 流式</h1><ul><li>模型结构优化：因果卷积、chunk-wise causal attention。</li><li>长序列生成优化：如 speech tokenizer 帧率压缩、IntrinsicVoiceGroupFormer。</li><li>流式音频生成 如 Encodec、LLama-Omni。另外，流式 ASR 如 U2++Conformer、流式 TTS 如 XTTS-v2。</li></ul><h1 id="全双工">7. 全双工</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc4b94b0aad4a9e4a0aad64f245f9fae9?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="561"></p><h2 id="级联系统">7.1. 级联系统</h2><ul><li>Duplex Conversation [130]：3个核心模块<ul><li>用户状态检测：多模态模型，输入音频和文本，结合韵律、音高、停顿等特征，检测用户意图为switch turn、继续说话或犹豫。</li><li>响应信号选取：采用 multi-label 分类，在适当的时间插入适合对话情境的backchannel。</li><li>打断检测：多模态模型，输入音频和文本，识别用户中断，并避免将背景噪声、非预期的声音信号误判为中断。</li></ul></li><li>Outbound Agent System [98]：采用有限状态机。<ul><li>采用基于 BERT 的流式文本分类器，每 300ms 对 ASR结果进行语义分析，以识别中断（并避免 backchannel 被识别为中断）。</li><li>处理用户提及命名实体时的停顿、不连续表达，避免被检测为对话结束。</li></ul></li><li>Full-duplex LLM [211]：采用神经有限状态机、流式 ASR、流式 TTS。</li><li>CleanS2S [159]：VAD + ASR + LLM + TTS。</li></ul><h2 id="端到端系统">7.2. 端到端系统</h2><p>如 dGSLM、Moshi、SyncLLM、OmniFlatten。</p><h1 id="数据集">8. 数据集</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5133929439e840d2769d501c6415ca1e?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="981"></p><p>Table 4: Music and Non-Speech Sound Datasets</p><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>Dataset</th><th>Size</th><th>URL</th><th>Modality</th></tr></thead><tbody><tr><td>ESC-50</td><td>2,000 clips (5s each)</td><td><a href="https://github.com/karoldvl/ESC-50">https://github.com/karoldvl/ESC-50</a></td><td>Sound</td></tr><tr><td>UrbanSound8K</td><td>8,732 clips (&lt;=4s each)</td><td><a href="https://urbansounddataset.weebly.com/urbansound8k.html">https://urbansounddataset.weebly.com/urbansound8k.html</a></td><td>Sound</td></tr><tr><td>AudioSet</td><td>2000k+ clips (10s each)</td><td><a href="https://research.google.com/audioset/">https://research.google.com/audioset/</a></td><td>Sound</td></tr><tr><td>TUT Acoustic Scenes 2017</td><td>52,630 segments</td><td><a href="https://zenodo.org/record/400515">https://zenodo.org/record/400515</a></td><td>Sound</td></tr><tr><td>Warblr</td><td>10,000 clips</td><td><a href="https://warblr.net/">https://warblr.net/</a></td><td>Sound</td></tr><tr><td>FSD50K</td><td>51,197 clips (total 108.3 hours)</td><td><a href="https://zenodo.org/record/4060432">https://zenodo.org/record/4060432</a></td><td>Sound</td></tr><tr><td>DCASE Challenge</td><td>varies annually</td><td><a href="http://dcase.community/">http://dcase.community/</a></td><td>Sound</td></tr><tr><td>IRMAS</td><td>6,705 audio files (3s each)</td><td><a href="https://www.upf.edu/web/mtg/irmas">https://www.upf.edu/web/mtg/irmas</a></td><td>Music</td></tr><tr><td>FMA</td><td>106,574 tracks</td><td><a href="https://github.com/mdeff/fma">https://github.com/mdeff/fma</a></td><td>Music</td></tr><tr><td>NSynth</td><td>305,979 notes</td><td><a href="https://magenta.tensorflow.org/datasets/nsynth">https://magenta.tensorflow.org/datasets/nsynth</a></td><td>Music</td></tr><tr><td>EMOMusic</td><td>744 songs</td><td><a href="https://cvml.unige.ch/databases/emoMusic/">https://cvml.unige.ch/databases/emoMusic/</a></td><td>Music</td></tr><tr><td>MedleyDB</td><td>122 multitrack recordings</td><td><a href="https://medleydb.weebly.com">https://medleydb.weebly.com</a></td><td>Music</td></tr><tr><td>MagnaTagATune</td><td>25,863 clips (30s each)</td><td><a href="https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset">https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset</a></td><td>Music</td></tr><tr><td>MUSDB</td><td>150 songs</td><td><a href="https://paperswithcode.com/dataset/musdb18">https://paperswithcode.com/dataset/musdb18</a></td><td>Music</td></tr><tr><td>M4Singer</td><td>700 songs</td><td><a href="https://github.com/M4Singer/M4Singer">https://github.com/M4Singer/M4Singer</a></td><td>Music</td></tr><tr><td>Jamendo</td><td>600k songs</td><td><a href="https://www.jamendo.com/?language=en">https://www.jamendo.com/?language=en</a></td><td>Music</td></tr></tbody></table><h1 id="评价">9. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe56a19b8ea4238bb657b6ec245178f58?method=download&shareKey=8d91fa1e08facf65c9b922dc8fd5b499" width="1007"></p><p>其中，SUPERB、AudioBench、AIR-Bench Foundation、SD-Eval、<a href="https://arxiv.org/abs/2410.19168">MMAU</a>主要为语音、音频、音乐理解相关任务，VoiceBench、AudioBench、AIR-Benchchat 还包含一般知识、指令跟随相关任务。VoiceBench还评价在不同说话风格、声学环境、口语错误表达下的鲁棒性。SpokenWOZ评价任务导向的口语对话中能否正确完成用户请求。SuperCLUE 非开源。</p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>维度</th><th>评价方式与指标</th></tr></thead><tbody><tr><td>文本智能</td><td>测试集如知识问答（TriviaQA）、专业考试（MMLU、C-Eval）、数学推理（GSM8K）、指令跟随（Flan、Self-instruct）、多轮对话（CoQA、OpenAssistant）。<br>封闭式问题，如多项选择、给定答案关键词，可采用准确度相关的评价指标。<br>开放式问题，可采用机器翻译中BLEU, METEOR, ROUGE 等文本相似度指标；或者 BertScore等语义相似度指标；或者采用 GPT4o 等 LLM评价生成文本是否符合人类偏好。</td></tr><tr><td>语音智能</td><td>能否基于不同的声学信息生成适当的响应。</td></tr><tr><td>音频智能</td><td>音频理解与生成：如语音情感识别准确度、自动音频字幕、声音事件检测、声学场景分类、音乐相关任务等。</td></tr><tr><td>语音生成质量</td><td>发音准确度（WER/CER）、清晰度和自然度（MOS）。</td></tr><tr><td>多语种能力</td><td>语音翻译任务、能否自动以用户使用的语言进行响应。</td></tr><tr><td>context learning</td><td>记忆能力、用户修改关键信息后的正确响应。</td></tr><tr><td>延迟</td><td>流式：first tokendelay（用户说完话到收到第一帧回复语音的延迟）。<br>turn-based：RTF。</td></tr><tr><td>交互能力</td><td>用户打断，适当的 backchannel、打断用户。</td></tr></tbody></table><h1 id="资源">10. 资源</h1><p>Table 6: A comprehensive list of publicly available codec models andtheir URL</p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>Model</th><th>URL</th></tr></thead><tbody><tr><td>Encodec [43]</td><td>https://github.com/facebookresearch/encodec</td></tr><tr><td>SoundStream [238]</td><td>https://github.com/wesbz/SoundStream</td></tr><tr><td>DAC [113]</td><td>https://github.com/descriptinc/descript-audio-codec</td></tr><tr><td>WavTokenizer [90]</td><td>https://github.com/jishengpeng/WavTokenizer</td></tr><tr><td>SpeechTokenizer [249]</td><td>https://github.com/ZhangXInFD/SpeechTokenizer</td></tr><tr><td>SNAC [193]</td><td>https://github.com/hubertsiuzdak/snac</td></tr><tr><td>SemantiCodec [135]</td><td>https://github.com/haoheliu/SemantiCodec-inference</td></tr><tr><td>Mimi [44]</td><td>https://github.com/kyutai-labs/moshi</td></tr><tr><td>HiFi-Codec [230]</td><td>https://github.com/yangdongchao/AcademiCodec</td></tr><tr><td>FunCodec [51]</td><td>https://github.com/modelscope/FunCodec</td></tr><tr><td>APCodec [4]</td><td>https://github.com/YangAi520/APCodec/tree/main</td></tr><tr><td>AudioDec [220]</td><td>https://github.com/facebookresearch/AudioDec</td></tr><tr><td>FACodec [100]</td><td>https://github.com/lifeiteng/naturalspeech3_facodec</td></tr><tr><td>Language-Codec [89]</td><td>https://github.com/jishengpeng/Languagecodec</td></tr><tr><td>XCodec [236]</td><td>https://github.com/zhenye234/xcodec</td></tr><tr><td>TiCodec [177]</td><td>https://github.com/y-ren16/TiCodec</td></tr><tr><td>SoCodec [70]</td><td>https://github.com/hhguo/SoCodec</td></tr><tr><td>FUVC [253]</td><td>https://github.com/z21110008/FUVC</td></tr><tr><td>HILCodec [3]</td><td>https://github.com/aask1357/hilcodec</td></tr><tr><td>LaDiffCodec [233]</td><td>https://github.com/haiciyang/LaDiffCodec</td></tr><tr><td>LLM-Codec [229]</td><td>https://github.com/yangdongchao/LLM-Codec</td></tr><tr><td>SpatialCodec [226]</td><td>https://github.com/XZWY/SpatialCodec</td></tr><tr><td>BigCodec [224]</td><td>https://github.com/Aria-K-Alethia/BigCodec</td></tr><tr><td>SuperCodec [254]</td><td>https://github.com/exercise-book-yq/Supercodec</td></tr><tr><td>RepCodec [86]</td><td>https://github.com/mct10/RepCodec</td></tr><tr><td>EnCodecMAE [164]</td><td>https://github.com/habla-liaa/encodecmae</td></tr><tr><td>MuCodec [225]</td><td>https://github.com/xuyaoxun/MuCodec</td></tr><tr><td>SPARC [32]</td><td>https://github.com/Berkeley-Speech-Group/Speech-Articulatory-Coding</td></tr><tr><td>BANC [172]</td><td>https://github.com/anton-jeran/MULTI-AUDIODEC</td></tr><tr><td>SpeechRVQ [188]</td><td>https://huggingface.co/ibm/DAC.speech.v1.0</td></tr><tr><td>QINCo [87]</td><td>https://github.com/facebookresearch/Qinco</td></tr><tr><td>SimVQ [257]</td><td>https://github.com/youngsheen/SimVQ</td></tr></tbody></table><h1 id="其它">11. 其它</h1><ul><li>Turn-taking cues 包含嗓音、韵律、呼吸、注视或手势。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语音对话数据集-开源</title>
      <link href="/blog/llm/yu-yin-dui-hua/shu-ju-ji/kai-yuan/"/>
      <url>/blog/llm/yu-yin-dui-hua/shu-ju-ji/kai-yuan/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"><col style="width: 8%"></colgroup><thead><tr><th>数据集</th><th>介绍</th><th>语种</th><th>音频采样率</th><th>对话数</th><th>对话时长 min(s)</th><th>max(s)</th><th>mean(s)</th><th>sum(h)</th><th>对话轮数 min</th><th>max</th><th>mean</th></tr></thead><tbody><tr><td>Fisher</td><td><a href="https://catalog.ldc.upenn.edu/LDC2004S13" class="uri">https://catalog.ldc.upenn.edu/LDC2004S13</a><br><a href="https://catalog.ldc.upenn.edu/LDC2005S13" class="uri">https://catalog.ldc.upenn.edu/LDC2005S13</a></td><td>EN</td><td>8000</td><td></td><td></td><td></td><td></td><td>1960</td><td></td><td></td><td></td></tr><tr><td>AnyInstruct</td><td><a href="https://huggingface.co/datasets/fnlp/AnyInstruct" class="uri">https://huggingface.co/datasets/fnlp/AnyInstruct</a><br>GPT-4生成文本多模态对话，分别合成图像、音乐、语音，得到模态交错的数据集，109k；<br>文本指令数据集过滤、TTS，107k。</td><td></td><td>24000</td><td>106770<br>排除3条音频为空</td><td>2.95</td><td>76.25</td><td>14.92</td><td>442.63</td><td>2</td><td>4</td><td>2.01</td></tr><tr><td>DailyTalk</td><td><a href="https://github.com/keonlee9420/DailyTalk" class="uri">https://github.com/keonlee9420/DailyTalk</a><br>开放域对话，录制。</td><td>EN</td><td>44100</td><td>2536<br>排除5条说话人未交替</td><td>3.94</td><td>102.93</td><td>29.31</td><td>20.65</td><td>4</td><td>20</td><td>8.88</td></tr><tr><td>MagicData-RAMC</td><td><a href="https://github.com/MagicHub-io/MagicData-RAMC" class="uri">https://github.com/MagicHub-io/MagicData-RAMC</a><br>15个领域的主题对话，录制。</td><td>Chinese Mandarin</td><td>16000</td><td>351</td><td>836.84</td><td>1979.43</td><td>1846.07</td><td>180</td><td></td><td></td><td></td></tr><tr><td>HQ-Conversations</td><td><a href="https://www.magicdatatech.cn/iscslp-2024" class="uri">https://www.magicdatatech.cn/iscslp-2024</a><br>日常生活场景对话。</td><td>Chinese Mandarin</td><td></td><td></td><td></td><td></td><td></td><td>100</td><td></td><td></td><td></td></tr><tr><td>NCSSD</td><td><a href="https://github.com/walker-hyf/NCSSD" class="uri">https://github.com/walker-hyf/NCSSD</a><br>电视剧 + ChatGPT生成对话脚本，人工录制。</td><td>ZH、EN</td><td>16000</td><td>19450<br>排除6条非对话</td><td>1.5</td><td>302.88</td><td>41.39</td><td>223.65</td><td>2</td><td>52</td><td>8.16</td></tr><tr><td>NMSQA train</td><td>https://github.com/DanielLin94144/DUAL-textless-SQA<br>SQuAD v1.1TTS，其中测试集人工录制。</td><td>EN</td><td>22050</td><td>18888<br>context 相同的拼接为多轮对话。第一轮对话用户发送 context +question，后续对话用户仅发送question。<br>由于计时方式与论文不一致，数据集时长有出入。</td><td>11.91</td><td>297.79</td><td>68.37</td><td>358.69</td><td>2</td><td>50</td><td>9.22</td></tr><tr><td>NCSSD dev</td><td></td><td>EN</td><td>22050</td><td>2066</td><td>17.07</td><td>260.45</td><td>73.6</td><td>42.24</td><td>2</td><td>60</td><td>10.16</td></tr><tr><td>NCSSD test</td><td></td><td>EN</td><td>22050</td><td>105</td><td>7.52</td><td>56.58</td><td>25.77</td><td>0.75</td><td>2</td><td>10</td><td>3.26</td></tr><tr><td>VoiceAssistant-400K</td><td><a href="https://arxiv.org/abs/2408.16725" class="uri">https://arxiv.org/abs/2408.16725</a><br>文本指令数据集TTS。</td><td>EN</td><td>22050；SNAC 合成 24000 Hz</td><td>251219<br>根据 split_name、index、round 拼接为多轮对话。<br></td><td>1.89</td><td></td><td></td><td>约 3000</td><td>2</td><td>68</td><td>3.74</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CosyVoice 2</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/cosyvoice-2/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/cosyvoice-2/</url>
      
        <content type="html"><![CDATA[<ul><li>Du, Zhihao et al. “CosyVoice 2: Scalable Streaming Speech Synthesiswith Large Language Models.” (2024).</li><li><a href="https://funaudiollm.github.io/cosyvoice2" class="uri">https://funaudiollm.github.io/cosyvoice2</a></li><li><a href="https://github.com/FunAudioLLM/CosyVoice" class="uri">https://github.com/FunAudioLLM/CosyVoice</a></li><li>官方公众号介绍 <a href="https://mp.weixin.qq.com/s/BgRxjrbnFOhPL99rzf3lYw" class="uri">https://mp.weixin.qq.com/s/BgRxjrbnFOhPL99rzf3lYw</a></li><li>作者来自阿里</li><li><font color="red">优化点</font><ul><li><font color="red">采用 FSQ（finite-scalarquantization，有限标量量化）替换 VQ，提升码本利用率（23% -&gt;100%），发音准确度显著提升。</font></li><li><font color="red">采用预训练的 LLM Qwen2.5-0.5B，删除 text encoder模块、speaker embedding。预训练 LLM文本语义建模能力更强，在可控生成、音频-文本的情感匹配、多音字发音上会有明显的收益。</font></li><li><font color="red">模型同时支持流式、非流式合成，且两者性能相当。语言模型采用文本、语音token 交错的数据格式 <span class="math inline">\(\text{N text tokens + Mspeech tokens}\)</span>，flow matching 采用 chunk-aware causalattention。接收5个文字就可以合成首包音频，延迟大致在150ms。</font></li><li><font color="red">多说话人同时SFT，以覆盖更多韵律和发音。同一模型也支持 zero-shot克隆音色、韵律和风格。支持更多细粒度的控制指令。</font></li><li><font color="red">增加 DPO 训练，进一步提升发音准确度。</font></li></ul></li><li>相较于 CosyVoice 1.0<ul><li>发音错误相对下降30%～50%，在Seed-TTS 测试集的 hard测试集上取得当前最低的字错误率。合成绕口令、多音字、生僻字上具有明显的提升。</li><li>跨语言语音合成具有明显提升。</li><li>合成音频的韵律、音质、情感匹配具有明显提升，支持更多细粒度的情感控制。</li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB31d2433f50b3376b10f220a426ada68f?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="804"></p><h2 id="text-tokenizer">1.1. Text Tokenizer</h2><p>去掉了1个 token 对应多个单词的tokens<font color="green">（仅针对中文？）</font>，避免1个 token对应的发音过长、数据稀疏导致的 corner cases。</p><h2 id="supervised-semantic-speech-tokenizer">1.2. Supervised Semantic SpeechTokenizer</h2><p>25Hz。</p><p><span class="math display">\[ \begin{aligned}\bar{H} &amp; =\operatorname{ROUND}\left(\operatorname{Proj}_{\text{down }}(H)\right) \\\hat{H} &amp; =\operatorname{Proj}_{\text {up }}(\bar{H}) \\\mu_i&amp;=\sum_{j=0}^{D-1} \bar{h}_{i, j}(2K+1)^j\end{aligned} \]</span> 其中，<span class="math inline">\(\operatorname{ROUND}\)</span> 表示有界 round <span class="math inline">\([-K, K]\)</span>。语音 token <span class="math inline">\(\mu_i\)</span> 通过将 <span class="math inline">\(\bar{H}\)</span> 视为 <span class="math inline">\((2 K+1)\)</span> 进制数转为十进制得到。</p><p>训练时采用 straight-through estimation 近似 <span class="math inline">\(\operatorname{ROUND}\)</span> 操作的梯度。</p><blockquote><p>将 round(x) 对 x 的导数视为1。</p></blockquote><h2 id="统一流式非流式">1.3. 统一流式、非流式</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5b0d852f327689b80c648c5bca7a1d04?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="796"></p><ul><li>流式模式下，若下一个为 text token，模型需要预测 filling token。</li><li>推理数据格式<ul><li>ICL (in-context learning) 非流式：<span class="math inline">\(\text{(S), prompt_text, text, (T), prompt_speech}\)</span></li><li>ICL 流式：<span class="math inline">\(\text {(S), mixed_text_speech,(T), remaining_speech}\)</span>，若文本 token 长度超过提示语音token，推理时需要手动将 filling token 替换为 text token。</li><li>SFT 非流式：由于已经进行了说话人微调，不需要提示语音。<span class="math inline">\(\text {(S), text, (T)}\)</span></li><li>SFT 流式：<span class="math inline">\(\text {(S),first_N_text}\)</span>，手动组织交错 token。</li></ul></li></ul><h2 id="flow-matching">1.4. Flow Matching</h2><ul><li><p>50Hz mel 谱, 24kHz 音频。</p></li><li><p>look-ahead convolution，右 padding：<span class="math inline">\(\text{pad}=P,\text{kernel}=P+1\)</span><font color="green">（待确认代码：pad tokenid）</font></p></li><li><p>2倍上采样到 50Hz。</p></li><li><p>chunk-aware causal convolutional Transformer UNet。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb705305143f52f91fc49c5bef2104a5d?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="192"></p><ul><li>Chunk-M Mask：可以看到历史和未来的 M 帧。第一个 chunk 采用 M以降低延迟，后续 chunk 可采用 2M 以实现更好的效果。</li></ul></li><li><p>condition: 提示文本-语音对、<a href="https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++">speakerembedding</a>、timestep <span class="math inline">\(t\)</span>。</p></li><li><p>训练</p><ul><li><p>batch 内每个样本从 非因果、完全因果、chunk-M、chunk-2M mask中均匀采样。</p></li><li><p>L1 loss</p></li><li><p>classifier-free guidance <span class="math inline">\(\beta=0.7\)</span>。</p></li><li><p>训练时 <span class="math inline">\(t\)</span> 服从均匀分布 <span class="math inline">\(U[0,1]\)</span>。推理时采用余弦 scheduler使得生成初期有更多 step。 <span class="math display">\[t:=1-\cos\left(\frac{1}{2} t \pi\right)\]</span></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdd7985586bb40419d906c6d31b9a4093?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="320"></p></li></ul></li><li><p><span class="math inline">\(\text{NFE}=10\)</span></p></li></ul><h1 id="训练">2. 训练</h1><h2 id="speech-tokenizer">2.1. speech tokenizer</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd5ee31b2fd20e1ea143eb3c99ed57372?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="329"></p><ul><li>20wh。label 做了归一化。</li><li>虽然仅包含中、英文数据，但可用于日语、韩语。</li></ul><h2 id="cosyvoice-2-训练集">2.2. CosyVoice 2 训练集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1eafd5563f823d1966ae8322684d641e?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="305"></p><ul><li>搜集仅语音数据：语音检测、SNR 估计、说话人 diarization、分离。</li><li>生成伪标签：Paraformer、SenseVoice。</li><li>强制对齐：过滤低质量数据、增强标点的准确性。</li></ul><h2 id="指令集">2.3. 指令集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc84b9064c3b4e589c942f97889fc1ba8?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="755"></p><p>1500h。与基础训练集混合。</p><h2 id="多说话人-sft">2.4. 多说话人 SFT</h2><ul><li>说话人提示：<code>Speaker A&lt;|endofprompt|&gt;</code>，未知说话人<code>unknown&lt;|endofprompt|&gt;</code>。</li><li>400 条音频就可以达到较好的合成质量。</li></ul><h2 id="llm-dpo">2.5. LLM DPO</h2><p><span class="math display">\[ L_{DPO}\left(\pi_\theta ;\pi_{\mathrm{ref}}\right)=-\log \sigma\left(\beta \log\frac{\pi_\theta\left(\mu^w \mid y\right)}{\pi_{\mathrm{ref}}\left(\mu^w\mid y\right)}-\beta \log \frac{\pi_\theta\left(\mu^l \midy\right)}{\pi_{\mathrm{ref}}\left(\mu^l \mid y\right)}\right) \]</span>其中，<span class="math inline">\(\mu^w\)</span>、<span class="math inline">\(\mu^l\)</span> 分别为偏好、拒绝样本的语音token。</p><p>可以将 WER、说话人相似度作为奖励函数。</p><p>将语音 token 转换为音频耗时，将音频 token 还原为量化表示 <span class="math inline">\(\bar{H}\)</span>，送入 speech tokenizer的后半部分，计算输入文本的负对数后验，作为 ASR 奖励函数。</p><p><span class="math display">\[ \begin{gathered}\bar{h}_{i, j}=\left\lfloor\frac{\mu_i}{(2K+1)^j}\right\rfloor \bmod(2K+1) \\\hat{H}=\operatorname{Proj}_{up}(\bar{H}) \\L_{ASR}=-\log P\left(Y \mid \hat{H} ; \theta_{ASR}\right)\end{gathered} \]</span></p><p>由于 <span class="math inline">\(P\left(\mu_i \mid \mu_{1: i-1}, Y ;\theta_{LM}\right)\)</span> 进行了采样，采用 gumbel softmax计算梯度。</p><blockquote><p>直接优化 softmax 分布，模型可能无法正确学习到“采样”生成离散 token的不确定性对序列质量的影响；会导致生成序列趋向于概率分布的最大值（即贪心生成），可能会降低生成序列的多样性；Gumbel-Softmax通过引入噪声模拟采样行为，同时可微，保留了梯度传递能力。</p><p>预训练采用 softmax 和 CE loss。因为预训练的目标是建模 token的条件概率分布，而不是生成序列本身的采样行为；预训练阶段通常需要处理大规模数据集，引入Gumbel-Softmax 会增加计算复杂度和数值不稳定性；实践证明直接优化 softmax已足够有效。采样行为主要用于下游任务微调。</p></blockquote><h1 id="评价">3. 评价</h1><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th></th><th>评价方式</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>tokenizer</td><td>1. 码本利用率<br>2. WER<br>3. 不同说话人的 token 可视化<br>4.说话人识别训练-S3prl</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc33f6434dce3bbe84f359223b881461e?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="595"><br>3.量化前，不同说话人的语音表示分布有差异；量化后，几乎不可分。见论文图4。<br>4.采用量化前的表示训练可收敛；采用量化后的表示训练不收敛。见论文图5。</td><td>1+2: VQ -&gt; FSQ，码本利用率显著提升（23% -&gt;100%），发音准确度显著提升。<br>3+4:量化前的语音表示包含说话人信息，量化后解耦。</td></tr><tr><td>TTS</td><td>说话人相似度：两个模型 WavLM-finetuned SV、ERes2Net，提取说话人embedding。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2e262409db07da02afe283d04fd2cc63?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="606"><br>CosyVoice2-S：流式。</td><td>流式模式下说话人相似度更高，可能由于流式合成初期prompt-to-generation ratio 较高，而非流式合成由于<font color="green">padding token？</font> 该比例较低。</td></tr><tr><td></td><td><a href="https://github.com/BytedanceSpeech/seed-tts-eval" class="uri">https://github.com/BytedanceSpeech/seed-tts-eval</a></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3c543a0354cc00a845f3027117bf40d2?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="598"></td><td></td></tr><tr><td>日语、韩语</td><td>测试集已开源</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf15f3a954b08053c2912a667e4f993bd?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="599"></td><td></td></tr><tr><td>指令 TTS</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1635d6745250fbd8cf1bfbe0e80f50d4?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="599"></td><td>韵律控制很难从文本内容隐式地推断学习。</td></tr><tr><td>说话人微调</td><td>CosyVoice 2 SFT 模型中英文CER/WER。<br><font color="green">说话人相似度？</font></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBd906007d51d343485243c4108816c398?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="602"><br>SpkE 只有中文训练数据。</td><td></td></tr><tr><td>DPO</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB09ffdc5cce8a5620f9fe6240a5f1b6c4?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="598"></td><td>DPO 训练对发音准确度有显著提升。</td></tr><tr><td>流式</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB45a7273fc46d8d342df5a5dbcb5606f1?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="597"><br>FM:flow matching</td><td>流式、非流式合成性能差异较小，仅在难样本上发音准确性有下降。</td></tr><tr><td>消融实验</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa50f6889e8385e73b25a31ea09ba3d28?method=download&shareKey=1b38fd65280c3803eb1a1ef02c131c07" width="570"><br>pitchloss：加入 speech tokenizer 训练。</td><td>1. LLM初始化：发音准确性、说话人相似度均有显著提升，文本语义建模能力更强，在可控生成、音频-文本的情感匹配、多音字发音上会有明显的收益。<br>2.speaker embedding 还包含语种和副语言信息，影响跨语言能力，LM 移除speaker embedding 输入后识别错误显著减少，说话人相似度由后续的 flowmatching 保留。<br>4. speech tokenizer 训练加入 pitchloss，识别错误显著减少，但说话人相似度略有下降。</td></tr></tbody></table><ul><li>存在的问题<ul><li>支持有限的语种。对于有重叠字符集的语种，合成质量可能会退化。</li><li>不支持文本指令控制音色，如角色扮演应用。</li><li>唱歌。</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语音对话数据集-音视频处理</title>
      <link href="/blog/llm/yu-yin-dui-hua/shu-ju-ji/yin-shi-pin-chu-li/"/>
      <url>/blog/llm/yu-yin-dui-hua/shu-ju-ji/yin-shi-pin-chu-li/</url>
      
        <content type="html"><![CDATA[<table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>步骤</th><th>目的</th><th>工具/操作</th></tr></thead><tbody><tr><td>声源分离</td><td>去除 BGM</td><td><a href="https://github.com/TRvlvr/model_repo/releases/download/all_public_uvr_models/UVR-MDX-NET-Inst_HQ_3.onnx">UVR-MDX-NET-Inst_HQ_3</a></td></tr><tr><td>说话人分离（含 VAD）</td><td></td><td><a href="https://huggingface.co/pyannote/speaker-diarization-3.1#requirements">pyannote/speaker-diarization-3.1</a></td></tr><tr><td>Cut &amp; Merge，得到接近 30s 的 chunk</td><td>减少 ASR 模型幻觉、便于 batch ASR</td><td>可参考 WhisperX</td></tr><tr><td>ASR</td><td></td><td>英文 whisper large V3、中文 paraformer 等</td></tr><tr><td>强制对齐</td><td></td><td><a href="https://github.com/m-bain/whisperX/issues/424">wav2vec2.0large</a> 或 Whisper-timestamped</td></tr></tbody></table><ul><li>中间过滤步骤：无说话人重叠、WER 低于指定阈值、<a href="https://github.com/microsoft/DNS-Challenge/blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx">DNSMOSP.835</a> &gt; 3.0</li><li>数据源：访谈、播客、影视剧对话数据。</li></ul><h1 id="参考">1. 参考</h1><h2 id="emilia-pipe">1.1. Emilia-Pipe</h2><p><a href="https://github.com/open-mmlab/Amphion/blob/main/preprocessors/Emilia/README.md">Emiliapipeline</a>，用于 TTS。</p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>步骤</th><th>工具/操作</th></tr></thead><tbody><tr><td>音频归一化</td><td></td></tr><tr><td>声源分离<br>去除 BGM</td><td><a href="https://github.com/TRvlvr/model_repo/releases/download/all_public_uvr_models/UVR-MDX-NET-Inst_HQ_3.onnx">UVR-MDX-NET-Inst_HQ_3</a></td></tr><tr><td>说话人分离</td><td><a href="https://huggingface.co/pyannote/speaker-diarization-3.1#requirements">pyannote/speaker-diarization-3.1</a></td></tr><tr><td>VAD<br>获取 3-30s 单说话人“短”语音片段</td><td><a href="https://github.com/snakers4/silero-vad">snakers4/silero-vad</a></td></tr><tr><td>ASR</td><td><a href="https://github.com/m-bain/whisperX">Whisperx-medium</a></td></tr><tr><td>过滤</td><td>目标语种、whisper 置信度 &gt;= 80%、<a href="https://github.com/microsoft/DNS-Challenge/blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx">DNSMOSP.835</a> &gt; 3.0、平均字符持续时间</td></tr></tbody></table><ul><li>TODO<ul><li>更新声源分离模型以更好地处理嘈杂的音频（例如混响）。</li><li>将 VAD 移至第一步以过滤掉非语音片段（以提高速度）。</li><li>提高标点符号的转录准确性。</li><li>标记：说话人信息（例如性别、年龄、母语、健康状况）、情绪、说话风格（音调、语速、口音）、非语言线索（例如笑声、咳嗽、沉默、filters）、副语言特征等。</li></ul></li><li>存在的问题：pyannote结果中说话人时间段存在重叠，即存在一个时刻在多个说话人的片段中。</li></ul><h2 id="whisperx">1.2. WhisperX</h2><p><a href="https://github.com/m-bain/whisperX" class="uri">https://github.com/m-bain/whisperX</a></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>步骤</th><th>工具/操作</th><th>存在的问题</th></tr></thead><tbody><tr><td>VAD Cut &amp; Merge（接近30s）<br>减少幻觉、batch ASR，降低了WER</td><td>pyannote VAD</td><td></td></tr><tr><td>ASR</td><td>whisperlarge-v2<br><code>without_timestamps=True</code>、<code>condition_on_prev_text=False</code>以减轻幻觉，greedy 解码。</td><td>无法处理重叠语音</td></tr><tr><td><font color="green">音素级</font>强制对齐</td><td>nltk PunktSentenceTokenizer 分句<br>各语种的 <a href="https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self">wav2vec2.0large</a></td><td>词典外的词无法对齐，如"2014."、"£13.60"。<br>英语默认加载的对齐模型似乎为字符级，音素级可参考<a href="https://github.com/m-bain/whisperX/issues/424">issue#424</a></td></tr><tr><td>音频级说话人分离</td><td><a href="https://huggingface.co/pyannote/speaker-diarization-3.1">pyannote/speaker-diarization-3.1</a></td><td>"Diarization is far from perfect"。<br>对识别结果中的每个 segment选取其中时长占比最大的说话人</td></tr></tbody></table><h2 id="whisper-diarization">1.3. whisper-diarization</h2><ul><li><a href="https://github.com/MahmoudAshraf97/whisper-diarization" class="uri">https://github.com/MahmoudAshraf97/whisper-diarization</a></li><li>【弃】无处理流程说明文档</li><li>ASR: whisper</li><li>对齐: ctc-forced-aligner</li><li>VAD: MarbleNet</li><li>说话人分离：TitaNet</li><li>punctuation</li></ul><h2 id="pyannote-audio">1.4. pyannote-audio</h2><ul><li><a href="https://github.com/pyannote/pyannote-audio" class="uri">https://github.com/pyannote/pyannote-audio</a></li><li>VAD -&gt; 说话人转换检测 -&gt; 语音 turn 表示 -&gt; 语音 turn聚类</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语音对话评价方案</title>
      <link href="/blog/llm/yu-yin-dui-hua/ping-jie/"/>
      <url>/blog/llm/yu-yin-dui-hua/ping-jie/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://mp.weixin.qq.com/s/HZ69UiUJdH_g-l9qaku6eQ">SuperCLUE实时语音交互中文基准 2024.12</a></li><li><a href="https://mp.weixin.qq.com/s/X5xxHYp8dyL4PMU9gTUS-A">SuperCLUE实时音视频中文基准 2025.1</a></li><li><a href="https://arxiv.org/pdf/2410.17196">VoiceBench: BenchmarkingLLM-Based Voice Assistants</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hertz-dev</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/hertz-dev/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/hertz-dev/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://github.com/Standard-Intelligence/hertz-dev" class="uri">https://github.com/Standard-Intelligence/hertz-dev</a></p></li><li><p><a href="https://si.inc/hertz-dev/" class="uri">https://si.inc/hertz-dev/</a></p></li><li><p>hertz-codec</p><ul><li>VAE，采用因果卷积，<font color="green">6s 感受野，</font>输出 8Hz32维 latent。</li><li>采用重建、对抗、KL 正则损失训练。</li><li>参数量：encoder 5M、decoder 95M。</li></ul></li><li><p>hertz-codec quantizer：MLP + FSQ。将 latent 投影为语音学相关的15-bit 量化表示。</p></li><li><p>hertz-ar：8.4B，40层 decoder-only transformer。</p><ul><li>前 32 层输入历史 latent 预测 next 15-bit 量化 latent。随机初始化或从LM 初始化。</li><li>后8层基于历史 latent 和 15-bit 量化 latent 预测 next latent。</li><li>上下文长度为 2048，约 4.5min。</li></ul></li><li><p>训练</p><ul><li>duplex post-training。</li></ul></li><li><p>延迟：理论 80ms，实际 RTX 4090 120ms = 62.5ms(给定任意句子到得到1个 token 的平均时间) + forward +往返网络延迟。</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>VITA</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/vita/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/vita/</url>
      
        <content type="html"><![CDATA[<ul><li>Fu C, Lin H, Long Z, et al. Vita: Towards open-source interactiveomni multimodal llm[J]. arXiv preprint arXiv:2408.05211, 2024.</li><li><a href="https://vita-home.github.io" class="uri">https://vita-home.github.io</a></li><li><a href="https://github.com/VITA-MLLM/VITA" class="uri">https://github.com/VITA-MLLM/VITA</a>开源了模型、训练代码、推理部署框架。</li><li>作者：来自腾讯优图等</li><li><font color="red">创新点</font><ul><li><font color="red">无需唤醒：采用 SileroVAD判断输入音频是否包含语音；若为语音，模型响应中第一个 token 为 statetoken，判断输入语音是否为提问，若非查询则停止生成。</font></li><li><font color="red">双工部署：同时部署两个 VITA模型。一个为当前查询生成响应；另一个持续检测输入音频，若有新查询，打断当前生成，聚合历史context，为新查询生成响应；两个模型交换身份。</font></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4a0ad9fa6e368f7347382dd47a645bf9?method=download&shareKey=54b409c0d3539cf3120b23fab6da3674" width="809"></p><ul><li>支持中英文视频、图像、文本、音频输入。级联 TTS。</li></ul><h1 id="训练">2. 训练</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB48cf38f77a9bcb6fd0e9768fdc47eab9?method=download&shareKey=da1f410371158978b5c63ee0329c5f02&inline=true" width="803"></p><ul><li>提升 LLM 中文能力<ul><li>基于 <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">Mixtral8×7B</a>，扩充中文词（32000-&gt;51747），进行中英双语纯文本指令微调（数据量500w）。</li></ul></li><li>多模态对齐<ul><li>添加 visual encoder、audio encoder、各自的 2 层 MLP connector。</li><li>视觉模态<ul><li>encoder：<a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT-300M-448px</a></li><li>高分辨率图像：采用动态分块策略 [8] 来捕获局部细节。</li><li>视频：采样图像帧。&lt;4s，均匀采样 4 帧；[4,16]s，1帧/s；&gt;16s，均匀采样 16帧。不进行动态分块以控制序列长度。</li><li>视频 QA：重新标注 Video-ChatGPT、VideoChat。</li><li>synthetic data: 采用 MLLM 模型生成回复。</li><li>训练：拼接 QA 对使样本长度为 6k，支持长上下文，提高训练效率。</li></ul></li><li>音频模态<ul><li>encoder：CNN 下采样 + 24层 Transformer，12.5Hz。</li><li>训练集：ASR: Wenetspeech + Gigaspeech；音频字幕：Wavcaps AudioSet SL子集。</li></ul></li></ul></li><li>多模态指令微调<ul><li>在响应开头插入状态 token，实现无需音频唤醒。</li><li>文本指令 TTS：<a href="https://github.com/RVC-Boss/GPT-SoVITS">GPT-SoVITS</a></li><li>不需要响应的语音：从答案中抽取，TTS。由于仅输出 &lt;2&gt;性能显著下降，训练时将 LLM 对其文本的响应作为 label，推理时将 &lt;2&gt;视为 EOS。</li><li>通过提示区分图像、视频。</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB122e2cd2b4c94b73b1e9e9e6675a769b?method=download&shareKey=6e70dfe9846031258ff963c6c37a5eaa&inline=true" width="813"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SyncLLM</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/syncllm/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/syncllm/</url>
      
        <content type="html"><![CDATA[<ul><li>Veluri B, Peloquin B N, Yu B, et al. Beyond Turn-Based Interfaces:Synchronous LLMs as Full-Duplex Dialogue Agents[J]. arXiv preprintarXiv:2409.15594, 2024.</li><li><a href="https://syncllm.cs.washington.edu/" class="uri">https://syncllm.cs.washington.edu/</a></li><li>作者：来自 Meta AI、华盛顿大学</li><li><font color="red">创新点</font><ul><li><font color="red">通过对交替的用户语音 token chunk、系统语音 tokenchunk 自回归生成建模，支持全双工口语对话。</font></li><li><font color="red">重复的语音 token影响模型的语义能力，去重。</font></li><li><font color="red">考虑 240ms网络延迟，可能无法立即访问用户语音，采用延迟输入模式。交替地生成系统语音chunk N、用户语音 chunk（用户接下来可能说的话）、系统语音 chunkN+1，但预测后续系统语音 chunk <span class="math inline">\((\mathrm{N}+2,\mathrm{~N}+3, \ldots)\)</span> 时，替换为用户对 chunk N的真实响应。</font></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3823dab3b33e456b07779fd6fc2edba4?method=download&shareKey=cad04b66be0bea1f303f3483b964a1f1" width="690"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB85b7c0eece7cf97f0ff71dcc71021f8f?method=download&shareKey=9a61ab6ab1a3648c4dba252f741ddd0b" width="689"></p><ul><li>Llama3-8b，上下文长度 8192。</li><li>采用 speaker special token [S0] [S1] 分别指示两个说话人的 token序列起始位置。</li><li>语音 tokenizer 与合成：参照 SPIRIT LM，采用 HuBERT +HiFi-GAN，25Hz，词典大小 501。</li><li>重复的语音 token 影响模型的语义能力，去重。通过上述 speaker tag保留粗略的时间信息。注意：若 [S1] 新 chunk 不包含新的 token，删除 [S1]tag，但若 [S0] 出现上述情形，保留 [S0] 以保留时间信息。</li><li>语音合成前对 token 序列插值：若 chunk 内有多个 token，每个 token重复相同的次数。观察到这种粗略的方式的影响难以察觉，可能由于误差上限被chunk 时长限制，且若包含多个 token，误差会更小。</li></ul><h1 id="阶段训练">2. 3阶段训练</h1><ul><li><p>turn-based 对话 <span class="math display">\[\text{[S1]&lt;sent0&gt;[S0]&lt;sent0&gt;&lt;sent1&gt;[S1].. }\]</span></p><p>每个句子随机选择采用文本形式或去重的语音token，从下图的截断正态分布中采样序列中语音句子的百分比。相较于纯语音或逐步增加语音占比，训练更稳定。（附录A.2 句子级语音-文本交错优于 turn 级交错）</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3e227beb79ff9cc5cd504779173a243d?method=download&shareKey=a545540cf98966054375499fcb179cca" width="336"></p><ul><li>数据：文本 SFT 数据 Bark TTS，193kh。</li></ul></li><li><p>无重叠的全双工对话，chunk 交替，一人说话人时另一人为静音。</p><ul><li>数据：文本对话数据 Bark TTS，20kh。SFT数据中系统回复较长，对话数据中 turn-taking 更频繁，语句较短。</li></ul></li><li><p>真实口语对话数据。Fisher 2kh。</p></li><li><p>训练超参数详见附录 A.1</p></li></ul><h1 id="评价">3. 评价</h1><ul><li><p>给定 10s 对话提示生成延续的 10-30s对话，忽略重叠语音，用文本对话模型计算 ASR 文本的 PPL。结果见论文图5。</p></li><li><p>上述提示分别从 Fisher （集内）和Candor（集外）测试集中采样。结果见论文图 6。</p></li><li><p>给定 30s 对话提示生成延续的 90s 对话，计算与 ground truth 延续的turn-taking event 时长的 Pearson correlation。结果见论文表 2。</p><p>* Resynth-GT 考虑了本文的 tokenization 策略，作为方案上限。</p></li><li><p>人工评价 N-MOS（turn-taking自然度）、M-MOS（Meaningfulness，内容连贯合理、可理解）。结果见论文表3。</p><ul><li>评价指南详见附录 B。</li></ul></li><li><p>给定对话提示，用两个模型实例对话生成延续，考虑 one-chunk延迟。统计 ASR 文本的 PPL、人工 MOS 评分、turn-taking event 时长的Pearson correlation。分别见论文图 7、表 4、表 6。<font color="red">chunksize 为 160/200ms 时性能相当，240ms 时性能下降。</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2de188a67b0606d85f10c348f8141985?method=download&shareKey=b87b04b26656943b914ab372ba6cbf11" width="323"></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OmniFlatten</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/omniflatten/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/omniflatten/</url>
      
        <content type="html"><![CDATA[<ul><li>Zhang Q, Cheng L, Deng C, et al. OmniFlatten: An End-to-end GPTModel for Seamless Voice Conversation[J]. arXiv preprintarXiv:2410.17799, 2024.</li><li><a href="https://omniflatten.github.io/" class="uri">https://omniflatten.github.io/</a></li><li>作者：来自阿里通义实验室</li><li><font color="red">创新点：不进行计算密集型的预训练，而是通过 10whASR、TTS 任务进行模态对齐。</font>相较于 SyncLLM，不对音频离散 token去重，避免重建时引入的误差导致退化。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBfd17db7ed1e694c16cfad004cf9e5018?method=download&shareKey=a02ef1318c7f19e3d18fb9f2ca7e3c22" width="698"></p><ul><li>离散语音 tokenizer: 同 CosyVoice，单码本 4096，25Hz。</li><li>Qwen2-0.5B。</li><li>采用固定大小的文本 token chunk size = 2、音频 token chunk size =10。文本结束后填充 <span class="math inline">\(\text{silent_text_token}\)</span>，音频流填充 <span class="math inline">\(\text {silent_speech_token}\)</span>。</li></ul><h1 id="训练">2. 训练</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB13b06bb475f1846d6a29e6b723c294fc?method=download&shareKey=fd70b43e04e492fec9f775a7266019c4" width="578"></p><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>多阶段后训练</th><th>数据</th><th>数据集</th><th>训练策略</th></tr></thead><tbody><tr><td>ASR + TTS</td><td><span class="math inline">\([ASR][SOS] \mathrm{S} _ \text{seq}[EOS][SOT] \mathrm{T} _\mathrm{seq}[EOT] \\ [TTS][SOT] \mathrm{T}_\mathrm{seq}[EOT][SOS] \mathrm{S} _ \text {seq}[EOS]\)</span></td><td>10wh 中英，Aishell-3 + LibriTTS + TED-LIUM + VoxPopuli + Librispeech+ MLS + 自有数据集</td><td>最大序列长度 1024。</td></tr><tr><td>flatten 4 流</td><td>用户+系统 文本+音频</td><td>36w turn-based 多轮对话，2kh，文本对话数据集 TTS。</td><td>最大序列长度 8192。<br>mask 用户通道 loss训练更稳定，可能由于用户通道进行了加噪。</td></tr><tr><td>flatten 3 流</td><td>上述去除用户文本</td><td></td><td></td></tr><tr><td>flatten 2 流</td><td>仅音频</td><td></td><td></td></tr></tbody></table><p>* batch size: 1亿 token。</p><ul><li>训练框架：<a href="https://github.com/karpathy/nanoGPT/tree/master">NanoGPT</a></li><li>语音对话数据集构建：高质量文本对话数据集 TTS<ul><li>Alpaca、Moss、BelleCN、ultraChat<ul><li>过滤：含大量非文本元素（如代码、数学表达式）、中英文超过 200个单词、包含不常见的符号等。</li></ul></li><li>CosyVoice TTS<ul><li>用户说话人 embedding 从 Librispeech、3DSpeaker中采样。用户通道加噪，MUSAN，SNR [15, 25]dB。</li></ul></li><li>用户语音结束后助手立即回复，助手语音结束后、用户开始说话前采样停顿时长。</li></ul></li></ul><h1 id="评价">3. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcb0e6762b605fa1f180ec3839885d1fd?method=download&shareKey=8b25eaa03696952671f9e646b74f2e48" width="572"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa260cc2a26dbc448db8760136daeef32?method=download&shareKey=70e936e1f4329ece4a6ed144996b01c5" width="574"></p><p>* <a href="https://help.aliyun.com/zh/model-studio/developer-reference/use-qwen-by-calling-api">QWen-max</a></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2d37acf019e0623d345e802f5b6b4f9c?method=download&shareKey=8bac17fdc78a3aa2f54520cc59769a96" width="282"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB58c32ed1ac62ba092fc83f095d4f1e6f?method=download&shareKey=96a4be923da7344deb2a32bb059e7d5d" width="691"></p><ul><li>Assistant Turn-taking Acc@k: 用户有语义的语音 token 结束后第 k 个token 助手正确预测为非 silent token。</li><li>User Turn-taking Acc@k: 助手说话时用户输入有语义的语音 token，第 k个 token 后助手正确预测为 silent token。</li><li><font color="red">taking turn 平均响应时间：系统 160ms，用户805ms。</font>训练集未覆盖用户打断助手语音的数据。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>全双工语音对话系统</title>
      <link href="/blog/llm/yu-yin-dui-hua/quan-shuang-gong-fang-an/"/>
      <url>/blog/llm/yu-yin-dui-hua/quan-shuang-gong-fang-an/</url>
      
        <content type="html"><![CDATA[<h1 id="系统结构">1. 系统结构</h1><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>提出时间</th><th>系统</th><th>作者所属机构</th><th>方案</th><th>备注</th></tr></thead><tbody><tr><td>2022.3</td><td><a href="https://nanyang2015.github.io/blog/yin-pin/yu-yin-dui-hua/dgslm/">dGSLM</a></td><td>Meta AI 等</td><td>参数共享的两个 Transformer，分别自回归生成对话中一方的音频，通过cross attention 交互。预测音频 token 和 delayed持续时间。支持重叠语音，系统打断用户、用户打断系统。</td><td><font color="green">语音助手类应用中不需要预测用户语音。后续研究中不再使用cross attention 结构是由于双 LLM显存占用加倍吗？个人感觉扩充更多模态（如视频）较方便。</font></td></tr><tr><td>2024.8.9</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/vita/">VITA</a></td><td>腾讯优图等</td><td>VAD +模型监测VAD片段是否为提问；<br>同时部署两个模型，一个生成响应，另一个监测用户语音输入。</td><td>部署成本较高。<br>级联 TTS。</td></tr><tr><td>2024.9.17</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/moshi/">Moshi</a></td><td>Kyutai</td><td>输入为用户音频、系统音频、系统文本 embedding相加，生成系统回复文本、音频。支持重叠语音，系统打断用户、用户打断系统。</td><td></td></tr><tr><td>2024.9.23</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/syncllm/">SyncLLM</a></td><td>Meta AI、华盛顿大学</td><td>用户、系统音频 token chunk 交替。<br>考虑网络延迟，多预测1个用户chunk、回复 chunk。</td><td></td></tr><tr><td>2024.10.15</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/mini-omni/">Mini-Omni2</a></td><td>Inspirai、清华</td><td>总体上 turn-based，双头并行预测文本、音频token。仅支持生成中倾听用户音频，同步预测帧级状态token（是否被用户打断），停止生成。</td><td>应用中用户仍需要发起录音、结束录音请求回答，也可通过应用中的停止操作打断生成。</td></tr><tr><td>2024.10.23</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/omniflatten/">OmniFlatten</a></td><td>阿里通义实验室</td><td>用户音频 token chunk、系统文本 token chunk、系统音频 token chunk交替。</td><td></td></tr><tr><td>2024.10.25</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/glm-4-voice/">GLM-4-Voice</a></td><td>智谱</td><td>输入用户音频，系统回复文本 token chunk 与音频 token chunk交替。</td><td><font color="green">宣传稿声称支持实时打断，从 demo 看是turn-based。</font></td></tr><tr><td>2024.11.1</td><td><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/freeze-omni/">Freeze-Omni</a></td><td>腾讯优图、西工大、南大</td><td>VAD + speech encoder + LLM + TTS，采用 VAD检测用户语音输入，流式输入用户音频，LLM 预测状态：沉默或开始生成。</td><td>原则上也可以利用 VAD 打断模型生成。</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Freeze-Omni</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/freeze-omni/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/freeze-omni/</url>
      
        <content type="html"><![CDATA[<ul><li>Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Modelwith Frozen LLM</li><li><a href="https://arxiv.org/pdf/2411.00774v1" class="uri">https://arxiv.org/pdf/2411.00774v1</a></li><li><a href="https://freeze-omni.github.io/" class="uri">https://freeze-omni.github.io/</a></li><li>作者：来自腾讯优图、西工大、南大</li><li><font color="red">创新点：【VAD + speech encoder + LLM + TTS】冻结LLM 避免灾难遗忘。采用 <a href="https://github.com/snakers4/silero-vad">VAD</a>检测“用户开始语音输入”。采用语音 encoder + adapter 接收 chunk-wise流式语音输入，LLM 预测状态：0 继续接收语音，1语音结束开始生成，<font color="green">2 语音结束不需要打断</font>。基于LLM 文本输出 + 隐藏状态，采用 NAR+AR 生成语音 token，采用流式 codec生成音频。系统平均延迟约1.2s。</font><font color="green">存疑：也可以利用 VAD打断模型生成？</font></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf972ed8dc6a6cdaf84ef64642c7d97e2?method=download&shareKey=52557e65674b126bb567f130523821ad" width="602"></p><ul><li>语音 encoder：4倍下采样多层卷积 + 24层 Transformer。输入为FBANK，10ms 帧移。</li><li>adapter：2倍下采样多层卷积。与 encoder 合计 350M 参数量，12.5Hz帧率。</li><li>LLM：Qwen2-7B-Instruct</li><li>NAR/AR decoder：4层 Llama decoder 层</li><li>流式 codec：<a href="https://github.com/y-ren16/TiCodec">TiCodec</a>，单码本，码本大小1024，帧率 40Hz，输出 24kHz 音频。输入为固定 chunk size 40。</li></ul><h1 id="训练">2. 训练</h1><ul><li><p>语音输入 3 阶段训练</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbd65061dcdfe2e762861dadf1a66d1cc?method=download&shareKey=61dbd99983ac7586098669bef0ae891f" width="600"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcbe37f6ae151285be8b526645867bb85?method=download&shareKey=755228648f266fa6023f1eaaeb55ff19" width="599"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>阶段</th><th>可训练的模块</th><th>任务</th></tr></thead><tbody><tr><td>1</td><td>encoder</td><td>11wh 中英 ASR，CTC 损失</td></tr><tr><td>2</td><td>encoder + adapter + 添加的 special tokens</td><td>ASR。<br>可采用 Wenet 中的动态 chunk 训练，本文采用固定chunk=4。</td></tr><tr><td>3</td><td>prompt embedding。<br>每个问题采用相同的 prompt embedding</td><td>6w <a href="https://huggingface.co/datasets/fnlp/moss-003-sft-data">moss-003-sft-data</a>语音-文本多轮问答（答案由冻结的文本 LLM 生成）+ chunk-wise打断状态预测</td></tr></tbody></table></li><li><p>语音输出 3 阶段训练</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc0cb3035a75a1ca8082d0a360512155f?method=download&shareKey=5444029dc55885d96bac1fb54800c4c2" width="601"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>阶段</th><th>可训练的模块</th><th>任务</th></tr></thead><tbody><tr><td>1</td><td>codec</td><td>语音编解码</td></tr><tr><td>2</td><td><font color="green">NAR decoder + ARdecoder（两者参数相同）</font></td><td>3kh TTS。输入为 LLM text embedding，输出为 codec 的语音 token。</td></tr><tr><td>3</td><td>NAR prefix decoder</td><td>文本-语音多轮问答。输入为 LLM 的隐藏状态和生成的文本token，输出为答案 TTS 音频的 codec token。</td></tr></tbody></table></li></ul><h1 id="评价">3. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd2a90ee6d8c29c6861d69822732fefbe?method=download&shareKey=291b42731a353ffd5c20dc3bdfbdcbc1" width="602"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa19e7f12d7491db0bb0fdb752fcad057?method=download&shareKey=64d38a0b8f82a2dae564dd85252660b0" width="589"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB64c78a3c26d609bc19075a16b67a5acf?method=download&shareKey=820a2924e456f7dda87a13f117e64d7e" width="607"></p><ul><li><a href="https://github.com/google-research-datasets/LLAMA1-Test-Set">LlaMA-Questions</a>、<a href="https://huggingface.co/datasets/Stanford/web_questions">WebQuestions</a>、<a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a>。Web Questions 和 Trivia QA 采用 <a href="https://github.com/rany2/edge-tts">edge-tts</a> 合成问题。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7670801929419a6a6ccb9b17229d318c?method=download&shareKey=3e7c53b637bba853860ebcff2f5909a3" width="601"></p><ul><li>语音结束到 LLM 输出中断状态需要手动统计，约 160-320ms。</li><li>文本 token 分段基于 sentence-split。</li><li>考虑网络延迟约 200-300ms。系统平均延迟约 1.2s。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Mini-Omni</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/mini-omni/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/mini-omni/</url>
      
        <content type="html"><![CDATA[<ul><li>Xie Z, Wu C. Mini-omni2: Towards open-source gpt-4o model withvision, speech and duplex[J]. arXiv preprint arXiv:2410.11190,2024.</li><li>Xie Z, Wu C. Mini-Omni: Language Models Can Hear, Talk WhileThinking in Streaming[J]. arXiv preprint arXiv:2408.16725, 2024.</li><li><a href="https://github.com/gpt-omni/mini-omni2" class="uri">https://github.com/gpt-omni/mini-omni2</a></li><li><a href="https://github.com/gpt-omni/mini-omni" class="uri">https://github.com/gpt-omni/mini-omni</a></li><li>数据集：<a href="https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K" class="uri">https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</a></li><li>作者：来自 Inspirai、清华。</li><li><font color="red">特点：语音、图像输入采用 encoder + adapter，与文本embedding 拼接。多个输出头，同时输出文本、音频多码本 token，采用1-1-1-1-1-1-1 delay 模式。音频流式合成。</font><ul><li>出发点：ASR 训练得到的表示与语义强对齐。离散 token不足以可靠地传达语音内容（相较于语义表示，用于 ASR性能较差）；一段音频的 token 受前后内容影响（Moshi: 生成音频-重新编码不一致；对时间偏移不鲁棒）；训练中音频 loss较高。<font color="green">存疑：为什么输出音频采用离散token？</font></li></ul></li><li>Mini-Omni2 还支持在生成过程中输入用户音频离散token，根据语义打断生成，如 “StopOmni”。<font color="red">出发点：双工训练不稳定。</font><font color="green">存疑：应用中用户仍需要发起录音、结束录音请求回答，也可通过应用中的停止操作打断生成。</font></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa077bec80194feb0b5194b64e64dac3d?method=download&shareKey=a94c85c210a5fbcc2802a5a400bb8481" width="510"></p><ul><li>Qwen2-0.5B base，扩充 <span class="math inline">\(7 \times4160\)</span> 个输出 token。</li><li>语音 encoder: whisper-small。</li><li>visual encoder: CLIP ViT-B/32，一张图转为序列长度 49的 patch 特征 +1维全局语义特征。</li><li>adapter: 1 层 LlamaMLP。</li><li>音频 codec: SNAC。</li></ul><h1 id="训练">2. 训练</h1><table><thead><tr><th>阶段</th><th>训练的模块</th><th>任务</th></tr></thead><tbody><tr><td>1</td><td>adapter</td><td>ASR + image caption</td></tr><tr><td>2</td><td>LLM</td><td>文本问答 + 多模态问答，文本输出</td></tr><tr><td>3</td><td>adapter + LLM</td><td>上述所有任务支持音频输出 + 打断生成</td></tr></tbody></table><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb7e9aa0543ef6203ebf75443caf8213b?method=download&shareKey=77238bbfb4d2b38c43178f6a987a076b" width="597"></p><ul><li>数据集构建<ul><li>用户音频：CosyVoice 合成，将 ASR数据集作为随机音色库；系统音频：内部 TTS，固定音色。</li><li>打断数据集：TTS 合成 "stop omni"，拼接在 ASR 音频或 MUSAN噪声音频后。模型同时输出帧级状态 token，“stop omni” 之后状态为irq，之前为 n-irq。</li></ul></li><li>训练框架：<a href="https://github.com/Lightning-AI/litgpt">LitGPT</a></li></ul><h1 id="评价">3. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7d3c619e555147d66debf7771988d4e9?method=download&shareKey=18da9e31b3b79cb575c42cacacabcd6f" width="600"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Codec-SUPERB</title>
      <link href="/blog/yin-pin/codec/lun-wen/codec-superb/"/>
      <url>/blog/yin-pin/codec/lun-wen/codec-superb/</url>
      
        <content type="html"><![CDATA[<ul><li>Wu H, Chen X, Lin Y C, et al. Codec-SUPERB@ SLT 2024: A lightweightbenchmark for neural audio codec models[J]. arXiv preprintarXiv:2409.14085, 2024.</li><li><a href="https://codecsuperb.github.io/" class="uri">https://codecsuperb.github.io/</a></li><li><a href="https://github.com/voidful/Codec-SUPERB/tree/SLT_Challenge" class="uri">https://github.com/voidful/Codec-SUPERB/tree/SLT_Challenge</a></li></ul><h1 id="评价指标">1. 评价指标</h1><ul><li><p>应用级评价</p><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>应用</th><th>评价指标</th><th>数据集</th><th>模型</th></tr></thead><tbody><tr><td>ASR</td><td>WER</td><td>LibriSpeech</td><td>Whisper</td></tr><tr><td>说话人验证</td><td>equal error rate (EER)</td><td>Voxceleb test-O</td><td><a href="https://github.com/TaoRuijie/ECAPA-TDNN">ECAPA-TDNN</a></td></tr><tr><td>情感识别</td><td>Acc</td><td>RAVDESS</td><td><a href="https://github.com/ddlBoJack/emotion2vec">emotion2vec</a></td></tr><tr><td>音频事件分类</td><td>Acc</td><td>ESC-50</td><td><a href="https://github.com/microsoft/CLAP">CLAP</a></td></tr></tbody></table></li><li><p>客观指标</p><ul><li><a href="https://github.com/ludlows/PESQ">Perceptual Evaluation ofSpeech Quality (PESQ)</a></li><li><a href="https://github.com/mpariente/pystoi">Short-Time ObjectiveIntelligibility (STOI)</a></li><li><a href="https://github.com/descriptinc/descript-audio-codec/tree/main">Signal-to-distortionratio (SDR)</a></li><li><a href="https://github.com/descriptinc/descript-audio-codec/tree/main">MelSpectrogram Loss (MelLoss)</a></li></ul></li><li><p>开放数据集</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB14f3137a9d7d131c4a6adbfdebb5d44b?method=download&shareKey=b2d2b544adebdc54d481710b807ecc6b" width="501"></p></li></ul><h1 id="结果">2. 结果</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaafa46970958f3414d69cb7bc9343f02?method=download&shareKey=a682954b962dbfb007bcde45343dd03e" width="373"></p><p>* 各 codec 介绍详见论文。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe72ea4045e1dc123da1955d9f9ca22df?method=download&shareKey=600bad682dcc065e98a720082f849aee" width="745"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc895ffe4de9f36c25e411b1d2abe7a9b?method=download&shareKey=1d587aea64687e74c176890b1d5358bd" width="1011"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4bc0f59be5d88157b9be3e71cfa3f9dd?method=download&shareKey=faf10af2ffdd5a889dfd400df1a2198f" width="313"></p><ul><li>分析<ul><li>在 7-8kbps，AFACodec 在各语音应用上性能最好。</li><li>在 4kbps，APCodec 几乎所有指标均优于 SpeechTokenizer。</li><li>4-8kbps，音频事件分类任务，Encodec 性能最好。</li><li>在 &lt;= 2kbps 时，SemantiCodec 性能较好。</li></ul></li><li>应用指标 与 客观指标 的相关性<ul><li>STOI 与语音任务性能有很强的相关性。PESQ 次之。Mel Loss相关性较弱。</li></ul></li></ul><h1 id="其它">3. 其它</h1><ul><li>codec 综述<ul><li>Wu H, Chen X, Lin Y C, et al. Towards audio language modeling-anoverview[J]. arXiv preprint arXiv:2402.13236, 2024.</li><li><a href="https://github.com/ga642381/speech-trident" class="uri">https://github.com/ga642381/speech-trident</a></li></ul></li><li>codec 对比<ul><li><p>Chang X, Shi J, Tian J, et al. The Interspeech 2024 Challenge onSpeech Processing Using Discrete Units[J]. arXiv preprintarXiv:2406.07725, 2024.</p><p>评价方式：ASR；TTS、歌声合成。</p></li><li><p>Mousavi P, Della Libera L, Duret J, et al. DASB--Discrete Audioand Speech Benchmark[J]. arXiv preprint arXiv:2406.14294, 2024.</p><p>评价方式：ASR、关键词检测、意图分类、说话人识别、情感识别；TTS、语音分离、语音增强。</p></li><li><p>Wu H, Chung H L, Lin Y C, et al. Codec-superb: An in-depthanalysis of sound codec models[J]. arXiv preprint arXiv:2402.13071,2024.</p><p>评价方式：编解码重新合成。</p></li></ul></li><li>后续计划：加入多语种数据集。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> codec </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GLM-4-Voice</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/glm-4-voice/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/glm-4-voice/</url>
      
        <content type="html"><![CDATA[<ul><li><p>Zeng A, Du Z, Liu M, et al. GLM-4-Voice: Towards Intelligent andHuman-Like End-to-End Spoken Chatbot[J]. arXiv preprintarXiv:2412.02612, 2024.</p></li><li><p>Zeng A, Du Z, Liu M, et al. Scaling speech-text pre-training withsynthetic interleaved data[J]. arXiv preprint arXiv:2411.17607,2024.</p></li><li><p><a href="https://github.com/THUDM/GLM-4-Voice" class="uri">https://github.com/THUDM/GLM-4-Voice</a></p></li><li><p><a href="https://mp.weixin.qq.com/s/bX8-PDyFq9Xuk_IHaX23Aw">官方微信公众号介绍</a></p></li><li><p>作者来自智谱、清华</p></li><li><p><font color="red"><a href="https://github.com/THUDM/GLM-4-Voice/issues/26">turn basedspeech-to-speech 对话模型，不支持微信公众号中提到的“随时打断”。</a> 采用whisper encoder 中间层量化提取离散 speech token，flow matching + HiFiGAN合成语音，支持流式 token提取及合成，音频或文本输入，文本+音频交错输出，降低语音输出的延迟。支持中、英语音。</font></p></li><li><p>出发点</p><ul><li>为了捕捉语音丰富的细微差别及表现力，需要进行语音预训练。</li><li>语音数据相对稀缺，利用文本预训练数据集合成文本-语音交错数据，用于文本、语音token 的对齐。</li><li>speech tokenizer：语音 token 与文本对齐，以迁移预训练 LM的知识。</li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB34bcc0fd0e1130da03da03f9aa09135f?method=download&shareKey=37bd6cde91d375c52a296f0994fbce28" width="796"></p><ul><li>交替生成 13 个文本 token、26 个语音 token，token 不足整数倍的不pad。若最后文本 token 已生成结束，而音频 token还没有，一次性输出所有剩余音频 token。1:2 保证文本快于语音。26基于经验，使得 TTS 前先生成一段连贯的内容。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1345e12d07810d67c2cb44dfb3441275?method=download&shareKey=4a4f320189ab1653e75c4b625b6e5bdf" width="416"></p><h2 id="speech-tokenizaion">1.1. Speech Tokenizaion</h2><ul><li>基于 whisper-large-v3 encoder，添加 pooling、VQ层，有监督微调。</li><li>流式推理、因果：whisper encoder 采用因果卷积、block 因果attention。</li><li>码本：采用指数移动平均（EMA）学习码本向量，当某个向量的使用率低于指定阈值时，重置为随机的连续表示，以避免码本坍塌。</li><li>训练：ASR 数据集 + 70wh 伪标签数据（en: whisper-large-v3, zh:paraformer-large）。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBde7da9be4001db456924f2e5bdf0aad2?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="549"></p><ul><li>总体上，随 chunk size 增大，语音 token 的语义区分能力变强，WER降低。chunk size 为 2s 时，达到了接近非流式 token 的性能。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB51867df7f781f3d3420ac97108f5222b?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="608"></p><p>* 为了补偿 pooling 的信息损失，帧率减小时增大码本大小。</p><ul><li>VQ 量化后 WER性能提升，可能由于量化减少了噪声，识别任务更简单。随着帧率降低，WER略微逐渐变大。</li></ul><h2 id="speech-decoder">1.2. Speech Decoder</h2><ul><li>从头训练语音 token encoder、flow matching 模型。<ul><li>token encoder：transformer，采用与 speech tokenizer 相同的 blockcausal attention。</li><li>为了简化，从 flow matching 模型中移除了 speaker embedding。</li><li>预训练：所有无监督语音数据（Emilia、Yodas2、LibriLight、自有中文数据），覆盖多种说话人与语音质量。</li><li>微调：单一说话人，高质量数据。</li></ul></li><li>流式推理<ul><li>将语音 token 分为长为 10 的 chunk，实现最少需要 10 个 token来生成初始语音。</li><li>微调阶段：引入截断的音频样本，即长度为 <span class="math inline">\(\mathrm{n}=1 、 2 、 3 \cdots \cdots\)</span> 个chunk。</li><li>推理：将前 (n-1) 个 chunk 的语音作为提示，预测第 n 个 chunk的语音。</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB79cd84d88e735d34329e04cacbf9d137?method=download&shareKey=75be5c908c3503651416bcc8617f0240" width="808"></p><p>* 重建质量在 LibriSpeech 上测试。</p><p>* 出自第二篇论文，重建质量指标 ground truth 的值为4.62、-、3.27。</p><ul><li>12.5Hz 时重建质量已下降，6.25Hz 时重建质量不可用。GLM-4-Voice 采用12.5Hz 的 tokenizer。</li><li><font color="green">Moshi 中文重建质量？</font></li></ul><h1 id="训练">2. 训练</h1><h2 id="语音-文本联合预训练">2.1. 语音-文本联合预训练</h2><ul><li><p>初始模型：GLM-4-9B，非 chat 版。</p></li><li><p>训练集</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3ea0efc3f1e5dbdb2f38bdeb40a0cd7a?method=download&shareKey=28ce4a78325f57cffdc47eaf065235bc" width="272"></p><ul><li>合计 1T tokens，其中纯文本占比 30%。</li><li>4 种数据格式<ul><li>多样化的文本：网页、Wikipedia、书、论文，10Ttokens。用于保持模型的语言理解能力。</li><li>无监督语音数据：70wh 高质量中英文语音，采用 Emilia pipeline收集，保留 DNSMOS P.835 &gt; 2.75 的，保证语音数据的多样性且干净。</li><li>有监督语音数据：ASR（约9wh）、TTS，用于学习语音-文本对齐。</li><li>交错的语音-文本数据：有助于跨模态知识迁移。<ul><li>训练 text-to-audiotoken 模型：提高合成语音数据的效率。<ul><li>1.5B Transformer。训练集：约 14wh TTS 数据集 + 约 2wh cosyvoice合成数据（用于提高短文本、不完整文本片段的准确性）。</li><li>SGLang 框架部署，25k tokens/s。对于帧率 12.5Hz，合成 RTF 为0.0005。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB0032e2e7277ab4b16f50d6209b6402e9?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="305"></li><li>采用高质量中英文文本数据集FineWeb-Edu、Chinese-Fineweb-Edu，从中采样文本片段（长度服从泊松分布<span class="math inline">\(\lambda=10\)</span>，直到总长度达到原始文本长度的预定义比例<span class="math inline">\(\eta=0.3\)</span>）。采用 text-to-audiotoken模型将其替换为对应的语音片段。合成交错数据 600B tokens，英:中=2:1。</li></ul></li></ul></li></ul></li><li><p>有监督语音数据仅训练任务目标 tokens，其它数据训练所有tokens。</p></li><li><p>学习率：6e-5 衰减到 6e-6。</p></li></ul><h2 id="sft">2.2. SFT</h2><ul><li>训练集<ul><li><p>多轮口语对话：Magpie 文本对话数据集，采用 GPT-4改写，使其适用于语音场景，过滤样本、缩短响应、避免无法朗读的文本，MeloTTS合成，共 9w 样本。<font color="green">+ 人工朗读录制？</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa861c66cb68aab97da1984263f7083f6?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="602"></p></li><li><p>输出语音风格控制：如情感、语调、语速、方言。</p></li></ul></li><li>由于相较于学习语音输出，学习文本输出更快。分别 mask语音输出、文本输出loss，学习文本输出，从输入语音、输出文本学习语音输出。在语音输出上微调20 个 epoch，文本上 4 个epoch（<font color="green">训练顺序？</font>）。为了避免过拟合，dropout=0.5。</li><li>学习率：1e-5 衰减到 1e-6。</li></ul><h1 id="评价">3. 评价</h1><h2 id="base-模型">3.1. base 模型</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6d4e738c139099d3ca537275e792c7d8?method=download&shareKey=77efab8183601e21fbe94630582bfd50" width="504"></p><p>* 出自第二篇论文，GLM-4-Voice T-&gt;S，测试结果为 85.0、63.2。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2a9c9b15d0e90f859ce259fa87e61104?method=download&shareKey=db193aa1a7d03784cc988d27a49984c0" width="501"></p><p>* 出自第二篇论文，在问题后添加了文本 prompt “the answer is”。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3c32e646d61e7f70012c4851bfce6152?method=download&shareKey=266461e44eb3fdb55b584a14a94a9b74" width="503"></p><ul><li><font color="green"><span class="math inline">\(S \rightarrowT\)</span> 准确度始终高于 <span class="math inline">\(S \rightarrowS\)</span></font>，语音回复仍需要文本指导。</li></ul><h2 id="chat-模型">3.2. chat 模型</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc88adf04928634d687b6e67366b8e352?method=download&shareKey=0cb22c1254c70b7e13dbb38dde747325" width="466"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBffaa8375eb230ae5c8e2a81b631bbf38?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="599"></p><p>*出自第二篇论文，回复有文本引导时性能更好，去掉交错数据性能显著下降。</p><ul><li>测试集<ul><li><p>General QA：AlpacaEval helpful base、vicuna子集，去除数学相关问题。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB62003c94466666212c596bdc749953a2?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="600"></p><p>参照 MT-Bench，采用 GPT-4o 对回复打分 1-10。</p></li><li><p>Knowledge：从 Web Questions, Llama Questions, TriviaQA中选取100个问题。</p><p>GPT-4o 判断回复是否正确。评分由 0%-100% 归一化为 0-10。</p></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB4ab6b3ec2b325a95cc106ec54cc9db9a?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="605"></li><li>ASR-WER：GLM-4-Voice有时会用中文回答英文问题。为了可比，将输出限制为英文 token。</li></ul><h1 id="消融实验">4. 消融实验</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaacb40f4cf56d1eef16f58f54ac6cc26?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="605"></p><ul><li>其它数据不变，调整交错数据量（<font color="green">训练数据量随之变化？</font>）。</li><li>预训练时去掉交错数据，模型性能显著下降。移除纯文本、有监督语音数据，性能提升，可能是由于在较小的模型中各模态之间存在容量竞争。对于大模型，仍保留所有数据类型。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf2084bbf61c5f099ddaa18c431131a6f?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="608"></p><ul><li>相同 token训练量下，低帧率时模型见到了更多的训练数据，且序列变短可能更有利于模型生成。</li><li>语音-文本交错样本中替换为语音 token 的片段占比在 0.3时性能最佳。</li><li>有文本引导时模型生成效果更好。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WavLM</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/wavlm/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/wavlm/</url>
      
        <content type="html"><![CDATA[<ul><li>Chen S, Wang C, Chen Z, et al. Wavlm: Large-scale self-supervisedpre-training for full stack speech processing[J]. IEEE Journal ofSelected Topics in Signal Processing, 2022, 16(6): 1505-1518.</li><li>作者：来自微软</li><li><a href="https://github.com/microsoft/unilm/tree/master/wavlm" class="uri">https://github.com/microsoft/unilm/tree/master/wavlm</a></li><li>创新点<ul><li><font color="red">模拟带噪、重叠语音，在掩码区域预测原始语音类HuBERT 获取的离散标签，对说话人相关的下游任务有显著提升。</font></li><li><font color="red">采用 gated relative positionbias，相较于卷积相对位置embedding，允许根据当前语音内容自适应地调整，对内容相关的下游任务有显著提升。</font></li><li>训练数据：相较于之前的模型无监督语音主要为有声读物，与现实场景不匹配，采用Libri-Light + GigaSpeech + VoxPopuli，共 94kh 英文语音。</li><li>在 SUPERB、说话人验证、语音分离、说话人分离任务上实现了 SOTA性能，ASR 性能与 wav2vec 2.0、HuBERT 相当。</li></ul></li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd6fdd8b95da68d43b01e834819e8cd3a?method=download&shareKey=5586dc923e86064d1747b4cef2dd8a1f" width="308"></p><h1 id="gated-relative-position-bias">2. gated relative position bias</h1><p><span class="math display">\[ \begin{aligned}a_{ij} &amp; \propto \exp \left\{\frac{\mathbf{q}_i \cdot\mathbf{k}_j}{\sqrt{d_k}}+r_{i-j}\right\} \\g_i^{(\text {update })}, g_i^{\text {(reset) }} &amp;=\sigma\left(\mathbf{q}_i \cdot \mathbf{u}\right),\sigma\left(\mathbf{q}_i \cdot \mathbf{w}\right) \\\tilde{r}_{i-j} &amp;= w g_i^{\text {(reset) }} d_{i-j} \\r_{i-j} &amp;= d_{i-j}+g_i^{\text {(update) }}d_{i-j}+\left(1-g_i^{\text {(update) }}\right) \tilde{r}_{i-j} \\d_{|i-j|} &amp;= \begin{cases}|i-j|, &amp; |i-j|&lt;\frac{n}{4} \\\left\lfloor\frac{n}{4}\left(\frac{\log (|i-j|)-\log\left(\frac{n}{4}\right)}{\log (m)-\log\left(\frac{n}{4}\right)}+1\right)\right\rfloor, &amp; \frac{n}{4}\leq|i-j|&lt;m \\\frac{n}{2}-1, &amp; |i-j| \geq m\end{cases} \\d_{i-j} &amp;=d_{|i-j|}+\frac{n}{2} \cdot \mathbb{1}_{\{i-j&gt;0\}}\end{aligned} \]</span> 其中，<span class="math inline">\(a_{ij}\)</span> 为 attention 系数，<span class="math inline">\(r_{i-j}\)</span> 为 gated relative positionbias，<span class="math inline">\(\sigma\)</span> 为 sigmoid 函数，<span class="math inline">\(w\)</span> 可学习，<span class="math inline">\(n=320\)</span>，<span class="math inline">\(m=800\)</span>。</p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">import numpy as npimport matplotlib.pyplot as pltn &#x3D; 320m &#x3D; 800def d_ij(i_minus_j, n, m):    abs_diff &#x3D; np.abs(i_minus_j)    if abs_diff &lt; n &#x2F; 4:        d_abs &#x3D; abs_diff    elif n &#x2F; 4 &lt;&#x3D; abs_diff &lt; m:        d_abs &#x3D; np.floor(n &#x2F; 4 * (np.log10(abs_diff) - np.log10(n &#x2F; 4)) &#x2F; (np.log10(m) - np.log10(n &#x2F; 4)) + 1)    else:        d_abs &#x3D; n &#x2F; 2 - 1    return d_abs + (n &#x2F; 2) * (i_minus_j &gt; 0)# Values for i-ji_minus_j_values &#x3D; np.arange(-m-100, m+100, 1)d_values &#x3D; [d_ij(x, n, m) for x in i_minus_j_values]# Plotplt.figure(figsize&#x3D;(10, 6))plt.plot(i_minus_j_values, d_values)plt.xlabel(&#39;$i-j$&#39;)plt.ylabel(&#39;$d_&#123;i-j&#125;$&#39;)plt.grid(True)plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbeab9936196d6399045598daccaeaa88?method=download&shareKey=a2e8354a005bfbabefcf2bf4bf470c5c" width="500"></p><p><font color="green">存疑：代码实现？</font></p><h1 id="模拟带噪重叠语音">3. 模拟带噪、重叠语音</h1><p>随机选取噪声音频（取自 Deep Noise Suppression, DNS）或同一 batch的语音，根据能量比叠加，重叠区域不大于原音频的 50%。信噪比 <span class="math inline">\(r\)</span> 取值范围：噪声 [-5, 20]、重叠语音[-5,5]。 <span class="math display">\[ \begin{aligned}E^{\text {pri }} &amp;\leftarrow \frac{\sum \mathbf{u}^{\text {pri }}\cdot \mathbf{u}^{\text {pri }}}{L} \\scl &amp;\leftarrow \sqrt{\frac{E^{\mathrm{pri}}}{10^{\frac{r}{10}}E^{\mathrm{sec}}}} \\\mathbf{u}^{\mathrm{pri}}\left[s^{\mathrm{pri}}:s^{\mathrm{pri}}+l\right] &amp;\leftarrow\mathbf{u}^{\mathrm{pri}}\left[s^{\mathrm{pri}}:s^{\mathrm{pri}}+l\right]+s c l \cdot\mathbf{u}^{\mathrm{sec}}\left[s^{\mathrm{sec}}:s^{\mathrm{sec}}+l\right]\end{aligned} \]</span></p><h1 id="训练-label">4. 训练 label</h1><ul><li>WavLM Base：采用 HuBERT Base 模型第一次迭代 transformer第6层的输出，聚类。</li><li>WavLM Base+、WavLM Large：采用 HuBERT Base 模型第一次迭代transformer 第9层的输出，聚类。</li><li>虽然 WavLM 第一轮迭代的模型优于 HuBERT，仍采用 HuBERT生成的伪标签可得到更好的 WavLM 第二轮迭代模型。</li><li><font color="green">输入音频已被破坏，为了使输出表示降噪后、主说话人的语音，借助另一个模型基于原始音频生成伪标签？</font></li></ul><h1 id="评价">5. 评价</h1><ul><li>超参数：带噪、重叠语音概率 20%，其中重叠语音占比，WavLM Base100%，Base+、Large 模型 90%。</li><li>下游任务语音表示：预训练模型各层隐藏状态的加权和，权重可学习。<ul><li>分析各层表示的重要性时，用对应层隐藏状态的值对权重归一化，以消除隐藏状态值的大小引入的权重bias。</li></ul></li></ul><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>任务</th><th>数据集</th><th>下游模型</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td></td><td>SUPERB</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBda71e94430660854cc274dcfbc227974?method=download&shareKey=600a4127782d4047177fb22170d09220" width="1277"></td><td><font color="red">1.（加噪、重叠语音）降噪目标对说话人分离任务有显著提升。<br>2.结构修改，即 gated 相对位置 bias，对内容相关的任务有显著提升。<br>3.说话人信息从低层表示学习，内容、语义信息从高层表示学习。</font></td></tr><tr><td>说话人验证</td><td>VoxCeleb1、VoxCeleb2</td><td>ECAPA-TDNN (small)</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1f899ad40f46a0fa79a0f53e74c03ff4?method=download&shareKey=6e971559a9e1a06db66932b0f8cbbda1" width="316"></td><td></td></tr><tr><td>说话人分离</td><td>CALLHOME</td><td>[65]</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2e99d6351b6c41eacba3aa8abe0ec49a?method=download&shareKey=d201ea3c72baf33e30889e3873635954" width="502"></td><td></td></tr><tr><td>语音分离</td><td>LibriCSS</td><td>Conformer [22]</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBb80c76930baabf2e1f2f38792dbe7001?method=download&shareKey=7966294b29d58ba942b029dd0d9ecb02" width="320"></td><td></td></tr><tr><td>ASR</td><td>LibriSpeech</td><td>Linear</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB989415e3fcae2b996f93970c3291ffbe?method=download&shareKey=b84a900ab00d68470904a84fccf1bf7c" width="508"></td><td><font color="red">1. 采用预训练表示优于直接有监督训练。<br>2. WavLMLarge、HuBERT Large、wav2vec 2.0 Large 在 LibriSpeech 960h上性能相当。</font></td></tr></tbody></table><p><font color="red">* 训练、评价详见论文。</font></p><table><tbody><tr><td>SID</td><td>Speaker Identification</td><td></td></tr><tr><td>ASV</td><td>Automatic Speaker Verification</td><td></td></tr><tr><td>SD</td><td>Speaker Diarization</td><td></td></tr><tr><td>PR</td><td>Phoneme Recognition</td><td></td></tr><tr><td>ASR</td><td>Automatic Speech Recognition</td><td></td></tr><tr><td>OOD-ASR</td><td>Out-Of-Domain Automatic Speech Recognition</td><td></td></tr><tr><td>KS</td><td>Keyword Spotting</td><td></td></tr><tr><td>QbE</td><td>Query by Example Spoken Term Detection</td><td></td></tr><tr><td>ST</td><td>Speech Translation</td><td></td></tr><tr><td>IC</td><td>Intent Classification</td><td></td></tr><tr><td>SF</td><td>Slot Filling</td><td></td></tr><tr><td>ER</td><td>Emotion Recognition</td><td></td></tr><tr><td>SE</td><td>Speech Enhancement</td><td></td></tr><tr><td>SS</td><td>Speech Separation</td><td></td></tr><tr><td>VC</td><td>Voice Conversion</td><td></td></tr></tbody></table><h1 id="其它">6. 其它</h1><ul><li>相关工作<ul><li>基于训练目标，SSL 方法可以分为生成式、判别式、多任务学习。<ul><li>生成式：自编码器重建语音；自回归生成；非自回归模型恢复 mask帧。</li><li>判别式：wav2vec 系列采用对比损失；HuBERT预测掩码区域的离散目标；w2v-BERT以端到端的方式结合上述对比损失、掩码预测损失。</li><li>多任务：PASE [43]、PASE+ [44]采用多个目标，如波形生成、韵律回归、对比损失；UniSpeech [7]、JUST [45]结合 SSL 和 ASR 监督训练。</li></ul></li></ul></li><li>解决 fp16 数值溢出问题，主要原因为 attention score <span class="math inline">\(\frac{\mathbf{q}_i \cdot\mathbf{k}_j}{\sqrt{d}}\)</span> 超出了 fp16 的上限，表现为 loss 为NaN。 <span class="math display">\[\begin{aligned}\alpha_{i, j} &amp; \propto \exp \left\{\frac{\mathbf{q}_i \cdot\mathbf{k}_j}{\sqrt{d}}+r_{i-j}\right\} \\&amp; =\exp \left\{\left(\frac{\mathbf{q}_i}{c \sqrt{d}} \cdot\mathbf{k}_j-\max _{j^{\prime} \leq T}\left(\frac{\mathbf{q}_i}{c\sqrt{d}} \cdot \mathbf{k}_{j^{\prime}}\right)\right) \timesc+r_{i-j}\right\}\end{aligned}\]</span> 其中，<span class="math inline">\(c\)</span>为缩放系数超参数，本文取 32。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>w2v-BERT</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/w2v-bert/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/w2v-bert/</url>
      
        <content type="html"><![CDATA[<ul><li>Chung Y A, Zhang Y, Han W, et al. W2v-bert: Combining contrastivelearning and masked language modeling for self-supervised speechpre-training[C]//2021 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU). IEEE, 2021: 244-250.</li><li><a href="https://huggingface.co/facebook/w2v-bert-2.0" class="uri">https://huggingface.co/facebook/w2v-bert-2.0</a></li><li>作者：来自 MIT、Google Brain</li><li><font color="red">创新点：采用对比学习损失和掩码预测损失，端到端联合优化。对比学习损失避免了掩码预测训练中的码本坍塌（详见论文图2）。相较于 w2v-Conformer，验证了相较于仅采用对比损失性能有提升，mask预测有助于减轻对比学习中的简单负样本问题（见论文表 4）。</font></li><li>相关研究：相较于先训练语音离散化模块，冻结，再训练表示学习模块，端到端联合优化总体的性能。</li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbc74d60a17658bd52f62abb94f5ee321?method=download&shareKey=9c2e348f15ffb523aa4f6787f453d92d" width="374"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbdef8365154aba41333f1c21d7fe9cb4?method=download&shareKey=514eddc8bee9943db0051ab12342bbf7" width="593"></p><ul><li>输入特征：80 维 FBANK。</li><li>卷积下采样：4 倍。</li><li>mask 位置被替换为随机向量。</li></ul><p><span class="math display">\[ \begin{align}\mathcal{L}_c &amp;=\mathcal{L}_w+\alpha \cdot \mathcal{L}_d \\\mathcal{L}_p &amp;=\beta \cdot \mathcal{L}_c+\gamma \cdot \mathcal{L}_m\end{align} \]</span> 其中，<span class="math inline">\(\mathcal{L}_c\)</span> 为对比学习损失，<span class="math inline">\(\mathcal{L}_m\)</span> 为 MLM loss，<span class="math inline">\(\mathcal{L}_d\)</span> 为码本 diversityloss。<span class="math inline">\(\alpha=0.1\)</span>，<span class="math inline">\(\beta = \gamma = 1\)</span></p><h1 id="实验结果">2. 实验结果</h1><ul><li>微调：添加线性 projection 层和 RNNT decoder。采用 selftraining（伪标签）、SpecAugment、LM fusion。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaa18673c05b551f0a835a26cbfbec565?method=download&shareKey=0f5e5a144ccbd001e47cb06f049f6479" width="765"></p><p>* 无监督数据采用 Libri-Light unlab-60k。</p><ul><li>在谷歌语音搜索数据上验证时，由于相较于朗读语音，真实音频噪声大、静音更多，对比学习负采样有效性降低；短音频较多，采用堆叠3帧特征而不是卷积4倍下采样。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Google USM</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/google-usm/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/google-usm/</url>
      
        <content type="html"><![CDATA[<ul><li>Zhang Y, Han W, Qin J, et al. Google usm: Scaling automatic speechrecognition beyond 100 languages[J]. arXiv preprint arXiv:2303.01037,2023.</li><li>作者：来自 Google。</li><li>USM: Universal Speech Model</li><li><font color="red">创新点</font><ul><li><font color="red">相较于 Wav2Vec 2.0、W2v-BERT，采用 BEST-RQ进行语音表示学习，可以扩展到非常大的数据规模、大模型size。并采用多个（16）独立的码本，提高训练的鲁棒性。</font><ul><li>BEST-RQ: 投影矩阵、码本均随机初始化、冻结。</li></ul></li><li><font color="red">采用 speech-text模态对齐，提高需要映射声学表示-文本的任务的性能，如 ASR、AST等。利用纯文本，提高低资源语种和领域、无配对训练数据的语言上的性能。</font></li><li><font color="red">对于特定领域的下游任务，可以通过训练轻量级的adapter 快速自适应。</font></li><li><font color="red">对于鲁棒的长时语音识别，采用 chunk-wiseattention，chunk size 8s（识别性能与计算量的折衷）。</font><ul><li>localself-attention，随着模型层数增加，感受野线性增加，导致采用短音频训练，未见过长音频推理时可达的感受野宽度（conformer-2B超过 327s），删除错误较高。</li><li>相较于分块解码 [65,66] 只能看到当前块，采用 chunk-wise attention，除self-attention 的其它层可以看到超过当前块的上下文帧。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB6bfb0720a4e82cf7bb5af75596957c9e?method=download&shareKey=623ca3d8e1591e9b39df8e07243a352d" width="604"></li></ul></li></ul><h1 id="模型结构">1. 模型结构</h1><ul><li>conformer, relative attention。</li><li>输入：128 维 log-mel filterbank。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe2d75334ad62cd0a53f3096c2b776859?method=download&shareKey=0500761542f35d1e42066b6c312fd183" width="569"></p><h1 id="训练流程">2. 训练流程</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB64022718030f97298cea7fc149f76f1b?method=download&shareKey=7f18324beb4541b98255b7ee104dd823" width="601"></p><h2 id="most">2.1. MOST</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9f13f73b14f02c5dff41e8f1e24a7af8?method=download&shareKey=ba53a74a1d73743b9e52f8870b5e82e9" width="797"></p><ul><li><p>具体参见谷歌论文</p><p>Chen Z, Zhang Y, Rosenberg A, et al. Maestro: Matched speech textrepresentations through modality matching[J]. arXiv preprintarXiv:2204.03409, 2022.</p></li><li><p>本文结构</p><ul><li>embedding 层。</li><li>时长预测模块：4个卷积块（Maestro）。利用 RNN-T预测迭代、自监督训练（课程学习：先采用有监督数据训练RNN-T，再训练时长预测模块、加入纯文本）。</li><li>上采样：拷贝 text embedding 到指定帧数。</li><li>refiner：1 个 conformer 层。</li></ul></li><li><p>loss:</p><ul><li>纯音频：BEST-RQ MLM loss。</li><li>音频 + 文本：模态匹配损失 + ASR 损失。 <span class="math display">\[\begin{aligned}&amp; \mathfrak{e}_{\mathbf{s}}=\theta_s(\mathbf{s}),\mathfrak{e}_{\mathbf{t}}=\theta_t(\mathbf{t}), \quad(\mathbf{t},\mathbf{s}) \in \mathcal{X}_{\text {paired }} \\&amp; \hat{\mathfrak{e}}_{\mathbf{t}}=\theta_{\text {Refiner}}\left(\operatorname{Resample}\left(\mathfrak{e}_{\mathbf{t}},\operatorname{Align}_{\text {Rnnt }}\left(\mathfrak{e}_{\mathbf{s}},\mathbf{t}\right)\right)\right) \\&amp;\mathcal{L}_{\mathrm{MM}}=\operatorname{MSE}\left(\mathfrak{e}_{\mathbf{s}},\hat{\mathfrak{e}}_{\mathbf{t}}\right)+\mathcal{L}_{\mathrm{Rnnt}}\left(\mathbf{t}\mid \mathfrak{e}_{\mathbf{s}}\right)\end{aligned} \]</span></li><li>纯文本：mask，ASR 损失。 <span class="math display">\[\begin{aligned}&amp; \mathfrak{e}_{\mathbf{t}}=\theta_t(\mathbf{t}),\hat{\mathfrak{e}}_{\mathbf{t}}=\theta_{\text {Refiner}}\left(\operatorname{Resample}\left(\mathfrak{e}_{\mathbf{t}},\theta_{\text {Duration}}\left(\mathfrak{e}_{\mathbf{t}}\right)\right)\right) \\&amp; \mathcal{L}_{\mathrm{A}-\mathrm{MLM}}=\mathcal{L}_{\text {Rnnt}}\left(\mathbf{t} \mid\operatorname{Mask}\left(\hat{\mathfrak{e}}_{\mathbf{t}}\right)\right),\quad \mathbf{t} \in \mathcal{X}_{\text {text }}\end{aligned} \]</span></li></ul></li><li><p>模态对齐，使得学习到的表示与说话人、韵律等特征无关。</p></li></ul><h2 id="数据集">2.2. 数据集</h2><table style="width:100%;"><colgroup><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"></colgroup><thead><tr><th>类型</th><th>数据集</th><th>数据量</th><th>语种覆盖</th><th>备注</th><th>使用</th></tr></thead><tbody><tr><td>纯音频</td><td>YT-NTL-U</td><td>1200 wh</td><td>300 多个</td><td>YouTube</td><td>无监督预训练、MOST</td></tr><tr><td></td><td>Pub-U</td><td>42.9wh</td><td>51</td><td>VoxPopuli + Common Voice + Multilingual LibriSpeech + BABEL</td><td>MOST</td></tr><tr><td>纯文本</td><td>Web-NTL</td><td>28B 个句子</td><td>1140 多个</td><td></td><td>MOST</td></tr><tr><td>ASR</td><td>YT-SUP+</td><td>9wh，75 个语种 + 10wh，en-US，伪标签，从 YT-NTL-U 生成</td><td></td><td></td><td>ASR 通用模型微调</td></tr><tr><td></td><td>Pub-S</td><td>1wh，102 个语种 + 1wh，英语，多领域<br>VoxPopuli + MLS + Babel</td><td></td><td></td><td>MOST</td></tr></tbody></table><p>AST 微调：CoVoST 2 + 机器翻译文本（CoVoST 2 转写文本 + WMT + TEDTalks）。</p><h1 id="评价">3. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB768d10c847b85092a3f67c424704ebdd?method=download&shareKey=6c64f1eee47616ae9bfba481113d13e4" width="601"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>任务</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>ASR、AST 性能</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB53bfb578e0d0db816b108b7959a507ef?method=download&shareKey=6ec0b23bedfa02ff07abc7c23c370814" width="607"><br>*Whisper large-v2</td><td><font color="red">1. 相较于 whisper，只用 1/7的有监督训练数据，在许多语种的领域内和领域外 ASR任务上性能可比或更好。<br>2. 在 YouTube 长音频上，USM-LAS需要分段解码以避免退化，USM-CTC 性能更好。<br>3. 对于短音频，USM-LAS一致地优于 USM-CTC。</font><br>4. 领域内微调仍可以进一步提升性能。</td></tr><tr><td>未见语种的自适应</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa0d58cf91c0081eaa8607e8f92313cf8?method=download&shareKey=02e234db3b6a43c115bdd5f5b073354c" width="601"><br>训练数据仅FLEURS 10h。</td><td>采用 adapter +NST，仅少量有监督数据，可快速自适应到新语种、新领域。</td></tr><tr><td>消融实验</td><td>论文表 5</td><td>1. 扩大模型规模、无监督数据量，对性能有提升。<br>2. BEST-RQ采用多个独立码本，减小了多次运行的性能变化，加快了收敛速度。</td></tr><tr><td>BEST-RQ vs W2v-BERT</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB0b1c8bca4c4daaae3b3e899c39c634d3?method=download&shareKey=830ffaf8b35f2bc303304c9cd2b20d05" width="608"></td><td><font color="red">W2v-BERT 在 2B规模上更容易出现码本崩溃和训练不稳定，而 BEST-RQ从构造上来说不会出现码本崩溃。</font></td></tr><tr><td>长语音识别 chunk-wise attention vs local self attention</td><td>论文图 7、表 7</td><td>采用 local selfattention，对于更深层数的模型，长语音识别性能下降。<br>采用 chunk-wiseattention 在长语音上不会出现性能退化。</td></tr><tr><td>RTF</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBd733ce59c68def68396b59a18635612c?method=download&shareKey=9576d15184db9c163e4784282726e355" width="558"><br>batch推理</td><td></td></tr></tbody></table><h1 id="讨论">4. 讨论</h1><ul><li>利用无监督数据是提升低资源语种 ASR 性能的有效方式。</li><li>优化特定领域性能最有效的方式是领域内微调。</li><li>CTC vs RNN-T vs LAS：取决于下游任务。</li></ul><h1 id="其它">5. 其它</h1><ul><li>相关工作<ul><li>ASR 预训练 [6,12,22-33]</li><li>ASR self-training [7,8,34-44]</li><li>多模态语音模型 [13,20,45-54]</li><li>长音频 ASR [61-66]</li></ul></li><li>音频领域分类，见图 6。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TTS 连续语音 tokenizer</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/tts-lian-xu-yu-yin-tokenizer/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/tts-lian-xu-yu-yin-tokenizer/</url>
      
        <content type="html"><![CDATA[<ul><li>Li Y, Xie R, Sun X, et al. Continuous Speech Tokenizer in Text ToSpeech[J]. arXiv preprint arXiv:2410.17081, 2024.</li><li>作者来自香港中文大学、腾讯</li><li><font color="red">基于 VALL-E，对比了采用 audio encoder-decoder +离散或连续表示 + 自回归 LM 进行 TTS 的效果，还对比了 mel谱输入。采用连续表示 TTS效果更好，各频带信息的保留率更高，特别是高频。</font></li><li>代码、模型暂未开源。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc431d16a47386f0098f551cde5abe870?method=download&shareKey=4d5c2f32d31b2e693b66ed5ff3c56e7c" width="579"></p><ul><li>音频：24kHz。</li><li>模型结构参照 VALL-E。</li><li>降噪模块：包含 frequency limiting 和现成的降噪模型。</li></ul><h1 id="训练">2. 训练</h1><ul><li>流程<ul><li>encoder-decoder 预训练，采用重建 loss；</li><li>冻结 decoder，训练 encoder 和 LM，采用下述 LM loss，encoder 学习率为LM 的 0.05 倍。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB0145044af82669e9f1a419df6c47c432?method=download&shareKey=0ba11852b12a5b4c7c59feedddc7544c" width="250"></li><li>loss<ul><li>speech tokenizer 重建 loss <span class="math display">\[\begin{aligned}\widehat{\mathbb{A}} &amp;=\mathrm{AD}(\text {Cont-SpeechTokenizer}(\mathbb{A})) \\\widehat{\mathbb{Y}}&amp;=\operatorname{ASR}-\operatorname{Decoder}(\widehat{\mathbb{A}}) \\\mathcal{L}_{spt} &amp; =\mathcal{L}_{rec}+\mathcal{L}_{ASR} \\&amp; =\operatorname{SIM}(\mathbb{A},\widehat{\mathbb{A}})+\operatorname{CTC}(\mathbb{Y},\widehat{\mathbb{Y}})\end{aligned} \]</span> 其中，<span class="math inline">\(\mathbb{A}\)</span> 为音频，<span class="math inline">\(\mathbb{Y}\)</span> 为对应的文本，<span class="math inline">\(\mathrm{AD}\)</span> 为 audio decoder。</li><li>LM loss <span class="math display">\[ \begin{aligned}\mathcal{M} &amp;=\text { Cont-SpeechTokenizer }(\mathbb{M}) \\\mathcal{L}_{LM} &amp;=\operatorname{MSE}(\mathcal{O}, \mathcal{M})\end{aligned} \]</span> 其中，<span class="math inline">\(\mathbb{M}\)</span> 为语音 label，<span class="math inline">\(\mathcal{O}\)</span> 为 LM 输出。</li></ul></li></ul><h1 id="评价">3. 评价</h1><ul><li>训练集：LibriSpeech 960h。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4318d96467a02cc4226e0e7352d802f5?method=download&shareKey=770a922a5fd42d1f8f7891449e332a79" width="282"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB25c165d956826d8b7c6438dec12e1839?method=download&shareKey=4e565b7c79a0ef62b3d0e00f0248c971" width="279"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9d053293eee48f27ffdb39a51c6c0df9?method=download&shareKey=d3b40964e61bf66056afafa46c53bb74" width="279"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>指标</th><th></th><th>工具</th></tr></thead><tbody><tr><td>WER</td><td></td><td>Whisper Large</td></tr><tr><td>SIM</td><td>Speaker Similarity</td><td>WavLM-TDNN</td></tr><tr><td>EMoS</td><td></td><td><a href="https://github.com/aliutkus/speechmetrics">MOSNet</a></td></tr><tr><td>CLVP Score</td><td>Contrastive Language-Voice Pretrained model</td><td><a href="https://github.com/neonbjb/tts-scores">CLVP</a></td></tr><tr><td>STOI</td><td>Short-Time Objective Intelligibility</td><td><a href="https://github.com/aliutkus/speechmetrics">STOI</a></td></tr><tr><td>Noisiness, Continuity, Loudness Quality, and Naturalness</td><td></td><td><a href="https://github.com/deepvk/nisqa-s">NISQA</a></td></tr></tbody></table><ul><li><font color="green">存疑：各频带保留率如何计算？不同采样率、窗长的鲁棒性（图2）？</font></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>wav2vec 2.0</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/wav2vec-2.0/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/wav2vec-2.0/</url>
      
        <content type="html"><![CDATA[<ul><li>Baevski A, Zhou Y, Mohamed A, et al. wav2vec 2.0: A framework forself-supervised learning of speech representations[J]. Advances inneural information processing systems, 2020, 33: 12449-12460.</li><li>作者：来自 Facebook。</li><li><a href="https://github.com/facebookresearch/fairseq" class="uri">https://github.com/facebookresearch/fairseq</a></li><li><font color="red">创新点：连续表示包含更多信息，用于上下文表示；对比loss采用量化之后的表示，连续表示包含说话人、背景声音等信息，使得对比任务更容易，但不利于学习用于语音识别的通用表示。</font></li><li>应用：提高低资源语种、仅少量标注数据的任务的性能。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe09be0a8c22216d242172065f60f2c1a?method=download&shareKey=949ae65740f11026ac1c78e20212b481" width="603"></p><ul><li><p>CNN encoder：输入波形，输出约为 20ms/帧，感受野约为25ms。</p></li><li><p>Transformer</p><ul><li><font color="green">相对位置 embedding：采用类似 [37]的1个卷积层。</font></li><li>mask 策略：以概率 <span class="math inline">\(p=0.065\)</span>随机采样 mask 段的起始索引，mask 连续的 <span class="math inline">\(M=10\)</span> 帧，可能重叠。导致约 49% 的时刻被mask，平均 mask 长度约为 299ms。消融实验详见表 5。</li><li>mask 表示：全部替换为1个可学习的特征向量。</li></ul></li><li><p>量化：product quantization，<span class="math inline">\(G=2\)</span> 个码本，码本大小为 <span class="math inline">\(V=320\)</span>。采用 hard (argmax) Gumbelsoftmax，将 encoder 输出 <span class="math inline">\(z\)</span> 映射到<span class="math inline">\(\mathbf{l} \in \mathbb{R}^{G \timesV}\)</span>，选择第 <span class="math inline">\(g\)</span> 个码本中的第<span class="math inline">\(v\)</span> 个值的概率为 <span class="math display">\[p_{g, v}=\frac{\exp \left(l_{g, v}+n_v\right) /\tau}{\sum_{k=1}^V \exp \left(l_{g, k}+n_k\right) / \tau}\]</span> 其中<span class="math inline">\(n=-\log (-\log (u))\)</span>，<span class="math inline">\(u\)</span> 从 <span class="math inline">\(\mathcal{U}(0,1)\)</span> 中均匀采样，为服从Gumbel 分布的噪声。Gumbel softmax 接近 one-hot 向量。采用straight-through estimator，前向使用 argmax 选择码字，反向使用 Gumbelsoftmax 的梯度。训练时，<span class="math inline">\(\tau\)</span> 从 2逐步退火到 0.5（BASE）/ 0.1（LARGE）。</p></li><li><p><font color="red">对比 loss + 码本 diversity loss</font></p><p><span class="math display">\[ \begin{align}\mathcal{L} &amp;=\mathcal{L}_m+\alpha \mathcal{L}_d \\\mathcal{L}_m &amp;=-\log \frac{\exp\left(\operatorname{sim}\left(\mathbf{c}_t, \mathbf{q}_t\right) /\kappa\right)}{\sum_{\tilde{\mathbf{q}} \sim \mathbf{Q}_t} \exp\left(\operatorname{sim}\left(\mathbf{c}_t, \tilde{\mathbf{q}}\right) /\kappa\right)} \\\mathcal{L}_d &amp;=\frac{1}{G V}\sum_{g=1}^G-H\left(\bar{p}_g\right)=\frac{1}{G V} \sum_{g=1}^G\sum_{v=1}^V \bar{p}_{g, v} \log \bar{p}_{g, v}\end{align}\]</span></p><p>其中，<span class="math inline">\(\alpha=0.1\)</span>，<span class="math inline">\(\kappa=0.1\)</span>，<span class="math inline">\(\operatorname{sim}(\mathbf{a},\mathbf{b})\)</span> 为余弦相似度，<font color="green"><span class="math inline">\(\bar{p}_{g, v}\)</span> 为 batch 内所有句子的<span class="math inline">\(l_{g, v}\)</span> 的平均 softmax分布</font>，最大化其熵。对比 loss 中干扰项从同一句其它被 mask的时刻均匀采样 100 个。diversity loss激励模型训练时平等地使用每个码本中的所有条目。</p></li><li><p>结果分析：预训练模型得到的语音表示可以很容易地用于识别特定的声音，进一步微调可以使其对应到正确的单词拼写。</p></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6e2d0a270d5c34b615bde07ddbc48a83?method=download&shareKey=29a2a4a27bc8cb4a3c163a8c1c4923a0" width="477"></p><p>* LV-60k: LibriVox</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6c1acf9749fa2a3c26b294f25feea5d2?method=download&shareKey=54e0de694a42481b42c7f3d04e47fbc4" width="504"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa1788efa5756fb4faa43a1ed13ca01e8?method=download&shareKey=1f6d077c7e63248c87b2843aa5e42f91" width="498"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Moshi</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/moshi/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/moshi/</url>
      
        <content type="html"><![CDATA[<ul><li>Défossez A, Mazaré L, Orsini M, et al. Moshi: a speech-textfoundation model for real-time dialogue[J].</li><li><a href="https://github.com/kyutai-labs/moshi" class="uri">https://github.com/kyutai-labs/moshi</a></li><li><a href="https://huggingface.co/docs/transformers/model_doc/moshi" class="uri">https://huggingface.co/docs/transformers/model_doc/moshi</a></li><li><font color="red">创新点</font><ul><li><font color="red">第一个 multi-stream全双工、实时流式口语对话框架。理论延迟 160ms，实际延迟200ms（自然对话平均延迟约为 230ms）。支持的上下文约为 5min。</font></li><li><font color="red">semantic-acoustic audiocodec，将自监督音频模型的语义表示蒸馏至 codec，并采用 split RVQ权衡语言区分性及重建质量；因果、低帧率、低比特率，支持实时流式。</font></li><li><font color="red">相较于语义到声学 token生成，首先生成时间对齐的文本 token，指导音频 token 的生成；audio codec引入语义 token；以及在 RQ-Transformer结构中引入声学延迟，显著提高了生成语音的语言质量。</font></li><li><font color="red">通过改变不同类型 token 之间的延迟，支持流式ASR、TTS。</font></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe721e7c5380c87964861df08ef42dbe7?method=download&shareKey=57a9634224c385492c2784bfa7c71d6d" width="545"></p><h2 id="llm---helium">1.1. LLM - Helium</h2><ul><li>7B 参数。</li><li>tokenizer：SentencePiece，32000，主要为英语，数值全部拆分为单个数字，byte-backoff。</li><li>RoPE。</li><li>RMS 归一化：attention block、feed-forward block、线性输出层的输入。</li><li>FFN：采用 GLU 及 SiLU 门控函数。</li></ul><h2 id="audio-tokenizer---audio-codec---mimi">1.2. audio tokenizer - audiocodec - Mimi</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc398c2d0a4b21eab74e123fddc0ad3ef?method=download&shareKey=b93d716fdeb9650099677345c71e861c" width="659"></p><h3 id="结构">1.2.1. 结构</h3><ul><li><a href="https://arxiv.org/abs/2009.02095">SeaNet</a>。输入 24kHz的采样点，通过卷积得到 12.5帧/s、512 维的表示。decoder采用类似的结构还原为 24kHz 的音频。因果卷积，支持流式。</li><li>量化前后分别添加 Transformer 模块，8层，<font color="green">10s上下文（论文中两处有出入）</font>，因果 mask。<ul><li><font color="green">为了稳定训练，采用 LayerScale，初始化为对角线值0.01。</font></li><li>收益见表 3。</li></ul></li></ul><h3 id="量化">1.2.2. 量化</h3><ul><li>8 码本，码本大小 2048，256 维。</li><li>split RVQ：语义量化 + 7层传统声学 RVQ。<ul><li>类似 SpeechTokenizer，从自监督模型 <a href="https://huggingface.co/microsoft/wavlm-large">WavLM</a>中蒸馏语义信息。</li><li>音频下采样到 16kHz --&gt; WavLM（50Hz，1024维）--&gt;平均池化（s=4，k=8，12.5Hz。非因果：对性能很重要，仅用于训练）。</li><li>RVQ 语义表示线性投影到 1024 维。计算与 WavLM embedding 的 cosine距离损失。</li></ul></li><li><a href="https://arxiv.org/abs/2107.03312">采用 quantizerdropout，bitrate scalability。</a></li><li><a href="https://arxiv.org/abs/2306.06546">训练时，各序列仅 50%的时刻被量化</a>，收益见表 3。</li></ul><h3 id="训练损失">1.2.3. 训练损失</h3><p>对比了 重建 + 对抗 损失、仅对抗损失<font color="green">（特征 +鉴别）</font>。采用后者重建质量（与参考音频的声学相似度）客观指标显著下降，但主观评价显著提高，见表4。</p><h2 id="rq-transformer">1.3. RQ-Transformer</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf365e61bbacebea0a0f25d6dd0e4c236?method=download&shareKey=db9465e062b675f2150c74002801505f" width="654"></p><p><span class="math display">\[ \begin{aligned}z_s &amp;=\operatorname{Tr}_{\operatorname{Temp}}\left(V_0, \ldots,V_{s-1}\right) \in \mathbb{R}^d \\l_{s, 1} &amp;=\operatorname{Lin}\left(z_s\right) \in \mathbb{R}^{N_1}\\l_{s, k} &amp;=\operatorname{Tr}_{\text {Depth }}\left(z_s, V_{s, 1},\ldots, V_{s, k-1}\right) \in \mathbb{R}^{N_k}\end{aligned} \]</span></p><ul><li><p>Depth Transformer 中，对于不同的子序列 <span class="math inline">\(k\)</span>，线性层参数不一样。</p></li><li><p>声学延迟</p><p>生成更稳定。同时生成的声学 token 与语义 token的依赖减弱，同一时刻子序列间的依赖越复杂，需要越强大的模型来建模。语义、声学token 间的关系由更大的 Temporal Transformer 建模，而不是小的 DepthTransformer。<font color="green">（存疑：<span class="math inline">\(z_s\)</span>表示的语义信息、声学信息不同步，表示难度更大？）</font></p><p><span class="math display">\[\begin{cases}V_{s, 1}=A_{s, 1} &amp; \\V_{s, q}=A_{s-\tau, q} &amp; \text { if } \quad s \geq \tau+1, q&gt;1 \\V_{s, q}=0 &amp; \text { if } \quad s&lt;\tau+1, q&gt;1\end{cases}\]</span></p></li></ul><h2 id="inner-monologue">1.4. Inner Monologue</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3b48d26a4b9cbcb214d02853185fd683?method=download&shareKey=94e7cf1afffacc38cde1fb80a25b9f02" width="658"></p><p>label：whisper ASR，带词级时间戳。添加 special token <span class="math inline">\(\text {PAD}\)</span>、<span class="math inline">\(\text {EPAD}\)</span>。</p><p><span class="math display">\[ \text{初始化} \quad \mathrm{Wt}\leftarrow \mathrm{PAD} \quad \forall \mathrm{t} \\\left\{\begin{array}{ll}W_{t_i+j} &amp; \leftarrow w_{i, j} \quad \quad \forall j \leq n_i \\W_{t_i-1} &amp; \leftarrow \mathrm{EPAD} \quad \text{若不覆盖文本 token}\end{array} \right. \]</span></p><p>其中，<span class="math inline">\(t_i\)</span> 为第 i个单词对应的起始索引，<span class="math inline">\(w_{i, j}\)</span>为该单词对应的文本 tokens。<font color="green">若第一帧为语音，插入<span class="math inline">\(\text {EPAD}\)</span>。</font>可以强制采样<span class="math inline">\(\text {EPAD}\)</span> 使 Moshi立即开始说话。<font color="green">未根据单词结束时刻区分静音段？</font></p><h1 id="训练">2. 训练</h1><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>阶段</th><th>数据</th><th>备注</th></tr></thead><tbody><tr><td>audio codec 无监督训练</td><td>音频</td><td>随机的 12s 音频，batch size 128，4M steps。<br>AdamW，参数详见论文Section 3.3.1。</td></tr><tr><td>LLM 无监督预训练</td><td>纯文本，英语，2.1T token。<br>高质量数据源（Wikipedia、StackExchange、科学文章）+ CommonCrawl网络爬取数据（去重、语种识别、质量过滤）。详见论文 Section4.1、3.2.2。</td><td></td></tr><tr><td>single-stream 无监督预训练</td><td>音频+转写：700wh，主要为英语语音。24kHz（重采样），单声道（downmixed），Whisperlarge-v3转写。<br>为避免灾难性遗忘，一半训练步在上述纯文本数据集上。</td><td><font color="red">用 LLM 初始化 Temporal Transformer。</font><br>以30% 的概率 mask 文本 token。<br>文本、音频 token 时延：<span class="math inline">\(\pm 0.6s\)</span>。<br>音频 batch 中，文本embedding、线性层学习率 <span class="math inline">\(\times0.75\)</span>，<span class="math inline">\(\text {PAD}\)</span>、<span class="math inline">\(\text {EPAD}\)</span> token CE loss 权重 <span class="math inline">\(\times0.5\)</span>。<br><font color="red">纯文本、音频 batch采用两个独立的优化器状态。</font></td></tr><tr><td>multi-stream post-training</td><td>对无监督音频数据集，<font color="red">采用 <a href="https://hal.science/hal-04247212/document">PyAnnote</a>进行说话人分离</font>，随机选1个主说话人，他不说话的时刻作为另一个流，不含说话人重叠。<br>10%的时间步为纯文本 batch。</td><td></td></tr><tr><td>全双工微调</td><td>Fisher 2kh（AudioSR 上采样），包含说话人重叠。<br>无纯文本batch。</td><td></td></tr><tr><td>指令微调</td><td>合成指令数据集：微调 LLM 生成口语对话，多流流式 TTS，Moshi conditionon 固定音色、用户音色随机采样。2wh。<br>*Moshi音色：单个说话人录制独白，覆盖表 19 中超过 70 种说话风格。</td><td>* 文本指令数据集内容、风格不匹配。<br> *用户音频数据增强：<br>50%音量缩放 [-24, 15] dB；<br>30% 加噪 [-30, 6] dB（ Deep NoiseSuppression challenge）；<br>30% <a href="https://arxiv.org/abs/2006.12847">回声</a>（将 Moshi流缩放后加到用户流上，缩放系数 [0, 0.2]，延迟 [100, 500] ms）+ 用户流 <a href="https://arxiv.org/abs/2006.12847">reverb-like echo</a>。</td></tr></tbody></table><p>* Fisher、multi-stream TTS 数据集（有监督）获取时间戳：采用 <a href="https://github.com/linto-ai/whisper-timestamped">whisper-timestamped</a>+ whisper medium。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb2e335c7382a3d06f172bdb48222512a?method=download&shareKey=2e53c25ca18932035fa66569f906780e" width="654"></p><p>*AdamW: weight decay 0.1, [0.9, 0.95]. FlashAttention, FSDP,activation checkpointing.</p><h2 id="合成指令数据集">2.1. 合成指令数据集</h2><p>在 Open Hermes 文本指令数据集、真实对话转写上微调Helium，用于生成对话。</p><h3 id="tts-训练">2.1.1. TTS 训练</h3><p>训练流式、multi-stream TTS，用于合成指令微调数据集。</p><ul><li>audio 预训练共享 Moshi 的。</li><li>post-training<ul><li>数据集：两人 自然 + 脚本 对话，高质量、分声道录制，转写。170h。</li><li>文本、音频延迟：2s。</li></ul></li></ul><h3 id="对话生成">2.1.2. 对话生成</h3><ul><li>一般知识：给定 Wikipedia 段落、StackExchange帖子、或者关于它自己的信息，作为<code>&#123;&#123;context&#125;&#125;</code>，生成关于<code>&#123;&#123;title&#125;&#125;</code> 的对话<code>&#123;&#123;摘要&#125;&#125;</code>，进一步生成对话。</li><li>声音指令：单轮，指示模型用 <code>&#123;&#123;特定的声音&#125;&#125;</code> 说关于<code>&#123;&#123;实体&#125;&#125;</code> 的内容（句子、诗歌等）。</li><li>角色扮演：系统为 <code>&#123;&#123;特定情绪或说话风格&#125;&#125;</code> 的<code>&#123;&#123;角色&#125;&#125;</code>，生成<code>&#123;&#123;场景&#125;&#125;</code>，进一步生成对话。Moshi的文本可以包含说话风格、动作等，如<code>MOSHI: (relieved) *raises fists* Hooray! To the moon and beyond!</code>。</li><li>安全性、数学、闲聊。</li><li>特定 case，增强鲁棒性。<ul><li>用户指令包含拼写错误，模型要求用户重说，如<code>Sorry, I'm not sure what you're referring to.</code>。</li><li>用户提问包含事实性错误，模型回答 "No" 并纠正。</li></ul></li></ul><h2 id="loss">2.2. loss</h2><p><span class="math display">\[ L(V, l)=\frac{1}{S}\sum_{s=1}^S\left(\mathrm{CE}\left(l_{s, 1}, V_{s,1}\right)+\frac{1}{\sum_{k=2}^K \alpha_k} \sum_{k=2}^K \alpha_k\mathrm{CE}\left(l_{s, k}, V_{s, k}\right)\right) \]</span> 其中，<span class="math inline">\(k=1\)</span> 对应文本 token，语义 token <span class="math inline">\(\alpha_k=100\)</span>，声学 token <span class="math inline">\(\alpha_k=1\)</span>。</p><h1 id="评价">3. 评价</h1><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>模块</th><th>测试方式</th><th>评价指标</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>文本 LLM（仅在文本数据上训练）</td><td>各领域的开源 benchmark，详见论文 Section 5.1。</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBee1d94b70f513620a4388191c2a45c29?method=download&shareKey=3a7ef380849a467ea6d4499bf820017a" width="1308"><br>*第 2 组模型均约为 7B，在少于 2.5T token上训练；最后一组模型训练计算量大得多。</td><td>与同等训练计算量的模型可比，验证了预训练文本数据的质量。</td></tr><tr><td>audio tokenizer</td><td></td><td><a href="https://github.com/facebookresearch/libri-light/blob/main/eval/README.md">ABX错误率</a>：语义token（量化后）区分性。<br>VisQOL：重建音频与参考音频的声学相似度。<br>MOSNet：音质，无参考。<br>主观评价：与参考音频的相似性，MUSHRA协议。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcdb1587b1b4bc758e5e9abc6ab2a9e7a?method=download&shareKey=d9eb7446109a1004346fe078d78aa844" width="1318"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB1825e5c987bcbe49e89389e1ae42c3b6?method=download&shareKey=2a54ec813a5f537aa79e21fdb7e1b9a2" width="1310"><br>*分别保留 RVQ 的前几层，使比特率相当。</td><td>1. 语言区分性：<br>a) 蒸馏语义token，虽然音频表示的重建质量变差，但语言区分性显著提升。<br>b) 在encoder 中添加 Transformer 有收益，增加了 encoder的容量和感受野。<br>2. 重建质量：<br>贡献程度：decoder 中的Transformer &gt; split RVQ &gt; encoder 中的 Transformer &gt; 50%量化率<font color="green">（对与参考音频的声学相似度有收益）</font>。<br>*VisQOL 与主观评价不一致。</td></tr><tr><td>音频预训练-消融实验</td><td></td><td>PPL；<br>识别文本长度：模型较差时通常坍塌为静音。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB23b5afba612a53ec979a2920ed3eaec2?method=download&shareKey=420378ce962d5924a92e15b8ba5307d3" width="1314"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB03083ab6b0e1fba51a8514584c0ae2ae?method=download&shareKey=865abcdf7a885cdd051927aaa3fcc791" width="1314"></td><td>1. 相较于 [0, 1, 2, 3, 4, 5, 6, 7], [0, 2, 2, 2, 2, 2, 2, 2]生成延迟更小；在后者延迟模式下，相较于采用独立的分类头，采用RQ-Transformer 预测 RVQ token 显著改善了生成质量。<br><br>2.生成音频的语言能力：<br>a) 先生成文本 token 对其有显著提升；<br>b)提高语义 token loss 权重、声学延迟、Depth Transformer 预测 RVQ每一级时线性层参数不一样，均有收益。</td></tr><tr><td>音频语言建模能力</td><td>1. sWUGGY：对比单词及其不存在的变体；<br>2.sBLIMP：句法对比；<br>3. SpokenStoryCloze：常识性的5句的故事，最后一句语义连贯/不连贯，可能仅微妙差异；<br>4.Spoken Topic-StoryCloze：负例从不相关的句子中随机采样。<br>* 不考虑文本token，语义 token 权重 100。<br><br>5. MMLU</td><td>准确度：基于序列长度归一化的负对数似然。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB94cdef81dbfd0133a1a328f6b2898098?method=download&shareKey=ced7dde9e05ca4232ef0f2c9b149ba1b" width="1312"><br>*cold start：LM 随机初始化；warm start：用预训练的文本 LM初始化。<br><font color="green">* 最后两行的差异：Moshi 音色的condition 来自真人录音或合成语音。</font></td><td>1.指令微调后，词汇、句法判断能力下降，可能由于微调数据集覆盖各种音质，并对用户音频流进行了增强。但未观察到模型词法多样性、可懂度的退化。<br>2.相较于 Helium (54.3)，音频训练后在 MMLU 上性能下降。</td></tr><tr><td>口语问答</td><td>TTS 生成音频</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1bef7a25edc3dfc83c8b5e2e753853b3?method=download&shareKey=f702cb10399de93a494b5525f007dfff" width="1310"></td><td>1. 预测 text token 显著提升了模型性能。<br>2. 相较于文本 LLM的性能退化：<br>模型容量不变，增加了音频语言建模能力；<br>微调数据为口语对话风格，覆盖更多语法场景可能可以减小特定测试集上的差距。</td></tr><tr><td>对话生成</td><td>从 Fisher 数据集中随机采样 1000 个 10s 的提示，为 multi-stream同时生成延续。</td><td>PPL：转写，采用 DialoGPT；<br>Inter-Pausal Units(IPU)：连续语音（静音&lt;0.2s）；<br>Pause：同一说话人 IPUs间的静音；<br>Gap：不同说话人 IPUs 间的静音；<br>Overlap：IPUs重叠。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB133882d19baf10a32bc95aca23a45838?method=download&shareKey=a289ede67af4a7a406b6dc108ec46b1e" width="1314"><br>*级联：ASR + LM + TTS</td><td>温度值越小，生成越确定，文本更连贯，但连续语音段更短、静音段更长、说话人重叠更少。</td></tr><tr><td>流式 ASR、TTS</td><td>1. ASR + 对齐：推理时 teacher force 音频 token，文本 token 延迟2s。<br>2. TTS 类似。模型可以自由采样 <span class="math inline">\(\text{PAD}\)</span>、<span class="math inline">\(\text {EPAD}\)</span> token来获得帧对齐的文本 token 序列。还可通过控制填充 token占比来控制语速。通过前缀提示控制音色。<br>3. multi-streamTTS：单个文本流，两个说话人的文本通过 &lt;bos&gt;、&lt;eos&gt; token分隔，模型输出两组音频 token。</td><td></td><td>LibriSpeech test-clean，ASR WER 5.7%，TTS WER 4.7%。</td><td>流式 ASR 延迟 2s。<br>流式 TTS lookahead 2s，可生成5min、富有表现力的语音。</td></tr><tr><td>模型量化<br>Post-Training Quantization (PTQ)。激活采用BF16，线性层输入采用 AbsMax 对称量化动态量化到 8bit。<br>权重采用MinMax 非对称量化。<br><font color="green">initial</font>embedding、RMSNorms、audio codec 不量化。</td><td>MMLU;<br>音质：浮点模型生成提示 (64 tokens)，量化模型生成延续 (1024tokens)。</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB15e342e26a52e51230a88a06f0b0fb05?method=download&shareKey=5a4e4164daac9fdf5d1c63da12bbecb2" width="1316"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB7c32d09aa5f2948b2e685a962113f6ad?method=download&shareKey=71cf2899778483688872c5a6be3e465b" width="1308"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB695b780f792e6ad4247a7ae221872f80?method=download&shareKey=7d7e63665d068963398adaa5bf553da3" width="1308"></td><td>1. 相较于 Moshi，LLM 对量化更鲁棒，权重可量化至 4bit。<br>2.权重量化为 8bit、<font color="green">量化 block size32</font>，模型大小约为一半，MMLU 上性能下降约2个点。量化至 6/5 bit以下时 MMLU 上性能显著下降。<br>3. 音质：至多可量化至4bit，音质与浮点模型相当，artifacts 很少。<br><a href="https://github.com/aliutkus/speechmetrics">MOSNet</a>指标与主观评价不一致，如对重复生成不敏感。<br>4.随生成序列变长，artifacts 出现概率变高，详见图 11。<br>5.展望：量化感知训练。</td></tr><tr><td>安全性-毒性分析</td><td><a href="https://github.com/Babelscape/ALERT">ALERTbenchmark</a></td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBbc82b6de4ae907f19cf43a5e377f2684?method=download&shareKey=08eb1ecb26b8d78d0db884abf0682988" width="1314"><br>详见表18</td><td></td></tr><tr><td>安全性-Regurgitation</td><td></td><td>10w 次生成，生成训练集中频次最高的1个音频片段的比例。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB92fcce58f844ec3c1921c6e9e6bff99e?method=download&shareKey=2d4a208614e0be23e532cff29c1d36da" width="1314"><br>提示：前3s</td><td>1. 对于预训练模型，提示前 3s 时反刍概率接近1，无条件生成时也有(0.13%-0.19%) 的概率反刍。<br>2.进行训练集去重，即使提示，反刍的现象也显著减少（0%）。<br><font color="green">存疑：微调训练集不一致，不可比。</font></td></tr><tr><td>安全性-系统音色一致性（避免模仿用户音色）</td><td>生成 100h 对话；采用说话人确认模型 WavLM large 提取音频片段的说话人embdedding；对比 Moshi 音频片段与 Moshi第一个片段、用户第一个片段的余弦相似度。</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB22a0bf0bc9e9b76e13434d948916298c?method=download&shareKey=f69703578f78f459ec8be6887c38a2ea" width="1312"></td><td>1. 生成音色更接近自身音色而不是用户音色的比例约为 98.7%；<br>2.随对话时长增加，上述比例不退化。</td></tr><tr><td>安全性-音频索引和水印</td><td>基于信号的水印 <a href="http://github.com/facebookresearch/audioseal">Audioseal</a>；<br>基于生成的水印：采样时bias 概率控制生成。</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc5ceed921b0be738ae0a109d56f8136b?method=download&shareKey=da72442956202a6bd1181e197255719a" width="1310"></td><td>1. 基于信号的水印：音频进行编解码后失效。<br>2.基于生成的水印：由于音频编解码器不是 idempotent 的，不可用。</td></tr></tbody></table><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6556744c4a340900a18e28a8f6474afb?method=download&shareKey=f01a43cc2e015a90055f9c087e875fec" width="655"></p><ul><li>音频 artifacts 检测<ul><li><font color="green">在各时间步 <span class="math inline">\(s\)</span>，对各 token level <span class="math inline">\(k\)</span>，分别计算窗 <span class="math inline">\(C=64\)</span>（约 4.5s）内 token 的熵 <span class="math inline">\(H_s^k=H\left(A_{s-C: s,k}\right)\)</span>。根据非重叠窗 <span class="math inline">\(\omega=64\)</span> 内的熵 <span class="math inline">\(H_{s: s+\omega}^k\)</span> 检测是否存在artifacts。</font></li><li>重复文本：文本 token 的熵比较平坦。对 <span class="math inline">\(H_{s: s+\omega}^0\)</span>拟合线性回归，斜率小于阈值 <span class="math inline">\(\eta_{\text {flat}}=10^{-3}\)</span>。</li><li>静音：文本 token 为 <span class="math inline">\(\text{PAD}\)</span>，<span class="math inline">\(H_{s:s+\omega}^0=0\)</span>。音频 token 熵较低 <span class="math inline">\(\operatorname{median}_{k&gt;1, s}\left(H_{s:s+\omega}^k\right) \leq \eta_{\text{audio_silence}} =2\)</span> 。</li><li>背景噪声：文本 token 为 <span class="math inline">\(\text{PAD}\)</span>，音频 token 熵超过上述阈值。</li><li>Gibberish：胡言乱语，文本 token 熵很高 <span class="math inline">\(H_{s: s+\omega}^0&gt;\eta_{\text {gibberish}}=3.5\)</span>。量化至 2bit 时出现频率较高。</li><li>噪声：文本 token 非 <span class="math inline">\(\text{PAD}\)</span>，<span class="math inline">\(\operatorname{SD}_{k&gt;1,s}\left(H_{s: s+\omega}^k\right) &gt; \eta_{\text{noise}}=0.6\)</span>。</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB79f6b55b0ba9b670929094313a827864?method=download&shareKey=4d4dd9aa0508e24e7c103739cb43615c" width="876"></p><h1 id="其它">4. 其它</h1><ul><li>相较于级联系统<ul><li>低延迟；</li><li>保留了非语言信息，如情感、声音事件；</li><li>支持打断、backchanneling（不打断的反馈，如 OK、I see）。</li></ul></li><li>纯语音模型虽然可以从语音中学习语言结构，如词汇、句法、语义，但事实知识、推理能力很差，因此通常结合文本模型的知识、推理能力和生成模型的生成能力。</li><li>声学 token 对精细的音频细节进行建模，用于高质量重建；语义 token用于表征连贯、可理解的音频，可以从自监督音频模型中提取。</li><li>audio codec 不是 idempotent 的：从 tokens 生成音频、重新编码，tokens大概率不一致。token 对时间偏移不鲁棒。特别是仅采用感知对抗目标训练的audio codec。详见表 16。</li><li>个人思考 Q&amp;A<ul><li><font color="green">为什么需要语义 token：仅重建目标训练的 codec语言区分性较差，用于表征用户语音输入时难以表示语言内容。（生成时，先生成文本token，语义 token 的作用还大吗？）</font></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 语音实验</title>
      <link href="/blog/llm/mo-xing/llama-3.1-yu-yin-shi-yan/"/>
      <url>/blog/llm/mo-xing/llama-3.1-yu-yin-shi-yan/</url>
      
        <content type="html"><![CDATA[<h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB47a6130f43290a41184e000baed1d507?method=download&shareKey=7268786e265abfcce92a4cf15a88dcac" width="784"></p><p>*<font color="red">系统提示</font>：采用系统提示启用不同的语音理解模式，没有系统提示时为通用的口语对话模型。* 相较于视觉模块采用 cross attention，语音表示作为 LLM输入，可以直接利用 LLM 的所有能力。</p><p>能力：ASR、语音翻译 AST、口语对话、<font color="red">流式 TTS（LLM解码期间即时生成语音波形）</font>。</p><h1 id="数据">2. 数据</h1><h2 id="语音理解">2.1. 语音理解</h2><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>类型</th><th>构成</th><th>筛选</th><th>数据量</th></tr></thead><tbody><tr><td>encoder 自监督预训练</td><td>多语种、语音、无标注</td><td>VAD &gt; 0.7；<br>采用 Presidio Analyzer过滤含个人身份信息的数据。</td><td>1500wh</td></tr><tr><td>ASR</td><td>34个语种，人工转写</td><td><font color="green">最大时长 60s</font></td><td>23wh</td></tr><tr><td>AST</td><td>X-EN、EN-X，X 为其它的 33 个语种。<br>监督数据 +<font color="green">NLLB 合成</font>（提高低资源语种的质量）</td><td>同上</td><td>9wh</td></tr><tr><td>口语对话</td><td><font color="red">ASR 子集，用转写文本提示 LLM生成响应。</font></td><td></td><td>6wh</td></tr><tr><td></td><td>采用 Voicebox TTS 合成 Llama 3 微调数据。</td><td><font color="red">匹配（口语对话场景）语音分布，如提示相对较短、结构简单、没有非文本符号。</font></td><td>2.5wh</td></tr></tbody></table><p><font color="green">支持文本、语音交错的输入。</font></p><h2 id="语音生成">2.2. 语音生成</h2><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>类型</th><th>说明</th><th>数据量</th></tr></thead><tbody><tr><td>TN 模型</td><td>(书面形式, spoken 形式, 应用的一系列人工制定的 TN规则)<br>涵盖广泛的符号类，如数字、日期、时间。</td><td>55K</td></tr><tr><td>韵律模型</td><td>(韵律特征：每个音素的 log 时长、log 基频均值、log power 均值)</td><td>从 5wh 专业配音演员录制的 TTS 数据中提取</td></tr></tbody></table><p><font color="red">输入、输出分块对齐，用于流式训练。</font></p><h1 id="模型架构">3. 模型架构</h1><h2 id="语音理解-1">3.1. 语音理解</h2><ul><li><p>新加两个 special token，标记语音表示序列起、止。</p></li><li><p>语音 encoder</p><p><a href="https://arxiv.org/abs/2005.08100">conformer</a> 1B。astride-4 stacking layer + 线性投影层 + 24 Conformer 层，帧率 40ms， <a href="https://arxiv.org/abs/2104.09864">rotaryattention</a>，各维度详见论文。</p></li><li><p>语音 adapter</p><p>100 M。卷积层（步长 2）+ rotary Transformer 层 + 线性层。</p></li></ul><h2 id="语音生成-1">3.2. 语音生成</h2><ul><li>TN 模型和韵律模型都将 Llama 3 embedding 作为额外的输入，crossattention，利用其上下文表征能力，实现最小的 token lookahead和流式输入/输出。</li><li>TN 模型<ul><li><font color="green">基于流式 LSTM 的序列标记模型，预测 TN规则序列。</font></li></ul></li><li>韵律模型<ul><li>输入：<font color="green">TN后得到的语言特征（音素序列？）</font>、token、embedding。</li><li><font color="green">decoder-only Transformer</font>，单向（因果mask）。双 cross attention 结构，一层用于语言特征输入，另一层用于embedding，用于处理不同的输入速率，而无需对齐。</li></ul></li></ul><h1 id="训练">4. 训练</h1><p>LLM 冻结。</p><h2 id="语音理解-2">4.1. 语音理解</h2><ul><li><p>系统提示</p><pre class="line-numbers language-none"><code class="language-none">Repeat after me in &#123;language&#125;Translate the following sentence into &#123;language&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>添加源语言、目标语言 LID信息，可以提高指定翻译方向的性能，但会导致泛化能力损失，如在训练中未见过的翻译方向上。本文提出包含目标语言LID，但不包含源语言 LID，也可能使得模型能处理 code-switched语音。</p></li><li><p>encoder 预训练</p><ul><li><font color="green">音频最大时长 60s</font>，超过时随机截取。</li><li><font color="green">BEST-RQ。</font>以 2.5% 的概率应用32帧的mask。每个码本 8192 个向量。<font color="green">为了稳定训练，采用 16个码本，码本、投影矩阵随机初始化，模型训练过程中不更新。</font>batch-size2048 句，500K 步。</li></ul></li><li><p>SFT</p></li></ul><h2 id="语音生成-2">4.2. 语音生成</h2><ul><li>embedding：取 Llama 3 8B 第16层输出。</li><li>采用流式输入训练 TN、韵律模型。</li><li>韵律模型<ul><li>考虑固定数量的未来音素、可变数量的未来 token（由 TN chunk大小决定）。</li></ul></li></ul><h1 id="评价">5. 评价</h1><h2 id="语音理解-3">5.1. 语音理解</h2><ul><li>greedy search。</li></ul><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>任务</th><th>结果</th></tr></thead><tbody><tr><td>ASR</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc80d7150f7cee7b54cb7ca7902dfff29?method=download&shareKey=fe89111b5b8c8651ed3d843fcaaf1ee6" width="679"><br>whisper TN</td></tr><tr><td>AST</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5d80948da259fd4cf3261877db9d62a7?method=download&shareKey=80d5c55dcda8d7d8e4f03cbdad37b4fb" width="677"></td></tr><tr><td>口语问答</td><td>可处理 code-switched语音，未经训练。<br>可进行多轮对话，仅用单轮对话训练。</td></tr><tr><td>安全性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5f7026241154c2d99e17bb10efc14227?method=download&shareKey=2766a200407564ec53089a6d20304e69" width="672"><br>输入音频，采用MuTox classifier判断输出的毒性。<br>AT：输入安全、输出有害；LT：输入有害，输出安全。</td></tr></tbody></table><h2 id="语音生成-3">5.2. 语音生成</h2><ul><li>采用基于 Transformer 的模型预测谱特征，WaveRNN neural vocoder生成波形。</li><li>实验验证了 Llama 3 embedding 在 TN准确性，韵律模型、流式合成的人工偏好 上的收益。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 post 训练之数学推理能力</title>
      <link href="/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-shu-xue-tui-li-neng-li/"/>
      <url>/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-shu-xue-tui-li-neng-li/</url>
      
        <content type="html"><![CDATA[<ul><li>提示<ul><li>从预训练数据中挑选数学相关的，转换为问答格式。</li><li>创建<a href="https://arxiv.org/abs/2405.12205">数学能力分类</a>，检测模型性能较差的能力，人工标注提示。</li></ul></li><li><font color="red">扩充逐步推理过程</font><ul><li>CoT，提高推理能力。</li><li>Llama 3 生成逐步解答。</li><li>用正确答案、Llama 3 自我验证过滤。</li></ul></li><li>奖励模型<ul><li>训练<font color="red">结果和过程奖励模型</font>。</li><li>过滤中间推理步骤错误的样本。</li><li>对于更具挑战性的提示，采用 Monte Carlo TreeSearch、奖励模型，生成有效的推理步骤。</li></ul></li><li><font color="red">文本、代码组合推理</font><ul><li>提示 Llama 3 采用文本、Python代码组合来推理，可以显著提高解决问题的能力。</li><li>代码执行用于过滤错误的推理过程。</li></ul></li><li><font color="red">从反馈、错误中学习</font><ul><li>用户可能要求模型根据反馈改进，为了维持这类场景下的推理性能，用这类数据进行训练。</li><li>利用错误的生成结果，提示 Llama 3 生成正确的输出。</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 post 训练之工具使用能力</title>
      <link href="/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-gong-ju-shi-yong-neng-li/"/>
      <url>/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-gong-ju-shi-yong-neng-li/</url>
      
        <content type="html"><![CDATA[<h1 id="能力">1. 能力</h1><ul><li><p>搜索引擎</p><p>采用 <a href="https://brave.com/search/api/">Brave Search</a>回答关于超出其知识截止日期的近期事件、需要从网络检索特定信息的问题。</p></li><li><p>Python 解释器</p><ul><li>支持生成、执行代码，进行复杂计算。如读取用户上传的文件进行分析。<ul><li>文件类型：.txt, .docx, .pdf, .pptx, .xlsx, .csv, .tsv, .py, .json,.jsonl, .html, .xml</li><li>基于提供的文件，进行摘要、查找并修复错误、优化代码片段、进行数据分析或可视化。</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2f222671274723ca0ef92f324b37ef47?method=download&shareKey=1ec378e72f696c0e4f8b104fe6c189cf" width="1072"></p></li><li><p>数学计算引擎</p><p>采用 <a href="https://products.wolframalpha.com/llm-api/documentation">WolframAlpha API</a> 解决数学、科学问题，检索信息。</p></li><li><p>multi-step 工具使用</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB156686f147f65e1815d5b27ba6dfe00b?method=download&shareKey=6438d525d5a01f7e9c2f4342c69ce961" width="1221"></p></li><li><p>zero-shot 工具使用/函数调用</p><p>在上下文中给定函数定义（可能未见过）、用户 query。</p></li></ul><h1 id="实现">2. 实现</h1><p>将核心工具实现为 Python 对象。</p><h1 id="数据集构建">3. 数据集构建</h1><ul><li>数据收集过程与常规 pipeline 的区别<ul><li>先采用合成数据微调，使模型具备基本的工具使用能力，减少人工标注修改量。</li><li><font color="red">人工偏好标注或编辑</font>：由于工具使用场景下助手消息通常不止1条，如调用工具、对工具输出进行推理；在消息级别进行标注，对上述两项能力分别进行人工反馈。将人工标注添加到上文，继续对话。</li><li>不进行拒绝采样，无增益。</li></ul></li><li>单步工具使用<ul><li>few-shot 生成用户提示，需要调用核心工具。</li><li>基于 few-shot，生成工具调用。</li><li>执行。将工具输出添加到上文。</li><li>提示模型基于工具输出生成用户提问的最终答案。 <br> 过滤</li></ul></li><li>多步工具使用 同上</li><li>随辅助标注的模型能力增强，逐步增加标注的复杂性，单步工具使用 -&gt;对话中的工具使用 -&gt; 多步工具使用 -&gt; 数据分析。</li><li>采用不同的系统提示，训练模型只在工具激活时使用工具。训练模型对于简单问题（取自数学、QA数据集）即使激活了工具也不使用。</li><li>zero-shot 工具使用<ul><li>单一、嵌套、并行的函数调用：从 Stack提取函数定义、调用，清洗、过滤，采用 Llama 3 针对函数调用生成自然语言query。</li><li>多轮对话函数调用：<font color="green">采用 agent合成数据，用不同的提示得到多个代理，逐步生成 领域、API、用户 query、API调用、响应。</font></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 post 训练之多语种能力</title>
      <link href="/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-duo-yu-chong-neng-li/"/>
      <url>/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-duo-yu-chong-neng-li/</url>
      
        <content type="html"><![CDATA[<p>德、法、意大利、葡萄牙、印地、西班牙、泰</p><h1 id="专家训练">1. 专家训练</h1><ul><li>用于检索、生成高质量的指令微调数据。</li><li>基于预训练模型，在主要为多语种 token 的数据上 continuepre-training、post 训练。</li></ul><h1 id="sft-数据">2. SFT 数据</h1><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>类型</th><th>占比</th><th>说明</th></tr></thead><tbody><tr><td>人工标注</td><td>2.4</td><td>采用代表真实用例的开放式提示，语言学家、母语者标注。</td></tr><tr><td>NLP 任务中的多语种数据</td><td>44.2</td><td>1. 改写为对话格式。<br>2. 采用 GlobalVoices、Wikimedia中的平行文本，提高语言对齐。并<font color="red">应用<a href="https://arxiv.org/pdf/2109.01652">模板</a>，模拟翻译、语言学习场景中的真实生活化对话</font>。<br>3.基于 LID、Blaser2.0 过滤低质数据。<br></td></tr><tr><td>拒绝采样</td><td>18.8</td><td>1. 对人工标注的提示进行生成，<br>早期随机选择温度参数 [0.2, 1]来生成多样化的响应，但容易出现不必要的 code-switching。最后一轮采用0.6。<br>采用系统提示改善格式、结构、可读性。<br>2.采样<br>先检查提示、响应之间的语言匹配。基于奖励模型。</td></tr><tr><td><font color="red">翻译数学推理数据</font></td><td>34.6</td><td></td></tr></tbody></table><p>*尽量避免采用机器翻译数据微调模型，以避免翻译腔，可能的名称、性别、文化偏见。</p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 post 训练之代码能力</title>
      <link href="/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-dai-ma-neng-li/"/>
      <url>/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-dai-ma-neng-li/</url>
      
        <content type="html"><![CDATA[<ul><li>高优语言：Python, Java, Javascript, C/C++, Typescript, Rust, PHP,HTML/CSS, SQL, bash/shell。</li><li>能力：代码生成、文档编写、调试、review。</li></ul><h1 id="专家训练">1. 专家训练</h1><ul><li>用于收集高质量的代码、代码提示的拒绝采样。</li><li>基于预训练模型，在主要为代码的数据上 continuepre-training、在仓库级代码上进行长上下文微调、post 训练。</li></ul><h1 id="合成数据">2. 合成数据</h1><p>采用 Llama 3、代码专家生成 SFT 对话。</p><h2 id="执行反馈">2.1. 执行反馈</h2><ul><li><p>生成问题描述</p><p>从各种来源随机采样代码片段，提示模型生成编程问题。</p></li><li><p>生成解决方案</p><p>提示 Llama 3用指定的编程语言响应。在提示中添加通用的好的编程规则、<font color="red">要求其在注释中解释思维过程</font>，提高生成质量。</p></li><li><p><font color="red">正确性分析</font></p><ul><li><p>静态分析</p><p>采用解析器、linter。</p></li><li><p>生成单元测试、执行</p><p>对于每个问题、解决方案，提示模型生成单元测试。在容器化环境中执行。</p></li></ul></li><li><p><font color="red">错误反馈</font>，迭代自我修正</p></li><li><p>微调，迭代改进</p></li></ul><h2 id="编程语言翻译">2.2. 编程语言翻译</h2><p>提示 Llama 3将常见的编程语言的数据翻译为较少使用的语言。并采用上述执行反馈保证质量。</p><h2 id="backtranslation">2.3. backtranslation</h2><p>用于提高文档编写、解释能力。</p><ul><li>提示 Llama 3，为代码片段添加注释、docstrings，或解释代码片段。</li><li>提示模型仅根据文档或解释生成代码。</li><li>将原始代码作为参考，提示 Llama 3确定生成代码的质量，如相较于原始代码的忠实度。采用高分数据用于SFT。</li></ul><blockquote><p>回译过程确认了文档、解释的准确性、完整性。</p></blockquote><h1 id="拒绝采样">3. 拒绝采样</h1><h2 id="采用系统提示引导">3.1. 采用系统提示引导</h2><p>用于提高代码的可读性、文档、全面性thoroughness、specificity，如添加必要的注释、使用信息更丰富的变量名、节省内存等。</p><h2 id="过滤">3.2. 过滤</h2><ul><li><p>执行。</p></li><li><p>对于不可执行的代码，如用户明确要求的伪代码、非常小的代码片段等，采用Llama 3 分别对代码正确性、代码风格进行 0-1 打分，只保留 2 分样本。</p><p>由于上述策略<font color="red">过度移除了难样本</font>，下游性能下降。修正被分类为最具挑战性的编程数据的响应，使其满足上述模型判断标准。</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Llama 3.1 post 训练之长文本能力</title>
      <link href="/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-chang-wen-ben-neng-li/"/>
      <url>/blog/llm/mo-xing/llama-3.1-post-xun-lian-zhi-chang-wen-ben-neng-li/</url>
      
        <content type="html"><![CDATA[<ul><li>仅采用短文本 SFT，长文本能力显著退化。</li><li>主要的长文本用例：（多轮）问答、长文本摘要、代码库推理<ul><li><p>问答</p><p>将预训练数据中的长文本分为 8K token 的块，采用早期版本基于文本块生成QA 对。SFT 训练时将整个文档作为上下文。</p></li><li><p>摘要</p><p>采用 8K 模型对 8K token 的文本块进行摘要，再对摘要进行摘要。SFT时对完整文档进行摘要。</p><p>基于文档摘要生成需要全局理解的 QA 对。</p></li><li><p>代码推理</p><p>基于 Python <code>import</code>语句，从代码仓库中删除最常被依赖的文本之一。提示模型识别依赖于缺失文件的文件、生成缺失的代码。</p></li></ul></li><li>进行消融实验，确定采用短文本数据 + <font color="red">0.1%</font>合成的长文本数据。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Audio-MAE</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/audio-mae/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/audio-mae/</url>
      
        <content type="html"><![CDATA[<p>论文：Huang P Y, Xu H, Li J, et al. Masked autoencoders thatlisten[J]. Advances in Neural Information Processing Systems, 2022, 35:28708-28720.</p><p>代码、模型：<a href="https://github.com/facebookresearch/AudioMAE" class="uri">https://github.com/facebookresearch/AudioMAE</a></p><ul><li>作者：来自 Meta AI、Carnegie Mellon University</li><li>发表时间：2022.07</li><li>创新点<ul><li><a href="#系统结构">系统结构</a></li><li><font color="red">减小 self-attention 计算量：encoder只处理很小部分的未 mask 的 patch、local attention。</font></li></ul></li><li>性能：在音频、语音分类任务上实现了 SOTA。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc516d2e508e201291956025406d346db?method=download&shareKey=62c36a374c6394382237c5b1a26af3de" width="713"></p><p>* patch embedding：送入 encoder、decoder 前均加 fixed 正弦位置embedding。</p><p>* decoder 处理的 mask patch embedding 是可学习的。</p><h1 id="实验">2. 实验</h1><ul><li>音频：16kHz、帧长 25ms、帧移 10ms、FBANK 128。</li><li>encoder：12 层 ViT-Base。</li><li>decoder：16 层 Transformer。</li><li>预训练<ul><li>数据集：AudioSet-2M，10s。随机采样开始时间、循环，得到 10s音频。随机抖动音频幅值至多 <span class="math inline">\(\pm 6\mathrm{~dB}\)</span>。不采用频谱增强，如 Specaugment、<a href="https://arxiv.org/abs/1905.04899">CutMix</a>、<a href="https://arxiv.org/abs/1710.09412">mixup</a>，因为发现在预训练阶段无收益或更差。</li></ul></li><li>微调<ul><li>数据集：AudioSet-2M（加权采样来均衡类别）、AudioSet-20K（类别均衡的）、ESC-50、SpeechCommands（关键词检测）、VoxCeleb (SID，说话人识别)。</li><li><font color="red">mask：用于正则化、减少计算量。</font></li><li>丢弃预训练的 decoder。average pooling + 线性层分类。</li><li>优化目标：当数据集为 multi-label 或采用 mixup 数据增强时，采用不带label smoothing 的 BCE loss。</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8d44422b421b13e0aea8542cab2a7094?method=download&shareKey=9f37f47ff4907317aebae8a0c4c71b26" width="573"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>任务</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>mask 策略</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB9c1743c6bcf2da26f4656404b23ee4b8?method=download&shareKey=0743aa5adbffae41c7b1a82607f11bf0" width="694"><br><img src="https://note.youdao.com/yws/api/personal/file/WEBbae3ff4a743b8043628d398c62da1b9e?method=download&shareKey=c7f92a4625ad3e6ff8b3479643d994c3" width="722"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB42c63d532af87f8d243a56e739f14872?method=download&shareKey=0d091e3cc1c203e3dd56e43a2dcda5cd" width="709"></td><td>1. mask 比例：<font color="red">预训练</font>采用高 mask 比例<font color="red">80%</font>，因为语音频谱、图像的高冗余特性；<font color="red">微调采用30%</font>。<br>2. mask 方式：<font color="red">预训练采用非结构化随机mask；微调时采用时间+频率 mask</font>。<br>3.预训练重建语谱图，采用非结构化随机 mask更容易，模型可以根据附近的上下文预测。而结构化 mask可能导致重建的语音单词缺失，如图 6e-3。<br>4. 随 mask 比例增加，结构化mask 性能下降，可能由于任务变得过于困难。</td></tr><tr><td>消融实验</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB63452a24a08d4e8ca634842f50ec240f?method=download&shareKey=bfb24d23d4f49b5d0ff84218e56afed5" width="722"><br>(c)除少量的最后几层采用全局 attention 外，其它采用 localattention。<br>(e) decoder width 指 embedding维度。<br><img src="https://note.youdao.com/yws/api/personal/file/WEB05f2e146e741b4807ea1451dde0cafcb?method=download&shareKey=c616958be9a08e2f75ed1a63ba7f1cdd" width="257"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB05a4e24e9e02a949dfb3ef013d2710ba?method=download&shareKey=cb5f035f4e319fd35056892d4053f3be" width="310"></td><td>(a) patch：卷积kernel=stride=(16,16)，<font color="red">不重叠</font>。<br>相较于 AST采用重叠 patch 提高终端任务的性能，Audio-MAE中未观察到性能提升，<font color="red">可能由于自监督训练中short-cuts（捷径）</font>。<br>(b) encoder 大小：采用ViT-B，性能与开销的权衡。<br>采用更多域内数据微调时，不同模型大小间的性能差异可以显著减小。<br>(c)decoder attention 机制：采用 <font color="red">shifted localattention</font>。相邻层间向左上移 50%。<br>如图 5 所示，采用 localattention 可以更好地重建元音的谐波结构。<br>(f)预训练性能随数据量增加而单调提升。<br>类别是否均衡对预训练影响不大。<br>(h)领域外预训练（如 ImageNet）对 Audio-MAE 有损，可能由于领域偏差。</td></tr><tr><td>预训练目标</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB375be07b2ddba64561f45d124aa98076?method=download&shareKey=09d1c2e5d64d44a3ee902eb8db614a6e" width="325"></td><td><font color="red">单独采用重建损失即可，采用或结合对比损失无性能收益。</font></td></tr><tr><td>对比 SOTA</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB173f7f39379f3d4ea111b7451f023db0?method=download&shareKey=bd192642d652ba52ff0dfebd7b95c9cc" width="714"><br>*SUPERB[53]：不完全可比，底层预训练模型未端到端微调。<br>采用不同的随机种子运行3次。</td><td></td></tr><tr><td>音频重建质量</td><td>图7</td><td>相较于音乐、声音事件有相对可预测的谱图模式，如跨时域的重复tempos（音乐）、跨频域的谐波，语音重建更具挑战性。</td></tr></tbody></table><h1 id="其它">3. 其它</h1><ul><li><p>展望：将 Audio-MAE decoder 用于 Voice over IP (VoIP)应用中的丢包重建（Packet LossConcealment，PLC）、语音频带扩展等任务。</p></li><li><p>局限性：数据集规模，音频持续时长（音频中的长时依赖性哪些场景存在？），transformer 建模长音频、高维数据计算量大。</p></li><li><p>Transformer 用于长序列任务，减小 attention 计算量的方式</p><ul><li>Swin-Transformer：shifted local attention</li><li>MViT [20]：分层 Transformers，pooling attention</li></ul></li><li><p>音频自监督训练</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>类别</th><th></th><th>模型</th></tr></thead><tbody><tr><td>输入</td><td>原始波形</td><td>wav2vec 2.0、data2vec</td></tr><tr><td></td><td>帧级特征</td><td>HuBERT、AV-HuBERT、<a href="https://arxiv.org/abs/2110.07313">Conformer-Based Self-SupervisedLearning for Non-Speech Audio Tasks [37]</a></td></tr><tr><td></td><td>语谱图 patch</td><td>SS-AST [18]、MAE-AST</td></tr><tr><td>优化目标</td><td>对比损失</td><td>wav2vec 2.0、HuBERT</td></tr><tr><td></td><td>预测/重建</td><td>data2vec、AV-HuBERT、[37]、SS-AST</td></tr></tbody></table></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Seed-ASR</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/lun-wen/seed-asr/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/lun-wen/seed-asr/</url>
      
        <content type="html"><![CDATA[<ul><li>Bai Y, Chen J, Chen J, et al. Seed-ASR: Understanding Diverse Speechand Contexts with LLM-based Speech Recognition[J]. arXiv preprintarXiv:2407.04675, 2024.</li><li><a href="https://bytedancespeech.github.io/seedasr_tech_report" class="uri">https://bytedancespeech.github.io/seedasr_tech_report</a></li><li>作者：字节 Seed 组</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0910179246773a52e8532a313e8d3137?method=download&shareKey=1b0d686212b3ed937d88c71898ed91e2" width="685"></p><h1 id="音频-encoder">2. 音频 encoder</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2d5d36eb69ef47bf1e16c22fbc20792a?method=download&shareKey=80db0a462fc3ad6aa92618cf8cf2cabe" width="432"></p><ul><li><p>取名 LUISE: Large-scale Unsupervised Iterative SpeechEncoder。</p></li><li><p>mel-filterbank 特征。</p></li><li><p>基于 conformer。参数量：2B。</p></li><li><p>离散 label</p><p><font color="green">iterative fixed。第 1 次迭代：codebook随机初始化，采用随机投影层 [10]。之后：中间层表示 K-means聚类。</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBeb0353db02163cc43172238fc99afc42?method=download&shareKey=5e08115fd1f510e9817a42ec619bf677" width="686"></p><p>取第 25/32 层的表示。</p></li><li><p>只计算 mask 帧的 CE loss。</p></li><li><p>预训练后，取其<font color="red">中间层的连续表示</font>。</p></li></ul><h1 id="converter">3. converter</h1><p>LUISE 中间层表示帧率为40ms。<font color="red">拼接4个连续帧进行下采样，再输入线性投影层。</font>因而输入LLM 的帧率为 160ms。</p><h1 id="训练">4. 训练</h1><ul><li>音频 encoder 自监督学习</li><li>LLM<ul><li>Seed-ASR (CN)：采用超过 10B 的 MoE LLM 初始化。</li></ul></li><li>SFT<ul><li>learnable audio encoder + learnable converter + fixed LLM</li></ul></li><li>上下文 SFT<ul><li>数据：&lt;上下文 <span class="math inline">\(c\)</span>, 语音 <span class="math inline">\(x\)</span>, 文本 <span class="math inline">\(y\)</span>&gt; + 一定比例的通用 ASR 数据。<ul><li>可以根据场景定制，如对话历史、视频字幕场景下的用户编辑历史、会议场景下的参会人名称。</li><li><font color="red">上下文由内部 LLM根据转写文本生成，实验表明相较于采用长语音的历史转写，性能更好。</font></li></ul></li><li><strong><font color="red">解码策略优化</font></strong>。采用原生beam search 存在严重的幻觉问题。<ul><li><p>剪枝：先采用 <span class="math inline">\(P(\boldsymbol{y} \mid\boldsymbol{x})\)</span> 过滤声学不合理的候选 token。</p><blockquote><p>说明此处的幻觉问题是上下文引入的，<span class="math inline">\(P(\boldsymbol{y} \mid \boldsymbol{x})\)</span>只存在音准字不准的问题。</p></blockquote></li><li><p>Joint beam search：<span class="math display">\[P_{\text {joint}}(\boldsymbol{y} \mid \boldsymbol{x},\boldsymbol{c})=\frac{\alpha}{1+\alpha} * P(\boldsymbol{y} \mid\boldsymbol{x}, \boldsymbol{c})+\frac{1}{1+\alpha} * P(\boldsymbol{y}\mid \boldsymbol{x})\]</span></p></li></ul></li></ul></li><li>强化学习<ul><li>训练集：数千小时，部分数据含 context。</li><li><font color="red">由于 SFT 采用的 CE 目标函数与 CER / WER不匹配，采用最小词错误率（minimum word errorrate，MWER），同时对关键词进行加权。</font><span class="math display">\[\mathcal{L}_{\text {mwer }}^{\mathrm{N} \text{-best }}\left(\boldsymbol{x}, \boldsymbol{y}^*\right)=\frac{1}{N}\sum_{\left.\boldsymbol{y}_{\boldsymbol{i}} \in \text { N-best(x, N}\right)} \hat{P}\left(\boldsymbol{y}_{\boldsymbol{i}} \mid\boldsymbol{x}\right)\left(\mathcal{W}\left(\boldsymbol{y}_{\boldsymbol{i}},\boldsymbol{y}^*\right)-\bar{W}\right)+\lambda\mathcal{L}_{\mathrm{CE}}\]</span> <span class="math display">\[\hat{P}\left(\boldsymbol{y}_{\boldsymbol{i}} \mid\boldsymbol{x}\right)=\frac{P\left(\boldsymbol{y}_{\boldsymbol{i}} \mid\boldsymbol{x}\right)}{\sum_{\boldsymbol{y}_{\boldsymbol{i}} \in\mathrm{N}-\mathrm{best}(\mathrm{N}, \mathrm{N})}P\left(\boldsymbol{y}_{\boldsymbol{i}} \mid\boldsymbol{x}\right)}\]</span> 其中 <span class="math inline">\(\boldsymbol{y}^*\)</span> 为 ground truth，<span class="math inline">\(\mathcal{W}\left(\boldsymbol{y}_{\boldsymbol{i}},\boldsymbol{y}^*\right)\)</span> 为 （加权的）WER。</li><li>为了提高 RL 训练效率，采用远程服务生成识别结果，同时在当前服务计算loss、更新模型。</li></ul></li></ul><h1 id="模型">5. 模型</h1><ul><li><p>Seed-ASR (CN): 支持普通话 + 13 种方言。</p><p>方言采用发音一致或相近的中文字人工标注。</p></li><li><p>Seed-ASR (ML): 多语种。</p></li></ul><h1 id="训练集">6. 训练集</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB21528af01689bd57f4696e263ebf1ecd?method=download&shareKey=1887613bba9befd9e32428f521992e20" width="685"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2acecd3709b395f47dd675259c49e4e5?method=download&shareKey=a44794eaebc698c05347e067b4c496a1" width="688"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe5ee6e28a0d282ef7988c3767c26e715?method=download&shareKey=06d804ba84658192b392d44ef871a6b5" width="690"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcd2af89299952fa7a60538527f0e0a27?method=download&shareKey=7bc5c62d614e77a39c4f0d8a467d0c75" width="693"></p><h1 id="实验">7. 实验</h1><ul><li>中文 ASR 基线之一：transducer-based end-to-end model<ul><li>MoE encoder，参数量超过 300M。</li><li>在有对话上下文的场景下，采用 context FST biasing提高关键词召回率。</li></ul></li></ul><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>模块/模型</th><th>任务</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>LUISE</td><td>scaling law</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2191f23a88aec0bc2664fd48c1923711?method=download&shareKey=8d31ef5e848881c824d853b3497c9ef5" width="916"><br>随模型大小增大，增大 batch size、weight decay，减小学习率。</td><td>取 2B 模型，性能和效率的权衡。</td></tr><tr><td>长音频 SFT</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe1231f6c8256fcdb0f36937d94758260?method=download&shareKey=661d800fa7e2dba9a73d80863dc39e99" width="570"><br>测试集中视频最大时长5min。<br>short-form SFT 采用 domain-adaptive VAD分段。<br><font color="green">with scheduler for significant lengthextension.</font></td><td></td></tr><tr><td>RL</td><td>reward、消融</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBdae99268abd29dabcf9d4301264ef2ad?method=download&shareKey=87670c0bbc39e7bf7b4bcee48efb8522" width="577"></td><td>相较于 WER，<font color="red">采用加权 WER作为奖励函数</font>，在各个测试任务上有一致的性能提升。<br>训练集加入部分带context 的数据，保留上下文推理能力。</td></tr><tr><td>分阶段训练</td><td>消融实验</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3b109bc16d822973470b15571d989591?method=download&shareKey=7a65973b850e78482d1a7681417ebeb8" width="665"></td><td>RL 在各个测试集上有一致的性能提升。<br>context SFT 显著提升了context-strict 测试集上的关键词召回率。</td></tr><tr><td>Seed-ASR (CN)</td><td>开源测试集</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB889416de34c24070bdfda3ea5ee582de?method=download&shareKey=830766d2841f796dc08d90946d4234b5" width="570"></td><td></td></tr><tr><td></td><td>多领域、多来源<br>场景包含视频、直播、语音搜索、会议、智能助手；<br>hardcase：书名、车名、习语、药名、电影名、古诗、产品名，音乐名等。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7e13b1e21402d67b0e4d4ae07a4b566d?method=download&shareKey=5aa639178f3094763c0abd584cdde946" width="572"></td><td></td></tr><tr><td></td><td>方言</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB8c33882faec81ead6cd8033bf3499999?method=download&shareKey=5910380324819d219303bf0951ebac18" width="368"><br>WhisperMedium-v2 采用相同的方言训练集微调。</td><td></td></tr><tr><td></td><td>口音<br>安徽、福建、甘肃、广东、贵州、湖南、江西、辽宁、陕西Shaanxi、山西 Shanxi、云南</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe29a84b060bbc7944bac338ee947dc93?method=download&shareKey=fa8679d0468e4e6c30f744b97f0f4268" width="380"></td><td></td></tr><tr><td></td><td>对话上下文</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa589e5df7a3ba2141ed8313126ea25ca?method=download&shareKey=b93fd71f23deb5eba687e87cd299e7f8" width="577"><br>strict:强依赖context，如人名；<br>Loose：弱依赖，如专有名词。<br>评价指标：关键词召回率。</td><td></td></tr><tr><td></td><td>可懂度主观评价<br>考虑识别准确率、关键词错误、分句错误、ITN错误等。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBdaf5f834661135392ae7bf0d9d5d4591?method=download&shareKey=8c49c185b9677867b33baae9a3afeec6" width="581"></td><td></td></tr><tr><td>Seed-ASR (ML)</td><td>开源测试集</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf4148a5e7d6b32c14e225f5b3a808092?method=download&shareKey=861964d1f0ad435937b391c6323ddda3" width="889"><br>MLSEN：本地测试 Whisper Large-v3，WER 5.29，见图1。<br>Fleurs: label 先ITN。</td><td></td></tr><tr><td></td><td>多领域、多口音、hardcase、多语种 内部测试集</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3ece8a75dd701159f6685b8bf9db1514?method=download&shareKey=77281144d8c73eca3a710d461a1aa33c" width="694"><br><a href="https://www.assemblyai.com/app/">Universal-1</a><br>英语口音：Great Britain, United States, Australia, Canada, China, India,Singapore, New Zealand and SouthAfrica.<br>hardcase：医疗健康、餐饮、运动、科技、服饰、游戏、娱乐、美容领域。</td><td></td></tr><tr><td></td><td>各语种</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB06a8624be6174394ebf91d43b8db9142?method=download&shareKey=bc709e7be3bc4e64fd92cb51ab0d00a5" width="684"></td><td></td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CLASI</title>
      <link href="/blog/yin-pin/yu-yin-fan-yi/lun-wen/clasi/"/>
      <url>/blog/yin-pin/yu-yin-fan-yi/lun-wen/clasi/</url>
      
        <content type="html"><![CDATA[<ul><li><p>论文：Towards Achieving Human Parity on End-to-end SimultaneousSpeech Translation via LLM Agent</p></li><li><p><a href="https://byteresearchcla.github.io/clasi/" class="uri">https://byteresearchcla.github.io/clasi/</a></p></li><li><p>作者：来自字节 Cross Language Agent Team</p></li><li><p>发表时间：2024.07.20</p></li><li><p>贡献</p><ul><li>端到端同声传译（语音输入，文本输出），性能接近人类转译员。</li><li><font color="red">提出了数据驱动的读-写策略，平衡翻译质量与延迟。</font>且输出是确定的，不会重写，可能用户体验更好。</li><li><font color="red">多模态检索增强生成。</font></li><li>提出了人工评价指标 valid information proportion(VIP)。发布了覆盖各类真实场景、长语音翻译、人工标注的<a href="https://github.com/byteresearchcla/RealSI">测试集</a>，en-zh、zh-en各 10 条，每条约 5min。相较于现有的朗读/准备良好的说话人，正式、清晰、流利，且人工分句的语音翻译数据集，更能反映真实的同声传译场景。</li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb42bdd84ad1ea1afb88c78c0da213fb4?method=download&shareKey=c0371518c23180ca5053bec8ba69b8ca" width="645"></p><p><font color="red"><span class="math display">\[\mathbf{y}_r ;t^r=\operatorname{TextDecoder}\left(\mathbf{x}_{t^{r-1}: T^r},\mathbf{k}_r, \mathbf{y}_{1: r-1}\right)\]</span></font> 其中，<span class="math inline">\(t^{r-1}\)</span>为系统预测的上一轮的截止时间，<span class="math inline">\(\mathbf{x}_{t^{r-1}: T^r}\)</span>为当前输入的音频，<span class="math inline">\(\mathbf{k}_r\)</span>为检索到的相关信息，<span class="math inline">\(\mathbf{y}_{1:r-1}\)</span> 为历史识别（可选）、翻译文本，<span class="math inline">\(\mathbf{y}_r\)</span>为当前预测结果，并用于更新记忆，<font color="red"><span class="math inline">\(t^r\)</span> 为预测的当前轮的截止时间</font>。</p><ul><li>agent：5 个操作 &lt;INPUT&gt;, &lt;RETRIEVE&gt;[可选], <load_mem>,&lt;OUTPUT&gt;, <update_mem></update_mem></load_mem></li><li>上文、语音、检索到的信息、指令，拼接，作为 LLM 的提示。<ul><li>结合历史信息有助于一词多义的正确翻译，也可与输入语音一起确定已经翻译了的部分。</li></ul></li></ul><h1 id="分段读-写策略">2. 分段/读-写策略</h1><p><font color="red">采用 LLM /人工，将长语音翻译切分为多个完整的语义块</font>。考虑语义完整性、逻辑连贯性（如条件句、使役句、对偶句应在同一段内）、语法完整性、停顿（语音中的停顿、标点、连词等）、适当的信息密度（避免信息超载）。通常是1个完整的句子。</p><blockquote><p>虽然系统内不需要实现复杂的分段规则，但标注指南需要明确规则；且有标注成本。</p></blockquote><p>训练：<font color="red"><span class="math display">\[\min\mathbb{E}_{t \sim U[1, M]}-\log p_\theta\left(\mathbf{y}_{1:j} ; t^j\mid \mathbf{x}_{1: t}\right) \quad j=\max _j\left\{j \midt^j&lt;t\right\}\]</span></font> 其中，<span class="math inline">\(U\)</span> 表示均匀分布，<span class="math inline">\(M\)</span> 为语音表示的帧数，<span class="math inline">\(\mathbf{y}_j\)</span> 为第 <span class="math inline">\(j\)</span> 个语义片段的翻译文本，<span class="math inline">\(t^j\)</span>为其对应的时刻。训练目标为给定随机截取的部分音频 <span class="math inline">\(\mathbf{x}_{1:t}\)</span>，输出所有完整语义片段的翻译文本，及对应的截止时间。</p><h1 id="multi-modal-retrieval-augmented-generation-mm-rag">3. Multi-ModalRetrieval Augmented Generation (MM-RAG)</h1><ul><li><p>key：文本，语音中可能出现的术语、人名、地名、缩略词等。</p></li><li><p>value：文本，可以是自身、目标语种的翻译、或解释。</p></li><li><p>embedding fusion 层</p><p>采用 multi-head attention + pooling，分别得到语音片段、各 key的表示。</p></li><li><p><font color="green">线性投影层，计算最终分，</font>表示语音片段包含key 的概率。</p></li></ul><h1 id="多阶段训练">4. 多阶段训练</h1><ul><li>音频 encoder、LLM 分别独立地预训练。<ul><li>音频encoder：conformer。pretrain-finetune。在数百万小时语音数据上预训练。达到了与人类相当的ASR 性能。</li><li>LLM：Doubao LLM。进行了纯文本预训练 +指令微调。在各种下游任务，特别是翻译任务上，性能很好。</li></ul></li><li>Multi-task Continual Training<ul><li>adapter：下采样 + 线性投影。</li><li><font color="red">任务：ASR、语音翻译、文本翻译。<img src="https://note.youdao.com/yws/api/personal/file/WEBb042b7f4238ed560099d0a1f72406539?method=download&shareKey=3533535fa07e1b17775ae5f75cfe9f18" width="756"></font></li><li>ASR、文本翻译数据集较多，采用 LLM 构建语音翻译数据。<ul><li>ASR 数据，LLM 翻译，被提示进行 ITN、filler word smoothing。</li><li><font color="red">分段流式数据：提示 LLM 将 ASR转写分为多个独立的语义块，然后翻译。音频进行强制对齐。</font></li><li>合成数据的 VIP 分达到了 81%，满足训练要求。</li></ul></li></ul></li><li>Multi-task SFT<ul><li>数据来源：真实场景，包含不流利、结巴、code-mixing、专业术语等各种语音现象。</li><li><font color="red">人工标注，分段、翻译、标注术语</font>，如<a href="https://github.com/byteresearchcla/RealSI/blob/main/data/en2zh/json/en2zh-01-tech.json">测试集</a>。覆盖表1 中的各语音翻译类型。</li><li>有助于对齐专业的人工转译员的分段方法，增强模型现实场景中翻译的鲁棒性。</li></ul></li><li><font color="red">多模态检索独立训练</font><ul><li>ASR数据。从转写中随机挑选词作为正例，从其它句子挑选的不在该句中的词作为负例。BCEloss。</li></ul></li><li><font color="red">利用少量的人工标注数据高效地学习，缓解了训练数据稀缺的问题。</font></li></ul><h1 id="vip">5. VIP</h1><ul><li>正确、清晰流畅（如不包含复杂的句子结构、语法结构，平滑口语中的重复、结巴等）地翻译了的语义片段的占比。</li></ul><h1 id="评价">6. 评价</h1><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>任务</th><th>结果</th></tr></thead><tbody><tr><td>翻译质量</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7de1a1bd330054e383366ae309fe6599?method=download&shareKey=c36232513e8272eb93a85f1f247a1733" width="643"><br>BLEU:SacreBLEU<br>专业人工转译员的 VIP 分约为 80%。</td></tr><tr><td>延迟</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf8fbf8a19673861d62b4193fa887e580?method=download&shareKey=8757609a85c043a02a808632818dd3fd" width="645"><br>AL:Average Lagging [45]<br>LAAL: Length Adaptive Average Lagging[52]<br>FLAL: First Letter AppearanceLagging，本文提出的，输出第一个<strong>确定的</strong>翻译的时间；<br>上述指标，对于会重写的系统，均采用输出<strong>确定的</strong>翻译的时间。<br>当预测结果显著长/短于参考翻译时，AL、LAAL可能不可靠。<br>专业人工转译员的 Ear-Voice-Span (EVS) 通常为3-6s。</td></tr><tr><td>开源测试集</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7637115766548c987935e086f406122b?method=download&shareKey=301daccf739f8a4387a16349d8f497f3" width="766"></td></tr><tr><td>多模态检索</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB30f3c87c115bec42af3700935c2cd945?method=download&shareKey=ace13550a21bfd898c827c1108feb501" width="496"><br>audio-to-audio：采用TTS 合成术语。<br>top-k：采用Maximum Inner Product Search (MIPS)。</td></tr><tr><td>MM-RAG</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa39b6fcb11c0423064f86f7a7af9290d?method=download&shareKey=a92eefe411030530a88d7338458956d0" width="642"></td></tr><tr><td>VIP 与客观指标的相关性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2055f6b02b00c9998621a3e0822cafb9?method=download&shareKey=892861c50f025c09b87086ea74bcb9e7" width="651"><br>Kendall’sTau 相关系数<br>当 VIP 超过 40%后，客观评价指标无法较好地反映翻译质量的变化，可能由于虽然文本差异较小，但客观指标无法很好地评价关键词的准确性等。</td></tr><tr><td>case 分析</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa98907331a56e0c4dfbd006789c0bfeb?method=download&shareKey=060cf96c791bdef5a9d4f49f68b43c91" width="748"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB2d24bb84a2daf1c8493262485c20644f?method=download&shareKey=893ef89b5bf8c4af5e829809b18f64af" width="662"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB89c5507679d980f37d5e23b534efaaeb?method=download&shareKey=ddf3a63d102909f8d7c9c03d1371f390" width="524"></td></tr></tbody></table><h1 id="其它">7. 其它</h1><ul><li>语音分段策略<ul><li>固定长度分段、检测语音中的单词边界[46]、检测语音单元边界（自适应分段）[16]。本文输入语音是固定分段。</li></ul></li><li>由于不同语言语法结构不同，延迟是不可避免的。</li><li>展望<ul><li>MM-RAG对于简单的场景是可选的，训练模型决定是否跳过不必要的动作。</li><li>RLHF 奖励函数。</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音翻译 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FunAudioLLM</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/funaudiollm/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/funaudiollm/</url>
      
        <content type="html"><![CDATA[<style>    .red-text {        color: red;    }    .green-text {        color: green;    }</style><ul><li><p>论文：SpeechTeam T. FunAudioLLM: Voice Understanding andGeneration Foundation Models for Natural Interaction Between Humans andLLMs[J]. arXiv preprint arXiv:2407.04051, 2024.</p><p>Du Z, Chen Q, Zhang S, et al. CosyVoice: A Scalable MultilingualZero-shot Text-to-speech Synthesizer based on Supervised SemanticTokens[J]. arXiv preprint arXiv:2407.05407, 2024.</p></li><li><p>作者：阿里 通义语音组</p></li><li><p>代码：<a href="https://github.com/FunAudioLLM" class="uri">https://github.com/FunAudioLLM</a></p></li><li><p>Demo：<a href="https://fun-audio-llm.github.io" class="uri">https://fun-audio-llm.github.io</a></p></li></ul><h1 id="sensevoice">1. SenseVoice</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0c0a94f4c962a334f2007ab364e5a550?method=download&shareKey=101e5efbd0b1ac52818f6b95f2b11456" width="579"></p><ul><li>能力：多语种语音识别、语种识别、情感识别、音频事件分类/检测、ITN带标点。</li></ul><div class="red-text"><ul><li>label 制作<ul><li>AED <a href="https://github.com/qiuqiangkong/audioset_tagging_cnn/tree/master" class="uri">https://github.com/qiuqiangkong/audioset_tagging_cnn/tree/master</a></li><li>语音情感识别 <a href="https://modelscope.cn/models/iic/emotion2vec_plus_large" class="uri">https://modelscope.cn/models/iic/emotion2vec_plus_large</a></li></ul></li></ul></div><h2 id="sensevoice-small">1.1. SenseVoice-Small</h2><ul><li>5个语种（中、英、粤语、日、韩）。支持音频事件分类（支持音乐、掌声、笑声、咳嗽、打喷嚏、喘息、哭），1句话内至多1个事件。训练集30wh+。</li><li>模型结构<ul><li>80 维 FBANK。<font color="green">堆叠连续帧，6倍下采样。</font>语音特征前拼接可选的 4 个任务embedding：⟨LID⟩、⟨SER⟩、⟨AEC⟩、⟨ITN⟩ / ⟨NoITN⟩。</li><li><font color="green">encoder: memory-equipped self-attention network(SAN-M)。</font></li></ul></li><li>训练<ul><li>训练时，以 0.8 的概率随机将 ⟨LID⟩ 替换为真实语种token。使得推理时，既支持语种预测，也支持提示语种。</li></ul></li></ul><h2 id="sensevoice-large">1.2. SenseVoice-Large</h2><ul><li>50+个语种。<font color="green">支持音频事件检测（含起止时间，仅支持音乐、掌声、笑声）。</font>训练集40wh+。<img src="https://note.youdao.com/yws/api/personal/file/WEB87ee4d75d1af1da8ff88a70ae8284e76?method=download&shareKey=783a8c52c6ec2ea5cfd36f320fe05cc8" width="565"></li></ul><h2 id="评价">1.3. 评价</h2><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>任务</th><th>结果</th></tr></thead><tbody><tr><td>参数量、支持语种、效率对比</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7a238373446c22d5ed320a4ea296d36c?method=download&shareKey=3dfb578e427d98886c3472235a3ce120" width="568"><br>10sAudio Latency: 转写10s音频的平均耗时；<br>A800, batch size=1, beamsize=5</td></tr><tr><td>中英文识别</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB605b17f2c877d5ab10043db6f4f63293?method=download&shareKey=9fae4ec78c692e0987900f50987791ba" width="565"><br>TN:Whisper TN + <a href="https://github.com/speechio/chinese_text_normalization/blob/master/python/cn_tn.py">speechioTN</a></td></tr><tr><td>多语种识别</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1a7f3543a1fe11668bdc40b3230e336a?method=download&shareKey=8f17a6f382e466d8dfa0da91a6e84092" width="678"><br><img src="https://note.youdao.com/yws/api/personal/file/WEBa06fee1592348de4097f8981ece3db8c?method=download&shareKey=c9aa9e648c8b8e7b2943323685670f11" width="522"><br>评价指标：CER：中、粤语、日、韩、泰；WER：其它语种。</td></tr><tr><td>语音情感识别</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB621a756f1adedf071213aeb065232bab?method=download&shareKey=4e80a0fbd62553095d05555a59375001" width="677"><br><img src="https://note.youdao.com/yws/api/personal/file/WEBbb9f5f76b53001aa2695001b83c5dec4?method=download&shareKey=69dff7085f20b9fda4b3233976759637" width="549"><br><a href="https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition">XLSR-SER</a></td></tr><tr><td>声音事件分类</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5f26b2743fac9c0937a2b83f1a82069b?method=download&shareKey=2ec356b9c546e8705ad6eb02c77b8d46" width="913"><br>SOTA:BEATs、PANNs；<br>数据集：<a href="https://github.com/giulbia/baby_cry_detection/tree/master">babycry/laugh detection</a>、<a href="https://github.com/iiscleap/Coswara-Data/tree/master">Coswara</a></td></tr></tbody></table><ul><li>声音事件分类<ul><li>SenseVoice、Qwen-audio：准确度显著地高于 recall，交互友好。</li><li>SenseVoice 未采用专门的 AED 数据训练，而是采用 ASR 数据 + AED伪标签训练。</li></ul></li></ul><h1 id="cosyvoice">2. CosyVoice</h1><h2 id="功能">2.1. 功能</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc51cd4370e74e696eb1ba0d00008b2f8?method=download&shareKey=3780b891dce3b80f1e29a1bc1253bdbb" width="612"></p><ul><li><p>多语种语音合成：中、英、日、粤语、韩。</p></li><li><p>CosyVoice-base-300M：支持 zero-shot in-contextlearning、跨语种音色克隆。<font color="red">提示语音3s</font>，克隆音色、情感、风格、韵律等。</p></li><li><p><font color="red">CosyVoice-instruct-300M：支持生成富有表现力的声音，采用文本指令进行细粒度控制，如说话人，说话风格（包含情感、性别、语速、音高等），副语言特征（如插入笑声、呼吸、笑着说话、强调某些单词）等。</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB789e536bc7a9227c9357aecd570d02ed?method=download&shareKey=8387eb16e28627caf0556c1825a231f5" width="570"></p></li><li><p>CosyVoice-sft-300M：在 7 个多语种说话人上进行了微调。</p></li></ul><h2 id="系统结构">2.2. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9d38a3b2013078ba227731cff3fc58f1?method=download&shareKey=b7b04a2c88bd398ff43f6a53b6cb9a3d" width="561"><img src="https://note.youdao.com/yws/api/personal/file/WEBc99c2e322c6bf43a70a95ef359b0fac7?method=download&shareKey=2cb78920922c4779f145ce7d2b83854a" width="147"></p><p>* <font color="green">CosyVoice 论文中，文本经过 BPE tokenizer后、输入 LLM 前，会经过额外的 text encoder 模块，用于对齐语音、文本token 的语义空间。在训练集文本上训练 word sentence-piece 模型，词典大小4000。</font></p><p>* LLM 建模语义内容和韵律。说话人 embedding：x-vector，从<a href="https://github.com/alibaba-damo-academy/3D-Speaker/tree/main/egs/3dspeaker/sv-cam++">预训练的voice-print模型</a>中提取。</p><p>* <font color="green">flow matching: ordinary differential equationbased (ODE-based) diffusion model。</font><font color="red">相较于传统diffusion 模型，加速了训练、推理。</font></p><p>* <font color="red">HiFTNet：支持流式生成。</font></p><p>*虚线模块是可选的，只用于特定的应用。</p><h2 id="离散-token">2.3. 离散 token</h2><ul><li><a href="https://nanyang2015.github.io/blog/llm/yin-pin-sheng-cheng/lun-wen/audiopalm">AudioPaLM</a>表明在语音翻译、ASR 任务上，采用辅助的 ASR loss 量化得到的离散token，相较于原始 k-means 聚类，性能更好。</li></ul><div class="green-text"><ul><li><p>初步实验表明，语音 tokenizer的选择对系统性能，对数据数量、质量的要求至关重要。评价了 3 类：</p><ul><li>基于残差量化，如 SoundStream、Encodec、FunCodec。</li><li>基于 multi-grouped 量化，如HifiCodec。</li><li>语义 token，如 HuBERT。</li></ul><p>均无监督或自监督训练，因而与语义关联较少，导致合成不稳定，容易受数据噪声影响，需要大量干净的训练数据。</p></li><li><p>本文提出 supervised semantic speech (<span class="math inline">\(S^3\)</span>) tokenizer。采用预训练的SenseVoice-Large，在 encoder 第6层后插入向量量化，单码本，大小4096，1token / 20ms。提取的 token和语音内容、副语言信息强相关。由于监督训练，增强了对数据噪声的鲁棒性、不依赖于干净的语音训练集。消除了文本转音素的需求。</p></li></ul></div><ul><li>量化 <span class="math display">\[\mu_l=\operatorname{VQ}\left(\mathbf{h}_l,C\right)=\arg \min _{\mathbf{c}_n \inC}\left\|\mathbf{h}_l-\mathbf{c}_n\right\|_2\]</span> 其中，<span class="math inline">\(\mathbf{h}_l\)</span> 为第 <span class="math inline">\(l\)</span> 帧的隐层表示，<span class="math inline">\(C\)</span> 为 codebook。</li><li>训练时，codebook embeddings 采用指数移动平均更新。<span class="math display">\[\mathbf{c}_{\mu_l}:=\alpha\mathbf{c}_{\mu_l}+(1-\alpha) \mathbf{h}_l\]</span> 其中 <span class="math inline">\(\alpha\)</span> 为预定义的衰减系数。</li><li>之后加入额外的位置 embedding，增强时序信息。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB29d94980872572a4daa401a86fa98ad8?method=download&shareKey=0e7868a9aaf19f0df6f97e2c193aa3e4" width="402"></p><h2 id="数据集">2.4. 数据集</h2><ul><li><p>训练集 17wh+<img src="https://note.youdao.com/yws/api/personal/file/WEB5c124e6edc996fdd726086c26262f838?method=download&shareKey=fafd752bcadb81969d8b0c4a1ca4c5f2" width="370"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbde7b8309d6f5b103713cb96fe101466?method=download&shareKey=38326ed5ba57a8fc01dc5e889959d2f0" width="358"></p></li></ul><div class="red-text"><ul><li>数据集构建<ul><li>语音检测，信噪比（SNR）估计，说话人 diarization、分离。</li><li>ASR：SenseVoice-Large、Paraformer。采用强制对齐进一步过滤、<font color="green">增强标点的准确性</font>。</li></ul></li></ul></div><h2 id="训练">2.5. 训练</h2><ul><li><p>text encoder + LLM</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2f2ec813d13030de7b60025f15bea1f7?method=download&shareKey=3b11e7f4d2d9af2379d98ba3a9bb5533" width="277"></p></li><li><p>flow matching</p><ul><li>估计条件概率 <span class="math inline">\(P\left(S \mid X, v,S_{ref}\right)\)</span>，其中 <span class="math inline">\(X\)</span>为语音 token（含提示语音 token + 生成语音 token），<span class="math inline">\(v\)</span> 为说话人 embedding，<span class="math inline">\(S_{ref}\)</span> 为参考语音的 mel谱，用于进一步增强音色、声学环境一致性，<span class="math inline">\(S\)</span> 为目标语音的 mel 谱。</li><li>采用 classifier-free guidance、cosine scheduler、maskedconditions。</li></ul></li><li><p>zero-shot in-context learning<img src="https://note.youdao.com/yws/api/personal/file/WEB5ec8723efcfbae568059bce352f32142?method=download&shareKey=e0d015b5257e32b749fb0db1433622b6" width="567"></p><p>* 其中，灰色方块表示在文本 token 和语音 token 间插入的 “turn ofspeech” token。</p><ul><li>若提示语音、输入文本同语种，拼接，作为 AR 模型输入，将提示语音 token视为已经生成的。</li><li>若提示语音、输入文本语种不同，丢弃源语种的提示文本、音频，避免源语种的韵律影响目标语种。</li><li>提示语音对应的文本可以人工标注或 ASR。</li></ul></li><li><p>CosyVoice-instruct</p><p>采用指令数据微调 CosyVoice-base，不输入说话人 embedding。</p></li></ul><h2 id="评价-1">2.6. 评价</h2><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>维度</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>中英文合成质量<br>WER: 分别采用 Whisper-Large V3、Paraformer；<br>SS: 说话人相似度，ERes2Net提取的提示语音、生成语音的说话人 embedding 的余弦相似度；<br>AR模型：随机采样解码，采用5个随机种子 0, 7, 42, 123, 1337分别合成；<br><font color="green">ASR re-ranking</font></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB740922900c4ff0366461bf1f8e1d9648?method=download&shareKey=568eb8302a5fc251021ec213612b9581" width="572"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB03bb668c3431023b32db28361436fbcc?method=download&shareKey=13f0a9b0e8f2a6b7ecf4b4c8d1e7f4dc" width="568"></td><td>CosyVoice 合成的语音达到了和人类水平相近的 WER /可比的CER，说话人相似度更高；<br>ASR re-ranking显著增强了内容一致性；<br><font color="green">ChatTTS 由于 speakerleaking，有更多的插入、删除错误。</font></td></tr><tr><td>情感控制<br>情感识别模型：<a href="https://modelscope.cn/models/iic/emotion2vec_base_finetuned">emo2vec</a><br>合成文本与目标情感是匹配的。<br>CosyVoice-base只输入文本。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5fcce28971440a9015c783612661d57d?method=download&shareKey=42396e691fdcf233e57bbb8cbd814707" width="570"></td><td></td></tr><tr><td>文本 token、语音 token、训练集+模型规模 的影响</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB17c53b753ec1e5b2d80f485e3892c2eb?method=download&shareKey=37cdba0cc8415ba5e723278b3f45a029" width="572"><br>Exp-x-LibriTTS:CosyVoice 论文中在 LibriTTS 数据集上训练的小模型</td><td>对比 Exp-1、Exp-2，<font color="red">相较于 HuBERT，采用 <span class="math inline">\(S^3\)</span>tokenizer，合成语音的内容一致性显著提升；</font><br>对比Exp-1、Exp-2、Exp-3，<font color="red">文本、语音 tokenizer对内容一致性都很重要</font>，但几乎不影响说话人相似度。</td></tr><tr><td><span class="math inline">\(S^3\)</span> 语音 tokenizer对识别率的影响</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB71f087a2584e9133fd35a3caebbb47fd?method=download&shareKey=1326a50e3dbf594653fc9973422ed539" width="572"></td><td><font color="red">采用离散化表示，语音识别性能损失较明显。</font></td></tr><tr><td>用于生成数据</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe59ae86c18b0722fa99a1fc14c566231?method=download&shareKey=8ed80287f6521063ffc63254546ff6da" width="576"></td><td><font color="red">仅在合成数据上训练，可以达到原始 Librispeech训练集可比的 ASR 性能；<br>结合两者，ASR性能显著提高；<br>添加<font color="green"> MLS训练集文本</font>的合成语音，显著提高了识别效果。可能表明对于ASR，文本丰富度的重要性高于语音时长。</font></td></tr></tbody></table><h1 id="展望">3. 展望</h1><ul><li>语音合成<ul><li>还不能从文本直接推断合适的情感、语音风格。</li><li>歌声。</li><li>有表现力的情感变化。</li></ul></li><li>与 LLM 端到端训练。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>离散 token、qwen、流式 ASR</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/lun-wen/bti/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/lun-wen/bti/</url>
      
        <content type="html"><![CDATA[<p>论文：Chen P, Sun S, Shan C, et al. Streaming Decoder-Only AutomaticSpeech Recognition with Discrete Speech Units: A Pilot Study[J]. arXivpreprint arXiv:2406.18862, 2024.</p><p>代码：<a href="https://github.com/chenpk00/IS2024_stream_decoder_only_asr" class="uri">https://github.com/chenpk00/IS2024_stream_decoder_only_asr</a></p><ul><li>作者：来自西北工业大学、度小满。</li><li>发表时间：2024.06</li></ul><h1 id="创新点">1. 创新点</h1><h2 id="流式">1.1. 流式</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8a93ff25bce345a05eb0d3d3ac2ae7a2?method=download&shareKey=1d954630708ad8a5b9b7004e677d58d5" width="833"></p><ul><li>基于强制对齐得到 label。对比了两种流式方案<ul><li>Text Token Insertion (TTI)：在语音 token 中插入文本 token。<ul><li>文本 token beam search 解码变复杂。</li></ul></li><li>Boundary Token Insertion (BTI)：在语音 token 中插入边界token，最后拼接文本 token。<ul><li>推理时，触发边界 token 后，即可一步推理文本 token。</li></ul></li></ul></li></ul><h2 id="right-chunk-attention">1.2. right-chunk attention</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB09d5ba1af886bd43bff66a3e12c1a6fc?method=download&shareKey=e6c8974127e4b43413a0bb17b3934cd2" width="331"></p><p>语音 token 序列 <span class="math inline">\(x=\left(x_1, \ldots,x_T\right)\)</span>，文本 token 序列 <span class="math inline">\(y=\left(y_1, \ldots, y_L\right)\)</span>。</p><ul><li><p>传统因果 attention mask</p><p>语音 mask：可以看见过去的语音 token。<span class="math display">\[M_{\text {speech }}(i, j)= \begin{cases}\text {True } &amp; i \leq j \\ \text { False } &amp; \text { otherwise}\end{cases}\]</span></p><p>文本 mask：可以看见过去的语音、文本 token。 <span class="math display">\[M_{\text {text }}(i, j)= \begin{cases}\text {True } &amp; i \leq t_{y_j} \\ &amp; \text { or } T+L&lt;i \leq j \leqT+2 L \\ \text { False } &amp; \text { otherwise }\end{cases}\]</span>其中 <span class="math inline">\(t_{y_j}\)</span> 为文本 token <span class="math inline">\({y_j}\)</span> 对应的边界 token 的index。</p></li><li><p>right-chunk attention</p><p><span class="math display">\[M_{\text {text }}(i, j)=\begin{cases}\text { True } &amp; j \leq t_{y_i+\Delta} \\ &amp; \text {or } T+L&lt;i \leq j \leq T+2 L \\ \text { False } &amp; \text {otherwise }\end{cases}\]</span> 其中，<span class="math inline">\(\Delta\)</span> 为常数，表示可以看见的 rightcontext token 数。训练时，动态地设为下一个 text token对应的语音片段长度。<font color="green">推理时?</font></p></li></ul><h2 id="损失函数">1.3. 损失函数</h2><ul><li><p>传统 decoder-only transformer、TTI <span class="math display">\[p(y \mid x ; \theta)=\prod_1^L p\left(y_i \mid x_{\leq t_{y_i+\Delta}},\theta\right) \]</span></p></li><li><p>BTI</p><p>对于音频-文本对，计算所有可能的边界路径下的后验概率复杂度很高，因此，只考虑最可能的路径<span class="math inline">\(b_p\)</span>。<span class="math display">\[p(y \mid x, \theta) \approx \prod_1^L p\left(y_i \mid b_p, x \leqt_{y_i+\Delta}, \theta\right) p\left(b_p \mid x \leq t_{y_i+\Delta},\theta\right)\]</span></p></li></ul><h2 id="数据增强">1.4. 数据增强</h2><p>发现相较于采用连续语音表示，采用离散表示更容易过拟合。</p><ul><li>语速扰动。</li><li>以 30% 的概率随机移动 trigger token 1-4 帧。</li><li>时间 mask：以 30% 的概率将语音、文本 token（不包含 triggertoken）替换为 special pad token。</li><li>随机去重：减小计算量、增加数据多样性。解码时，全局去重，以减小计算量。</li><li>label smooth：由于语音表示离散化丢失了信息。<span class="math display">\[\mathcal{L}_{\mathrm{LS}}=\sum_{t=1}^{T_s}D_{\mathrm{KL}}\left(q^{\prime}\left(x_t \mid x_{&lt;t}\right) \|p\left(x_t \mid x_{&lt;t} ; \theta\right)\right)\]</span> 其中，<span class="math inline">\(q^{\prime}\left(x_t \mid x_{&lt;t}\right)\)</span>采用 soft label。</li></ul><h1 id="评价">2. 评价</h1><ul><li>小模型 70M 参数，无 LLM 初始化；大模型 310M 参数，Qwen2-0.5B?Qwen1.5-0.5B? 初始化，最大序列长度1024？</li><li>离散 token：<a href="https://huggingface.co/TencentGameMate/chinese-hubert-large">ChineseHuBERT large</a> 第 21 层的表示（参考 [30] 选择）、k-means 聚为 2000类。文本 token 仅采用常用的 5000 个中文字符作为 token。</li><li>beam size 10。</li></ul><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>结果</th><th>分析</th></tr></thead><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7c77f3103140b7c437e6642e5c00096e?method=download&shareKey=cd0c672aa69ca4b92ac48d3df2322a1a" width="485"><br>large模型 LLM 初始化，先训非流式模型，再分别用两种流式策略微调。</td><td>S2 替换错误显著增加</td></tr><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEBd26217a287f3a58d1055c53708b8da5c?method=download&shareKey=a5faf6a0579900a6e1b65a1b80d7ed1a" width="493"></td><td></td></tr><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe2bb82f9967e72196e0cb7e9c22e1322?method=download&shareKey=1a1d65b0cc0bcef32a7a494709c6691c" width="421"></td><td>1. right-chunk attention 收益最大，去掉后存在较多替换错误；<br>2.数据增强策略中 label smoothing 收益最大。</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HTS-AT</title>
      <link href="/blog/yin-pin/yin-pin-shi-jian/lun-wen/hts-at/"/>
      <url>/blog/yin-pin/yin-pin-shi-jian/lun-wen/hts-at/</url>
      
        <content type="html"><![CDATA[<ul><li><font color="red">相较于传统 audio spectrogramtransformer(AST)，采用分层结构、patch merge 减小了序列长度，采用 windowattention 减小了 self attention计算量，减小了参数量、训练时间。</font></li><li><font color="red">声音事件时间定位：添加 token-semantic CNN层。</font></li><li>性能：优于或与 SOTA 相当。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbd464af80c97209dcb0033874c43a5c8?method=download&shareKey=64e7d83163db068a4976710c2efd7a6f" width="1157"></p><h2 id="patch-merge">1.1. patch merge</h2><p><font color="green"><span class="math inline">\(\left(\frac{T}{P}\times \frac{F}{P}, D\right)\)</span> -&gt; <span class="math inline">\(\left(\frac{T}{2P} \times \frac{F}{2P},4D\right)\)</span> -&gt; 线性层 -&gt; <span class="math inline">\(\left(\frac{T}{2P} \times \frac{F}{2P},2D\right)\)</span></font></p><h2 id="swin-transformer">1.2. Swin Transformer</h2><p>采用 swin transformer，将 transformer block 中的全局 self attention替换为 window attention：分别计算多个非重叠 <span class="math inline">\(M \times M\)</span> 的窗内的 attention矩阵。用于减少计算量。对于维度为 <span class="math inline">\((t,f,D)\)</span> 的音频表示，计算复杂度分别为：<span class="math display">\[ GA: \mathcal{O}\left(f t D^2+(f t)^2D\right) \]</span> <span class="math display">\[ WA: \mathcal{O}\left(ft D^2+M^2 f t D\right) \]</span></p><blockquote><p>其中，D 维的表示经过 <span class="math inline">\(D \times D\)</span>的矩阵进行映射，复杂度为 $(f t D^2) $ self attention QK 计算复杂度为$((f t)^2 D) $ window attention QK 计算复杂度为 $((M M)^2 D ) = (M^2 f tD) $</p></blockquote><h2 id="token-semantic-module">1.3. Token-Semantic Module</h2><p><font color="green">kernel <span class="math inline">\(\left(3,\frac{F}{8 P}\right)\)</span>，输出 <span class="math inline">\(\left(\frac{T}{8 P}, C\right)\)</span></font></p><h1 id="评价">2. 评价</h1><h2 id="测试集">2.1. 测试集</h2><ul><li>声音事件分类：AudioSet、ESC-50</li><li>关键词提取：Speech Command V2</li><li>声音事件检测：DESED</li></ul><h2 id="模型结构">2.2. 模型结构</h2><ul><li>音频预处理：10s 音频，mel 谱，64 bins，mel 谱补 24 个全 0帧，得到维度为 (1024, 64) 的表示。</li><li>patch：<span class="math inline">\(4 \times 4\)</span>，窗 256帧。</li><li><span class="math inline">\(D=96\)</span>。</li><li>swin-transformer group 中分别包含 2、2、6、2 个 blocks。windowattention：窗 <span class="math inline">\(8 \times 8\)</span>。</li><li>采用 ImageNet 预训练模型 <a href="https://github.com/microsoft/Swin-Transformer">Swin-T/C24</a>，由于<span class="math inline">\(256 \times 256 = 1024 \times64\)</span>，可以直接迁移预训练权重。</li></ul><h2 id="训练">2.3. 训练</h2><ul><li><font color="green">采用 balance sampler，<span class="math inline">\(\alpha=0.5\)</span> mix-up</font> ( [19] mixup:Beyond empirical risk minimization)。</li><li>集成：权重平均。</li></ul><h2 id="结果">2.4. 结果</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9cca8567f58687f7113b2b21ef7a8c26?method=download&shareKey=bcd9b92b28b737333cf2b22edb2388bd" width="326"></p><p>* H: 分层结构；HC: + token-semantic 模块；HCP：++预训练视觉模型。</p><ul><li>采用预训练的视觉模型，基于其较强的模式识别能力，显著提升了性能。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB42aeddd7ffdd43d236f0ffa1eeddc403?method=download&shareKey=151c5bb89274150bd29145531651b329" width="353"></p><p>*基于 best AudioSet-pretrained HTS-AT 训练。benchmark 模型也在AudioSet 或其它数据上进行了预训练。</p><p>*复制音频直到 10s。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB85b93b3c4afea72ec379ca8cbb7c3925?method=download&shareKey=8a4b405603e9f634cd0609391c1d4a18" width="722"></p><p>* <a href="https://github.com/audioanalytic/psds_eval">event-basedF1-scores Python 实现</a></p><h1 id="其它">3. 其它</h1><ul><li>Chen K, Du X, Zhu B, et al. Hts-at: A hierarchical token-semanticaudio transformer for sound classification and detection[C]//ICASSP2022-2022 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP). IEEE, 2022: 646-650.</li><li><a href="https://github.com/RetroCirce/HTS-Audio-Transformer">代码</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频事件 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Qwen2-Audio</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/qwen2-audio/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/qwen2-audio/</url>
      
        <content type="html"><![CDATA[<ul><li>优化<ul><li><p>对话能力，支持输入音频或文本，输出文本。指令微调 + DPO。</p><blockquote><p>SFT 数据的质量、复杂度严重影响模型性能。</p></blockquote></li><li><p>扩大训练数据量。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb3ebb9c50e9257bb980361eacbb3492a?method=download&shareKey=91a4600e9716453d6a5cc590295fe42c" width="268"></p></li><li><p>预训练阶段，复杂的结构化的任务描述 special tokens -&gt;自然语言任务提示。减小预训练、指令微调阶段的 gap。</p><blockquote><p>We find that using language prompts can improve better generalizationability and better instruction following ability.</p></blockquote></li><li><p>音频 encoder Whisper Large-V2 -&gt; Whisper Large-V3。LLM Qwen-7B不变。</p></li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbd1b33945c35e4cd4ff5cac1fc5ff12f?method=download&shareKey=f87dddd02afbaf8f97adf28545da8d8a" width="975"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5b9d0261d33950bb062808cd7f2c0cad?method=download&shareKey=c87e1e76d07b0e2a890fa69b9112c12c" width="666"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB43238bc3680edec539677b091efe4ad8?method=download&shareKey=b0d2103cf79b7cceebdb1663ef5eb4a6" width="679"></p><p><a href="https://www.arxiv.org/abs/2407.10759">Qwen2-Audio TechnicalReport</a></p><p><a href="https://github.com/QwenLM/Qwen2-Audio">测试代码（暂未发布）</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CLAP HTS-AT GPT2</title>
      <link href="/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/clap-htsat-gpt2/"/>
      <url>/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/clap-htsat-gpt2/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：微软</li><li>发表时间：2023.09</li><li><font color="red">创新点：更大的训练集、更新 encoder。</font></li><li><font color="red">性能：在 26个下游任务上的广泛评估显示了模型较好的泛化性，且在其中几个任务上建立了新的SOTA。</font></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB748d95383516cd72cb00cd83b2310108?method=download&shareKey=ec2d84e4538131bc2365e2eb9bcfcb6c" width="582"></p><h1 id="训练">1. 训练</h1><ul><li><p>训练集</p><p>4.6M 音频-文本对：WavCaps, AudioSet, FSD50K, Clotho, AudioCaps, MACS,WavText5k, SoundDesc, NSynth, FMA, Mosi, Meld, Iemocap, Mosei,MSP-Podcast, CochlScene, LJspeech, EpicKitchen, Kinectics700,findsounds.com.</p></li><li><p>音频预处理</p><p>44.1kHz，帧长 1024，帧移 320，mel 截止频率 [50, 8000]，64bins。随机截断或填充到 7s。</p></li><li><p>audio encoder：HTS-AT，在 22 个音频任务上训练。</p></li><li><p>text encoder：GPT2 base，AR decoder-only。</p><p><font color="green">训练时，在文本后添加 &lt;|endoftext|&gt;token，取该 token的表示作为句子级表示，使其包含文本的聚合信息。</font></p></li><li><p>线性层：输出 1024 维。</p></li><li><p>音频字幕任务</p><p>采用下列损失 <span class="math display">\[ \mathcal{L}=-\sum_{i=1}^N\sum_{j=1}^l \log p_\gamma\left(c_j^i \mid p_1^i, \ldots, p_{2 k}^i,c_1^i, \ldots, c_{j-1}^i\right) \]</span> 其中，<span class="math inline">\(p^i\)</span> 为音频提示，<span class="math inline">\(c^i\)</span> 为自回归文本输出。只训练 mapper网络。</p></li><li><p>batch size：1536。</p></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbcf7e3f3664de3c1d9be922477acb32a?method=download&shareKey=7d538b992201601ba210fe1541e5144b" width="722"><img src="https://note.youdao.com/yws/api/personal/file/WEBe41fe431669f4ae4735c6dece796e041?method=download&shareKey=db8d29fde0efe684866ea7a5ba430e08" width="723"></p><h1 id="其它">2. 其它</h1><ul><li>论文：Elizalde B, Deshmukh S, Wang H. Natural language supervisionfor general-purpose audio representations[C]//ICASSP 2024-2024 IEEEInternational Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 2024: 336-340.</li><li><span id="code"><a href="https://github.com/microsoft/CLAP">代码</a></span></li><li><font color="red">CLAP 提出后的后续研究表明：encoder的选择对表示能力、跨任务的性能至关重要。扩大训练集可以提高整体性能，且训练对的多样性、噪声程度影响特定任务的性能。</font></li><li>若直接采用类标签，而不是描述，可能丢弃了上下文和语义信息。</li><li>参考文献<ul><li>HEAR: Holistic Evaluation of Audio Representations</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 多模态表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CLAP</title>
      <link href="/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/clap/"/>
      <url>/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/clap/</url>
      
        <content type="html"><![CDATA[<style>    .red-text {        color: red;    }    .green-text {        color: green;    }</style><ul><li>作者：微软</li><li>发表时间：2022.06</li></ul><div class="red-text"><ul><li>CLAP：Contrastive Language-Audio Pretraining</li><li>相较于特定任务、预定义类，采用自然语言监督，学习音频概念。</li><li>性能：部分任务上实现了 zero-shot 和有监督的 SOTA 性能。</li></ul></div><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB37ee42790442537f74c5ced911401d34?method=download&shareKey=56aca09377ebe06fd8cd215a373f2149" width="726"></p><ul><li><p>audio encoder + 线性层、text encoder +线性层。线性层用于投影到联合 embedding 空间，统一维度。</p></li><li><p><font color="green">损失函数</font> <span class="math display">\[\begin{align}C &amp;= \tau *\left(E_t \cdot E_a^{\top}\right) \\\mathcal{L} &amp;= 0.5 *\left(\ell_{\text {text }}(C)+\ell_{\text {audio}}(C)\right) \\\ell_k &amp;= \frac{1}{N} \sum_{i=0}^N \log\operatorname{diag}(\operatorname{softmax}(C))\end{align}\]</span></p><p>其中，<span class="math inline">\(\tau\)</span> 为温度参数；<span class="math inline">\(C \in \mathbb{R}^{N \timesN}\)</span>；沿文本、音频轴分别计算。</p></li><li><p>推理：余弦相似度，softmax 或 sigmoid（multilabel）。</p></li></ul><h1 id="实验">2. 实验</h1><h2 id="训练集">2.1. 训练集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6af45c0fa3227c9941ff88d74ddf53c3?method=download&shareKey=ac1f8517e11579c994c72e2a65b8982b" width="256"></p><p>* FSD50k：拼接标题、描述，不采用类标签。</p><p>合计 128k 音频-文本对</p><h2 id="音频预处理">2.2. 音频预处理</h2><p>44.1kHz，帧长 1024，帧移 320，mel 截止频率 [50, 8000]，64 bins。随机截断或填充到 5s。</p><h2 id="encoder">2.3. encoder</h2><ul><li>audio encoder：PANNs CNN14，embedding 2048维，<font color="green">采用 AudioSet 2M 音频预训练</font>。</li><li>text encoder：BERT base uncased。取 [CLS] embedding 768 维。<ul><li>限制文本长度最多 100 字符。</li></ul></li><li>线性层：输出 1024 维。</li><li><span class="math inline">\(\tau\)</span>：可学习，初始值0.007。为了避免训练不稳定，<span class="math inline">\(\tau\)</span>缩放后的值采用最大值 100 进行裁剪。</li><li>训练：学习 encoder、线性层。</li></ul><h2 id="评价">2.4. 评价</h2><ul><li><p>测试集</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3bb1bc41cf2234ec8ba1e6b49a6fa82a?method=download&shareKey=edcbbad952e44a80ecdc33a044feff3d" width="554"></p></li><li><p>评价设置</p><ul><li><p>zero-shot：泛化到未见的类、音频。</p><pre class="line-numbers language-none"><code class="language-none">This is a sound of [class label]this person is feeling [class label]keyword # Keyword Spotting[0-10] persons speaking<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>有监督特征提取：CLAP audio 表示 + 1~3 全连接层。</p></li><li><p>有监督微调：微调 CLAP audio encoder + 1~3 全连接层。</p></li></ul></li><li><p>结果</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB742a4690fa66e99219cadba9cbe3aec7?method=download&shareKey=1ce8de8b894f5b166cc983dc6519416f" width="727"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBee1e8d11c7a3ebb67e5af3d1f5d98f08?method=download&shareKey=be36b9b1bb21391b42318fe6dfd2293c" width="672"></p><p>*由于训练集中语音及其内容、描述较少，相较于声音事件等类型的任务，在情感识别、关键词检测等任务上性能略差。</p></li></ul><h2 id="消融实验">2.5. 消融实验</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa56e1c5a0194d2a3f1c2071670187f96?method=download&shareKey=58dd5b75f84df7b58878a859ab2e1f6e" width="350"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1541acd954f90f041b136b845fd22c7f?method=download&shareKey=b4923629cc8cdcb6d8ee66be4a1152dd" width="308"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1a086c3a86a1d9861d17b71cd688b7e7?method=download&shareKey=8b381473ddabfe08c7c142d6fcfc7dbc" width="307"></p><p><font color="red">* 对比学习中，更大的 batch size可以提高性能。</font></p><h1 id="其它">3. 其它</h1><ul><li>论文：Elizalde B, Deshmukh S, Al Ismail M, et al. Clap learningaudio concepts from natural language supervision[C]//ICASSP 2023-2023IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 2023: 1-5.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 多模态表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>IMAGEBIND</title>
      <link href="/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/imagebind/"/>
      <url>/blog/llm/duo-mo-tai/duo-mo-tai-biao-shi/lun-wen/imagebind/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：Meta AI</li><li>发表时间：2023.05</li><li>核心思想<ul><li><font color="red">仅采用与图像自然配对的无监督数据（除图文数据外）、对比loss，训练6种模态：视觉、文本、音频、深度、thermal、Inertial MeasurementUnit (IMU) 的联合 embedding空间。不需要所有模态对齐的数据，实现所有模态的 embedding对齐。</font></li><li>可用于跨模态检索、embedding 空间的运算、跨模态生成。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB0e90234c1223db0db92f3052da4e0d36?method=download&shareKey=72fc89f3f35bbdfa2244e9cecbef890d" width="864"></li></ul><h1 id="数据">1. 数据</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0f815d7fdffa6f6717be693fe7943d5c?method=download&shareKey=4e67cc58729c354526a802e70802618b" width="854"></p><table><thead><tr><th>模态</th><th>数据集</th></tr></thead><tbody><tr><td>网络图文</td><td></td></tr><tr><td>视频、音频</td><td>Audioset</td></tr><tr><td>图像、深度</td><td>SUN RGB-D</td></tr><tr><td>图像、thermal</td><td>LLVIP</td></tr><tr><td>图像、IMU</td><td>Ego4D</td></tr></tbody></table><p>*由于 SUN RGB-D、LLVIP 数据集较小，复制 50 倍。</p><p>*IMU 数据：egocentric cameras video with IMU。</p><h1 id="训练">2. 训练</h1><h2 id="对比学习-loss">2.1. 对比学习 loss</h2><p>InfoNCE[54] loss <span class="math display">\[L_{\mathcal{I},\mathcal{M}}=-\log \frac{\exp \left(\mathbf{q}_i^{\top} \mathbf{k}_i /\tau\right)}{\exp \left(\mathbf{q}_i^{\top} \mathbf{k}_i /\tau\right)+\sum_{j \neq i} \exp \left(\mathbf{q}_i^{\top} \mathbf{k}_j/ \tau\right)}\]</span></p><p>其中，<span class="math inline">\(\mathbf{q}_i\)</span>, <span class="math inline">\(\mathbf{k}_i\)</span> 为成对的 image-x 对经过encoders 得到的 embedding 表示，<span class="math inline">\(\tau\)</span> 为温度标量，将 mini-batch 内所有<span class="math inline">\({j \neq i}\)</span>的样本视为负样本。实际使用对称 loss <span class="math inline">\(L_{\mathcal{I}, \mathcal{M}}+L_{\mathcal{M},\mathcal{I}}\)</span>。采用上述 loss 优化 encoders。</p><p><font color="red">由于图像 encoder 冻结，对比仅采用 L2 loss、L2+对比loss、仅采用对比 loss，仅采用对比 loss 性能最好。</font></p><h2 id="encoder">2.2. encoder</h2><ul><li>所有模态 encoder 均采用 Transformer 结构。可用预训练模型初始化。<ul><li>图像：ViT [13]。</li><li>视频：与图像采用同一 encoder [20]，<font color="green">temporally膨胀[7] ViT 的 patch 投影层，采样 2帧/2s。</font></li><li>音频：参照 [22]，mel 谱，ViT-B，patch size=16, stride=10。</li><li>thermal、深度：视为单通道图像，ViT-S。<font color="green">遵循 [21]将深度图转为 disparity maps 以实现尺度不变性。</font></li><li>IMU：加速度计、陀螺仪在 X、Y、Z 轴上的测量值，2000/5s，1D 卷积 +Transformer。</li><li>文本：CLIP [60]。</li></ul></li><li>各模态特定的 encoder（除视频外） + 线性层，以获得相同维度的embedding。<font color="green">归一化。</font></li><li>采用 OpenCLIP 预训练的视觉、文本 encoders (ViT-H 630M params、302Mparams)，在 billions 图文对上训练。</li><li>视觉、文本 encoder 冻结，音频、thermal、深度、IMU encoder更新。</li></ul><h2 id="超参数">2.3. 超参数</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdea9f29d26f052d20f47743703b4ff20?method=download&shareKey=c3741ca4e2558f71fdab65f132bed27f" width="399"></p><ul><li>batch size：每个 minibatch 采样一对模态。通常对比 loss 需要更大的batch size，但不会随模态数增加而增加。</li></ul><h1 id="评价">3. 评价</h1><p><a href="https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb">prompt</a></p><h2 id="测试集">3.1. 测试集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2aa36cc811dec36c64bc49f184f917ee?method=download&shareKey=6a3db6e37dc52f1d3b090463094da18e" width="409"></p><ul><li>图像：ImageNet (IN1K)、Places-365 (P365)</li><li>视频：Kinetics400 (K400)、MSR-VTT 1k-A (MSR-VTT)</li></ul><h2 id="结果">3.2. 结果</h2><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>实验</th><th>结果</th></tr></thead><tbody><tr><td>emergent zero-shot 分类</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe7bfc86133ff4630d0534f2173c0d5ea?method=download&shareKey=3d45f2d937c172ad39a4648c477c14e3" width="849"></td></tr><tr><td>zero-shot text-to-audio 检索、分类</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6ff724a98907131daede9d9b1eba70f2?method=download&shareKey=a5dee7cebe9020031be19bc1fde93608" width="409"></td></tr><tr><td>text-to-audio+video 检索、分类<br>A+V: 0.95 video + 0.05 audioembedding</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4f0a8da2b6fb1c43404cd5740b2b314a?method=download&shareKey=295122a6facf1c9e8ce3e1063986c659" width="410"></td></tr><tr><td>few-shot 分类<br>各类分别采样 k 个样本，冻结encoder，训练线性分类器</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc25b667a5f6c4610e7d0e241cfc41fbd?method=download&shareKey=780cb00b27a490c90e52ef6500b67fd9" width="409"></td></tr><tr><td>embedding 算术<br> embedding：L2 归一化、乘以 0.5、相加</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc7ef5a75175d858ec8c3d0f63129e98d?method=download&shareKey=ab8faaaec4257f85e1f2e33edc7afd9a" width="413"></td></tr><tr><td>不重训，将基于文本 CLIP embedding 的模型升级为采用 IMAGEBIND audioembedding</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7d01cec0ee25d2dcf3231405dd0ade68?method=download&shareKey=4eaaf897209caf473d79fa82665db9b5" width="411"><br>基于文本的检测模型Detic<br><br>见图1<br>基于文本的 diffusion 模型 DALLE-2</td></tr></tbody></table><h2 id="消融实验">3.3. 消融实验</h2><p>encoder 具体配置详见 Sec.5。</p><ul><li><p>缩放图像 encoder 的大小</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd1e9d979925bc4b34aaa99d5845d839b?method=download&shareKey=35a41b9bcca611f8ed2ac79ea4d013b0" width="410"></p><ul><li>随着视觉 encoder 性能提升，其它非视觉模态的 emergent zero-shot分类性能提升。</li></ul></li><li><p>研究不同的训练设计对深度图（视觉+空间）、音频（非视觉+时序）zero-shot 分类性能的影响，以验证方案的鲁棒性、可迁移性。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB194ab5bdcf9329eb20524b3a3515b29b?method=download&shareKey=84089f66b832c7778bb9f931925f7acb" width="865"></p><ul><li><p>(a) 对比 loss 温度（参照 [60]）</p><ul><li>相较于可学习的温度（初始值 0.07），固定温度更好；</li><li>对于深度、thermal、IMU，较大的温度值较好；对于音频，较小的温度值较好。</li></ul></li><li><p>(b) encoder 投影层</p><ul><li><p>相较于 MLP，线性层更好。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4ad0c74d4b7386783b4da9a0505cca02?method=download&shareKey=bff30ce4dfe5c9c4ea5e2dbcfcb946d6" width="786"></p></li></ul></li><li><p>(c) 训练 epoch</p><ul><li>随 epoch 增加性能持续提升。</li></ul></li><li><p>(d) 图像数据增强</p><ul><li>basic：裁剪、颜色抖动；strong：+ RandAugment、RandErase。</li><li>对于数据集较小的深度数据，强增强有助于性能提升；<br>对于音频，basic增强有助于性能提升，强增强性能下降。</li></ul></li><li><p>(e)对于图像-深度对，相较于使用空间对齐的随机裁剪，使用非对齐的随机裁剪显著降低了性能。</p></li><li><p>(f) 对于图像-深度对，RandomErase 数据增强提升了性能。</p></li><li><p>(g) 对于视频-音频对，使用时间对齐的样本性能更好。</p></li><li><p>(h) 对音频进行频率 mask 数据增强，有少量的性能提升。</p></li><li><p>音频、深度 encoder 容量大小</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4a98f8e9538e033f33e12f7e986dfdbc?method=download&shareKey=851bd36cf1743a9c4b1f3d6a6d2236f6" width="410"></p><ul><li>大音频 encoder + 大图像 encoder。</li><li>小深度 encoder + 大图像 encoder，由于深度数据集较小。</li></ul></li><li><p>batch size</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb83454ff98253ada754494175b28ab7b?method=download&shareKey=a2269e04239e4fe72db9a5a8ada620ca" width="409"></p><ul><li><font color="red">根据数据集大小、复杂度，不同模态的最优 batch size不同。</font></li></ul></li></ul></li><li><p>采用 IMAGEBIND 评价预训练视觉模型用于多模态任务的性能。</p><ul><li><p>采用不同的预训练视觉 encoder，冻结，使用图像配对数据训练，测试zero-shot 分类性能。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5e9f276d172e6b92d97504aefd308956?method=download&shareKey=bbfb9c7d4dd5a59b6811329ebc40c887" width="419"></p></li><li><p>有监督模型 DeiT 在图像分类任务上更好，自监督模型 DINO在深度、音频分类任务上更好。</p></li></ul></li></ul><h1 id="其它">4. 其它</h1><ul><li>Girdhar R, El-Nouby A, Liu Z, et al. Imagebind: One embedding spaceto bind them all[C]//Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2023: 15180-15190.</li><li><a href="https://github.com/facebookresearch/ImageBind">PyTorch实现、checkpoint</a></li><li><a href="https://imagebind.metademolab.com/">Demo</a></li><li>展望：使用其它模态对齐数据。</li><li>局限性：学习到的联合 embedding 仅限于数据集中存在的概念，如thermal、深度数据集分别局限于室外街道、室内场景。</li><li>伦理：学习到的 embedding 可能产生 unintentional 的语义关联。</li><li>实际应用：如文本查询 IMU 数据（如手机、AR/VR 耳机、health trackers等中）检索 healthcare/活动，如图7。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 多模态表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Seed-TTS</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/seed-tts/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/seed-tts/</url>
      
        <content type="html"><![CDATA[<style>    .red-text {        color: red;    }    .green-text {        color: green;    }</style><ul><li>作者：字节</li><li>发表时间：2024.06</li><li>支持英语、中文普通话语音合成，支持跨语种合成（提示语音、合成文本语种不同）。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBda21189ef8f727fd938b84b36dffdc7f?method=download&shareKey=a0dcc15498377487b7fca73360374a88" width="631"></p><p><font color="red">"We investigate both continuous and discrete speechtokenizers, and found that the design of the tokenizer is crucial to theperformance of the entire system."（没公布结论）</font></p><h1 id="训练">2. 训练</h1><p>3阶段</p><ul><li>预训练</li><li>微调<ul><li><a href="#说话人微调">说话人微调</a>：增强指定说话人s的性能。</li><li><a href="#指令微调">指令微调</a>：提高可控性和交互性，如表现力、语速、风格、情感等。</li></ul></li><li>偏好训练：<a href="#强化学习">强化学习</a>，增强鲁棒性、说话人相似度、可控性。</li></ul><h1 id="评价">3. 评价</h1><ul><li>客观测试集：1000 Common Voice + 2000 DiDiSpeech（中文）。</li><li>主观测试集：100 内部数据，包含不同口音、方言、情感、说话风格。</li><li><font color="red">参考语音：3</font>-20s。</li><li>评价指标<ul><li>客观<ul><li>WER：EN: Whisper-large-v3，ZH: Paraformer-zh。</li><li>说话人相似度：在说话人 verification 任务上微调WavLM-large，计算说话人 embedding 的余弦相似度。</li></ul></li><li>主观<ul><li><p>CMOS：提供参考语音、合成语音、groundtruth（<font color="red">后两者随机顺序</font>），评价说话人相似度、表现力。</p><p>[-2,2]，经验上 <span class="math inline">\(|CMOS|&lt;0.1\)</span>表明无显著偏好。</p></li></ul></li></ul></li></ul><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>实验目的</th><th>实验设置</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>zero-shot in-context learning (ICL)</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcfceec284eab4d8d829a6b596f29b8dc?method=download&shareKey=6e435f05c0a44ef0895a295f60f6f25f" width="475"></td><td>相较于 ground truth，ICL合成语音与提示音频的说话人相似度更高。由于同一说话人的多个语音，说话风格、背景环境等仍有差异。<br><br>说话人相似度更高时，WER不一定更低。更低的 WER可能由于语音更“标准”，而牺牲了口音、表现力、自然度等。<br><br>ICL CMOS得分较低的样本，真实语音句间变化更多，合成语音保持了与参考语音一致的韵律，导致长时TTS 中韵律变化较少。可能可以通过 few-shot 解决。</td></tr><tr><td>15s 音频提示 zero-shot ICL vs FastSpeech 说话人微调</td><td>每个说话人分别微调得到各自独立的模型。训练集约 5h/人。<br><br>hard说话人如 electronic high-pitched chipmunk 虚拟角色。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBb4c37b35fedbfb8aa69843aa4c40847e?method=download&shareKey=fabb538422415d12be04a48ad7a63596" width="536"></td><td><strong>对于普通、日常风格的说话人，</strong> zero-shot ICL自然度、表现力更好；<br><br><strong>对于有强烈口音、说话风格夸张的说话人，</strong> zero-shot ICL没有忠实地保留口音、风格，特别是语音提示缺少代表性韵律时。可以通过更长的语音提示、增加训练数据覆盖优化。<br><br><font color="green">存疑：都基于Seed-TTS，zero-shot ICL vs 说话人微调性能差异？</font></td></tr><tr><td><font color="red">使用合成音频进行 ASR等语音理解任务训练的可行性</font></td><td><strong>合成数据集构建方式：</strong>对训练集中的每个句子，将其作为语音提示，合成训练集中随机选择的文本，保证所有音频、文本均被采样一次。<br><br>采用相同的模型结构、超参数，分别采用原始数据集、合成数据集训练ASR 模型。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc4cadc7cd12a64fe3fee94019f0154d5?method=download&shareKey=f2bda2fa009552140056f96fbd30fe94" width="528"></td><td>clean 数据集上性能接近，other 数据集上性能较差，可能由于 TTS合成语音背景噪声较少，通过数据增强可以减少性能差距。</td></tr><tr><td><span id="说话人微调">说话人微调</span></td><td>数据：5人，各 1-10h，合计 20h。<br>添加说话人 indextoken。<br>ICL：20s 提示。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB0d11ba531748de96f04bd25126c06131?method=download&shareKey=e11920979f24587c19f648872fd5d920" width="484"></td><td>CMOS 0.37，学习到了目标说话人更细微的韵律特点。</td></tr><tr><td><span id="指令微调">指令微调</span></td><td>对4类主要情感，分别合成100个对应情感的文本，采用 SER模型预测合成语音的情感。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB09d6f29b88b956835ace79870cd41ed0?method=download&shareKey=491c6b2bb0e5e2d4546653ab74c9a656" width="489"></td><td><span class="math inline">\(\text {Seed-TTS}_{\text{SFT}}\)</span>根据文本推断目标情感，有一定的准确性；<br><br> <span class="math inline">\(\text {Seed-TTS}_{\text{IFT}}\)</span>根据明确的指令，合成音频的情感准确度显著提高。</td></tr></tbody></table><p>局限性：唱歌；提示包含背景音乐、噪声时，生成的背景音不一致，如完全忽略音乐。</p><section id="低延迟流式推理" class="red-text"><h1>4. 低延迟、流式推理</h1><ul><li><p>流式推理</p><ul><li>causal diffusion</li></ul></li><li><p>减少 diffusion 模型计算量</p><ul><li><a href="https://arxiv.org/abs/2303.01469">consistencydistillation</a></li><li><a href="https://arxiv.org/abs/2403.03206">modified flowmatching</a></li></ul></li><li><p>LM 优化</p><p>grouped-query attention、paged attention、flashattention、模型量化</p><ul><li><a href="https://arxiv.org/abs/2106.08295">A White Paper on NeuralNetwork Quantization</a></li><li><a href="https://arxiv.org/abs/2404.12759">decoupleQ: Towards 2-bitPost-Training Uniform Quantization via decoupling Parameters intoInteger and Floating Points</a></li></ul></li><li><p>相关文献</p><ul><li><a href="https://arxiv.org/abs/2310.04378">Latent consistencymodels: Synthesizing high-resolution images with few-stepinference</a></li><li><a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-awareweight quantization for llm compression and acceleration</a></li></ul></li></ul></section><p><img src="https://note.youdao.com/yws/api/personal/file/WEB46b5da2839c94b892742b3f58baa8ee7?method=download&shareKey=8770eaddba4ed52615d6ecef2656149c" width="475"></p><h1 id="speech-factorization-by-self-distillation">5. Speech factorizationby self-distillation</h1><p>Speechfactorization：将语音分解为各种独立的属性，如音色、韵律、内容。</p><p>自蒸馏</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4e4688a187fd38ab244ea6264bdf2d6a?method=download&shareKey=cffac6e8f5a8647e5744bea9164a9cf9" width="509"></p><ul><li>创建语音对，大部分属性一致，指定的一个或多个目标属性不一致。</li><li>在 diffusion模型中引入<font color="green">说话人扰动，生成内容、韵律相同</font>，音色不同的成对语音。原始音色、扰动音色数据分别表示为<span class="math inline">\(S_{ori}\)</span>、<span class="math inline">\(S_{\text {alt}}\)</span>。</li><li>重训 diffusion 模型，输入从 <span class="math inline">\(S_{\text{alt}}\)</span> 提取的 token、从 <span class="math inline">\(S_{ori}\)</span> 提取的音色，训练目标为恢复从<span class="math inline">\(S_{ori}\)</span> 提取的 vocoderembedding。</li><li><font color="green">存疑：原始模型如何区分音色、内容表示？若已区分，是否还需要解耦？若未解耦，直接用输入的从<span class="math inline">\(S_{ori}\)</span>提取的表示进行合成即可。原始音色的表示是否应该来自内容、韵律不同的语音？</font></li></ul><p>应用：voice conversion。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa1ea584bd462e2bb485c2f58c46d1991?method=download&shareKey=5c678248ce13b3cbfb54c40fbb206052" width="526"></p><h1 id="强化学习">6. 强化学习</h1><ul><li><p>对比了</p><ul><li>使用外部奖励模型，如 Proximal PolicyOptimization、REINFORCE，允许对特定的语音属性更明确的控制。</li><li>不需要奖励模型，如 DPO，实现简单。</li></ul><p>两类方法都有效。本文报告 REINFORCE 的结果。</p></li><li><p>两个版本，采用不同的奖励函数，<span class="math inline">\(\text{Seed-TTS}_{\text{RL-SIM-WER}}\)</span>、<span class="math inline">\(\text {Seed-TTS}_{\text{SER}}\)</span>。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe43d6523b6ed5823aa103a4ad1a429ad?method=download&shareKey=17be3aad3218a11d63114498a93bd9fd" width="437"></p><p>* Hard: 包含对 AR 模型特别难的文本模式，如单词重复、绕口令等。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB51011895c3bf752bdfca801e730ae6fe?method=download&shareKey=5a8b74f13c58611829faeeb203bd7152" width="458"></p><p>* 观察到 RL 普遍存在的 reward hacking 问题，如为了更低的WER，倾向于生成更慢、更清晰、自然度差的语音。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa9873910d7b1aea03bfc0b1f1c8b0001?method=download&shareKey=8c82b582b90dc33fca8e21483f622277" width="529"></p></li></ul><h1 id="全-diffusion-模型">7. 全 diffusion 模型</h1><ul><li>对比 LM + diffusion 与全 diffusion 模型。<ul><li><p>视觉领域的对比 <a href="https://arxiv.org/abs/2310.05737">Language model beats diffusion -tokenizer is key to visual generation</a></p></li><li><p>全 diffusion模型：<font color="green">非流式、支持内容编辑</font>。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbddb8a6d352ddff70eef01773ec2d82c?method=download&shareKey=564abcc56bdb070215a1a672f5bb0410" width="534"></p></li><li><p>LM：<font color="green">流式处理</font>、可集成文本 LM。</p></li></ul></li><li>时长预测<ul><li>经验：添加额外的时长预测模型，合成语音的自然度降低。</li><li>采用 diffusion模型直接预测语音的总时长。由于可以动态调整每个音素的时长，合成语音的自然度很高。</li><li><strong>本文</strong>：直接向模型提供总时长。diffusion模型输入提示语音、文本、具有目标语音总时长的高斯噪声片段。使其可用于语速编辑。</li></ul></li></ul><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>任务</th><th>实验设置</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>zero-shot TTS</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc066b521e1c98363825eaa2ecb106e2a?method=download&shareKey=32bcc4d09d78f21f2cdb60fdecc6d09e" width="527"></td><td>说话人相似度更好；WER 相当。</td></tr><tr><td>内容编辑</td><td>mask 一定比例的音频，根据文本恢复音频。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB25b63d743a79131d003dc48144772fa6?method=download&shareKey=a2785f9b7a41ca747719ef3735f8f812" width="470"></td><td></td></tr><tr><td>语速编辑</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBefad31e47ce6599dd41a6f658d73baa3?method=download&shareKey=6572a5d5f1888909d4e09af676df7846" width="531"></td><td>模型可以仅根据总时长自动调整语速。<font color="green">合成时长与指定时长的偏差？</font><br><br>拉伸总时长时，相较于传统方法统一改变句子整体语速，模型会根据文本自动在适当的时刻插入静音、拉伸某些元音的发音，整体语速在自然范围内。因而自然度、说话人相似度更高。</td></tr></tbody></table><h1 id="其它">8. 其它</h1><ul><li><a href="https://bytedancespeech.github.io/seedtts_tech_report">Demo</a></li><li><a href="https://github.com/BytedanceSpeech/seed-tts-eval">测试集、测试代码</a></li><li>稳定性优化：token、模型设计、训练推理策略、数据增强、强化学习。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PMG</title>
      <link href="/blog/llm/tiao-jian-sheng-cheng/lun-wen/pmg/"/>
      <url>/blog/llm/tiao-jian-sheng-cheng/lun-wen/pmg/</url>
      
        <content type="html"><![CDATA[<ul><li><p>论文：Shen X, Zhang R, Zhao X, et al. PMG: PersonalizedMultimodal Generation with Large Language Models[C]//Proceedings of theACM on Web Conference 2024. 2024: 3833-3843.</p></li><li><p>作者：Huawei Noah's Ark Lab 等。</p></li><li><p>核心思想</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe6e00603cdbba119101bfb29a81e6318?method=download&shareKey=a66e5012eb353b0a2aecbb495b5c8dce" width="655"><img src="https://note.youdao.com/yws/api/personal/file/WEB08b907e759dbc4b43534ead8dd272e4a?method=download&shareKey=8a0eb577140cc1f04f5623d225f8db34" width="254"></p><ul><li><p>预处理用户历史行为</p><p><span class="math display">\[ x_i=\left[LLM_g\left(t_{h_i}\right),LLM_g\left(v_{h_i}\right), \cdots\right] \]</span> <span class="math display">\[ y_i=\left[LLM_g\left(t_{c_i}\right),LLM_g\left(v_{c_i}\right), \cdots\right] \]</span> <span class="math inline">\(H=\left\{h_1, h_2, \cdots\right\}\)</span>表示用户历史点击的 items，<span class="math inline">\(C=\left\{c_1, c_2,\cdots\right\}\)</span> 表示历史对话， <span class="math inline">\(t\)</span>、<span class="math inline">\(v\)</span>…… 分别表示文本、视觉等多模态数据，LLM 为字幕模型或多模态模型，<span class="math inline">\(x\)</span>、<span class="math inline">\(y\)</span>为对应的文本 summary。</p></li><li><p>生成用户偏好的文本关键词</p><p>为每个场景人工设计</p><ul><li>指令 <span class="math inline">\(p\)</span>: user preferenceextraction。</li><li>属性 <span class="math inline">\(a_i\)</span>：如衣服的颜色、材质、形状；电影的类型、导演、起源等。</li><li>示例 <span class="math inline">\(e\)</span>：指定输出格式和示例关键词（如可爱、卡通等）。</li></ul><p>采用冻结的 LLM 回答关于每个属性的用户偏好。类似的，生成目标 item关键词。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0305f1b526cbbe8b027226b52cc0aee8?method=download&shareKey=74b6d021cb0a92a21128c36c0885a398" width="602"></p></li><li><p>生成 Soft Preference Embeddings</p><ul><li><font color="green">添加多模态 token，采用线性层对齐 LLM embedding和 generator 的文本编码器 embedding（参考[18]）。</font></li><li>采用 <font color="green">P-Tuning V2[20]</font> 微调 LLM，添加 <span class="math inline">\(S\)</span> 个可训练的前缀 embedding <span class="math inline">\(\mathbf{t}=\left\{t_1, \cdots,t_S\right\}\)</span>。<font color="green">训练方式见 Sec.3.3.2</font>。</li><li>将 <span class="math inline">\(p\)</span>、<span class="math inline">\(x\)</span>、<span class="math inline">\(y\)</span>、多模态 tokens <span class="math inline">\(\mathbf{m}\)</span> 输入 LLM 得到 embedding <span class="math inline">\(\mathbf{E}_m\)</span>。 <span class="math display">\[ \text {prompt}=(p, \mathbf{x},\mathbf{y})\]</span> <span class="math display">\[\left[\mathbf{E}_{\text {prompt}},\mathbf{E}_m\right]=LLM_f(\mathbf{t}, \text {prompt}, \mathbf{m})\]</span></li><li>用于 multimodal bias correction。</li></ul></li><li><p>权衡目标准确度、偏好</p><p><span class="math display">\[M=\operatorname{Generator}\left(w_p\cdot \mathbf{E}^p, w_t \cdot \mathbf{E}^t\right)\]</span> <span class="math display">\[ d_p =\frac{e_M \cdote_p}{\left\|e_M\right\|_2\left\|e_p\right\|_2} \]</span> <span class="math display">\[ d_t =\frac{e_M \cdote_t}{\left\|e_M\right\|_2\left\|e_t\right\|_2} \]</span> <span class="math display">\[z=\alpha \cdot \log d_p+(1-\alpha) \cdot \log d_t\]</span></p><p>通常 <span class="math inline">\(\alpha\)</span> 取0.5。采用多个预定义的 <span class="math inline">\(w_p\)</span>、<span class="math inline">\(w_t\)</span> 生成，选取使得分 <span class="math inline">\(z\)</span> 最高的。</p></li></ul></li><li><p>实验：根据用户的历史点击产品生成衣服图像，根据用户观看过的电影生成个性化海报，根据用户历史使用的表情图、历史对话和当前对话生成表情图。</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 条件生成 </category>
          
          <category> 论文 </category>
          
          <category> 泛读 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WenetSpeech</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/wenetspeech/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/wenetspeech/</url>
      
        <content type="html"><![CDATA[<ul><li><p>作者：西北工业大学、出门问问等，2022。</p></li><li><p>ASR数据集，中文普通话，多领域，万小时级。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB09871d16b5b5fc110c09bbb1cc830075?method=download&shareKey=6e07304e68491daa326588c4417251af" width="230"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe8c8b7652f5d4e1a3d2a16c5c3ec6b43?method=download&shareKey=55218c8ba22ad1233c81f461e6132ca7" width="348"></p><p>音频格式：16kHz，16bit，单声道，比特率 32 kbps，Opus 压缩。</p><p>S(100h), M(1000h) 训练子集置信度为1.0。</p></li><li><p>音频采集</p><ul><li>定义类别：audiobook, commentary, documentary, drama, interview,news, reading, talk, variety, others。</li></ul></li><li><p>label</p><ul><li>Youtube 视频：OCR 提取字幕<ul><li>对视频中的每一帧图像应用基于 CTPN[29] 的文本检测。</li><li>逐帧计算<font color="red">字幕区域</font>的结构相似度（SSIM），确定字幕片段的起、止时间。</li><li>采用基于 CRNN-CTC 的 OCR 识别字幕区域。</li><li>合并连续的片段，直到音频超过8s，以减少音频-字幕异步问题。</li></ul></li><li>Podcast：第三方商业ASR引擎转写（含VAD）</li><li>Dev(20h)、Test_Net(23h，match，还涵盖游戏解说、电商直播等热门、难点领域)、Test_Meeting(15h，mismatch，远场)：人工转写</li></ul></li><li><p>过滤</p><ul><li><p>强制对齐构图<img src="https://note.youdao.com/yws/api/personal/file/WEB21de03bb237f4254650d983fa0223f0e?method=download&shareKey=0fc562b98825ed627ff43a10db712144" width="467"></p><ul><li><font color="red">漏读惩罚 2.3（p=0.1）</font></li><li>filler 状态，用于支持插入、替换，各 CTC 建模单元的自旋惩罚4.6（p=0.01）</li><li><a href="https:github.com/wenet-e2e/wenet/blob/main/runtime/core/bin/label_checker_main.cc" class="uri">https:github.com/wenet-e2e/wenet/blob/main/runtime/core/bin/label_checker_main.cc</a></li><li><font color="red">存在的问题：filler_end与每个单词相连，易出现长距离漏读、跳跃</font></li></ul></li><li><p>置信度</p><p><span class="math display">\[c=1-\frac{\operatorname{EditDistance}(\text { ref }, \text { hyp})}{\max (\operatorname{len}(\text { ref }), \operatorname{len}(hyp))}\]</span></p></li></ul></li><li><p>text normalization: <a href="https://github.com/speechio/chinese_text_normalization" class="uri">https://github.com/speechio/chinese_text_normalization</a></p></li><li><p><a href="https://github.com/wenet-e2e/WenetSpeech" class="uri">https://github.com/wenet-e2e/WenetSpeech</a></p></li><li><p>Zhang B, Lv H, Guo P, et al. Wenetspeech: A 10000+ hoursmulti-domain mandarin corpus for speech recognition[C]//ICASSP 2022-2022IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 2022: 6182-6186.</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GigaSpeech</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/gigaspeech/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/gigaspeech/</url>
      
        <content type="html"><![CDATA[<h1 id="特点">1. 特点</h1><ul><li>英文，1wh 高质量标注数据，总共 4wh 音频</li><li>可扩展，记录的元数据使其还可用于其它任务，如说话人识别</li><li>包含原始转录与Normalized转录（包含标点符号，大小写、日期、时间normalization等）</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa7fad614ff99cd2d545715d908a8dea4?method=download&shareKey=8a6e7521cb75ad12ad21d5ddc756372a" width="300"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB95143dfb41ac6dc13714c70986e567e3?method=download&shareKey=195f09710a7b3356ad610b7caaafc1d9" width="210"></p><p>数据集中不包含Librispeech dev-*test-*，因此它们也可以作为开发、测试集</p><h1 id="音频格式">2. 音频格式</h1><ul><li>16kHz, 16bit, 单声道</li><li>压缩：Opus格式，压缩比为8，比特率 32 kbps。压缩的影响：WER 0.1-0.2%的损失<img src="https://note.youdao.com/yws/api/personal/file/WEB301b29a3f6d8c45a9748f0a6ec4b7296?method=download&shareKey=9d159edb144886027b1260aa3e2298ab" width="281"></li></ul><h1 id="创建流程">3. 创建流程</h1><ul><li>定义主题：24类，分别为Arts, Business, Education, Autos and Vehicles,Comedy, Crime, Entertainment, Film and Animation, Gaming, Health andFitness, History, Howto and Style, Kids and Family, Leisure, Music, Newsand Politics, Nonprofits and Activism, People and Blogs, Pets andAnimals, Religion and Spirituality, Science and Technology, Society andCulture, Sports, Travel and Events.<ul><li>播客：上述类别，带人工转写；Youtube将上述类别作为<font color="green">种子关键词</font>，带人工字幕。有声读物：不限定类别。</li></ul></li><li>文本normalization：大小写，数字到单词重写，日期/时间重写，删除特殊符号，可用于含normalization 的端到端ASR。</li><li><font color="red">识别+文本匹配：Youtube字幕中的时间戳不够准确。kaldi 对齐。用于获取句子级片段。<ul><li>音频和文本分别均匀地分为小片段；</li><li>用biased LM解码，生成带时间戳的识别结果；</li><li>根据TF-IDF相似性匹配识别结果片段与转写片段；</li><li>采用修改的Smith-Waterman算法进一步对齐，并处理静音和标点符号；</li><li>先采用close-domain AM 进行对齐与segment，然后用约3000h上述数据训练领域内AM，用于对齐整个数据集。</li></ul></font></li><li>segment规则<ul><li>超过1s的静音处</li><li>超过0.2s静音的标点符号（,.?!）处</li><li>删除WER &gt;= 75% 的片段</li><li>删除长度 &gt;= 20s 的片段</li><li>片段边界处的静音保留0.15s</li><li>,.?! 分别用 &lt;COMMA&gt;, &lt;PERIOD&gt;, &lt;QUESTIONMARK&gt;,&lt;EXCLAMATIONMARK&gt; 表示</li></ul></li><li>过滤<ul><li>强制对齐构图<img src="https://note.youdao.com/yws/api/personal/file/WEB951ba23cbefa17c329b5353f0611649e?method=download&shareKey=a04c09454c654eb908a590f2b41a154f" width="350"></li></ul>podcasts, YouTube 部分 WER限定为4%，audiobooks和小的训练子集WER限定在0%</li><li>修正 label<ul><li>filler loop<ul><li>fillers，如 AH, UH, UM, ER, ERR, YOU KNOW, I MEAN, SORT OF 等</li><li>连词，如 AND, OR, BUT 等</li></ul></li><li>disfluency detector<ul><li>如 It's it's it's a great thing!</li></ul></li><li>对于XL子集，符合上述类型的插入不计入WER</li></ul></li><li>选取working point<ul><li><p>选取 working point，使得各子集满足 WER 约束</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcf45e4dd3e789232c8c5ffdb605f96c1?method=download&shareKey=e284e0d5b94f7e2ac015dab8f91d793c" width="355"></p></li></ul></li><li><a href="https://github.com/SpeechColab/GigaSpeech" class="uri">https://github.com/SpeechColab/GigaSpeech</a></li><li>Chen G, Chai S, Wang G, et al. GigaSpeech: An evolving, multi-domainASR corpus with 10,000 hours of transcribed audio[C]//22nd AnnualConference of the International Speech Communication Association,INTERSPEECH 2021. International Speech Communication Association, 2021:4376-4380.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LibriSpeech</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/librispeech/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/shu-ju-ji/librispeech/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：陈果果、Daniel Povey 等，The Johns HopkinsUniversity，2015。</li><li>1000h，英语（口音接近美音），朗读，原始数据来自 LibriVox项目的audiobooks。音频：16kHz。<img src="https://note.youdao.com/yws/api/personal/file/WEBd8b20e97d041a654467f6d1f413ff011?method=download&shareKey=7c647fdd6723bd9aaf62cdec107210fb" width="387"></li><li>text normalization：转大写，删标点符号，扩展缩写、非标准单词。</li><li>词典：CMUdict，删除重音标记，OOV 词 G2P 生成发音。</li><li>识别 + 文本匹配 + 分段<ul><li>识别<ul><li>AM：kaldi gmm-decode-faster，约 100h 域内数据训练。</li><li>LM：本书文本训练 2gram，Witten-Bell 平滑。</li></ul></li><li><font color="red">识别结果、参考文本匹配</font>：Smith-Waterman对齐算法。相较于 Levenshtein对齐，该算法不要求消费完整的参考或识别文本，各种错误类型的权重可调。保留最大的匹配片段，丢弃多余的。</li><li><font color="red">置信区间：在对齐区域内，若识别结果与参考文本完全匹配的长度超过12个音素，则视为置信区间。</font></li><li>segment：<font color="green">采用DP算法</font>，将音频分割为 &lt;=35s 的片段，仅允许在置信区间内、至少0.5s的静音处分割。</li></ul></li><li>强制对齐 + 过滤<ul><li><p>错误来源：增、删、插入、换位、不流畅、TN 错误、G2P错误等。</p></li><li><p>AM：说话人自适应。</p></li><li><p><font color="red">强制对齐构图</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB856b4642f5b67712976cfcb5a448193c?method=download&shareKey=a9399af51dc78a98a82f641be5904626" width="485"></p></li><li><p>修改解码器，从单词 x 处进入 bigram 部分后，仅能返回 x（插入）或x+1 （替换）处。若拷贝多个 bigram 副本，解码图很大。</p></li><li><p>相较于单音素的 filler 模型，音素级 bigram效果更好。</p></li></ul></li><li>segment<ul><li>训练集：超过0.3s的静音处分割。</li><li>开发集、测试集：仅上述静音段与参考文本中的句子边界重合时分割。</li></ul></li><li>后处理<ul><li><font color="red">多说话人：采用 LIUM speaker diarization工具，过滤多说话人的片段。</font></li><li>丢弃有严重音质问题的数据。</li><li>说话人性别均衡。</li></ul></li><li>LM：LM 训练集 与开发集、测试集中的文本不重叠。文本过滤策略详见论文chapter 4。</li><li>实验：在 WSJ 测试集上，相较于用 WSJ 训练集（82h）训练的模型，在Librispeech上训练的模型识别效果更好，说明数据量的提升超过了音频不匹配的影响。</li><li>Panayotov V, Chen G, Povey D, et al. Librispeech: an asr corpusbased on public domain audio books[C]//2015 IEEE internationalconference on acoustics, speech and signal processing (ICASSP). IEEE,2015: 5206-5210.</li><li><a href="http://www.openslr.org/12/" class="uri">http://www.openslr.org/12/</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Fish Speech</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/fish-speech/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/fish-speech/</url>
      
        <content type="html"><![CDATA[<ul><li><p>开发者：Fish Audio Fish Speech</p></li><li><p><a href="https://huggingface.co/blog/lengyue233/fish-speech-1">系统结构V1</a><img src="https://speech.fish.audio/assets/figs/diagram.png" width="1111"></p></li><li><p>VQGAN + LLAMA。30wh 数据训练。在高质量数据集上 SFT。</p><blockquote><p>存疑：更新日志中，V1.1 引入了 VITS Decoder 来降低WER、提高音色相似度；V1.2 删除了 VITS解码器，极大地增强了零样本能力。</p></blockquote></li><li><p>推理</p><ul><li>使用 VQGAN 对给定的约 10 秒语音进行编码。</li><li>将编码后的语义标记和对应的文本作为示例输入到语言模型中。</li><li>给定一段新的文本，让模型生成相应的语义token。</li><li>将生成的语义token输入到VITS/VQGAN中进行解码并生成相应的语音。</li><li><font color="green">HTTP API 支持流式返回。</font></li></ul></li><li><p>支持音色克隆，最短 10s 参考音频。支持说话人、情绪音频-文本对提示。</p></li><li><p>支持中文、日语、英语。</p></li><li><p><a href="https://huggingface.co/blog/lengyue233/fish-speech-1">推理速度：V1、4090GPU, 20 tokens/s。</a></p></li><li><p>扩大模型容量、增大训练数据量，性能显著提升。</p></li><li><p><a href="https://speech.fish.audio/en/finetune/">LoRA微调代码</a></p></li><li><p><a href="https://huggingface.co/fishaudio/fish-speech-1.2">checkpoint</a></p></li><li><p><a href="https://fish.audio/zh-CN/">Demo</a></p></li><li><p><a href="https://speech.fish.audio/samples/">示例</a></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ChatTTS</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/chattts/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/lun-wen-chan-pin/chattts/</url>
      
        <content type="html"><![CDATA[<ul><li><p>开发者：2noise</p></li><li><p>推出时间：2024.05</p></li><li><p><font color="green">TN (homophone 替换) + 文本口语化(stream_batch，text2text) + GPT + GroupedResidualVQ + VAE (mel) + vocos(wav)</font></p></li><li><p>针对 LLM助手对话场景设计，训练集包含大量口语内容，支持对文本进行口语化处理。</p></li><li><p>训练数据 10wh+，SFT。开源版本仅 4wh，w/oSFT，通过加少量高频噪声、采用 MP3 格式压缩了音质。</p></li><li><p><font color="green">细粒度控制，支持控制语速、音色相关度，支持笑声、停顿、感叹词。</font><img src="https://github.com/libukai/Awesome-ChatTTS/raw/main/readme/WebUI-CN.png" width="720"></p><pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">params_infer_code &#x3D; ChatTTS.Chat.InferCodeParams(    prompt&#x3D;&quot;[speed_5]&quot;,    temperature&#x3D;0.3,    spk_emb&#x3D;rand_spk,)# For sentence level manual control.# use oral_(0-9), laugh_(0-2), break_(0-7)# to generate special token in text to synthesize.params_refine_text &#x3D; ChatTTS.Chat.RefineTextParams(    prompt&#x3D;&#39;[oral_2][laugh_0][break_6]&#39;,)# For word level manual control.# 目前仅支持 [laugh]、[uv_break]和[lbreak]text &#x3D; &#39;What is [uv_break]your favorite english food?[laugh][lbreak]&#39;wavs &#x3D; chat.infer(text, skip_refine_text&#x3D;True, params_refine_text&#x3D;params_refine_text,  params_infer_code&#x3D;params_infer_code)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p><font color="green">支持流式生成。</font></p></li><li><p>支持中文、英语、中英混合。</p></li><li><p>4090 GPU RTF 约为 0.3。</p></li><li><p>稳定性不好，存在多说话人、音质差的问题，自回归模型 (for bark andvalle) 普遍存在。合成质量可能受文本复杂度、长度影响。</p></li><li><p><a href="https://github.com/2noise/ChatTTS/tree/main">官方仓库</a></p></li><li><p><a href="https://github.com/libukai/Awesome-ChatTTS">社区扩展项目</a></p><ul><li><a href="http://region-9.autodl.pro:41137/">音色克隆：提取说话人embedding</a></li><li><a href="https://huggingface.co/spaces/doby4u/chattts/raw/ab5e9864eba39bc14e74db3381026b42939369c5/ChatTTS/experimental/llm.py">LLM生成日常对话回复 + LLM TN + ChatTTS</a></li></ul></li><li><p><a href="https://www.bilibili.com/video/BV1zn4y1o7iV/">介绍1</a>、<a href="https://www.youtube.com/watch?v=L4klnZ5Lox8&amp;ab_channel=SamWitteveen">2</a></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
          <category> 论文&amp;产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LDM</title>
      <link href="/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/ldm/"/>
      <url>/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/ldm/</url>
      
        <content type="html"><![CDATA[<style>    .red-text {        color: red;    }    .green-text {        color: green;    }</style><div class="red-text"><ul><li><p>相较于原始图像/波形/频谱，autoencoder 学习 latent 表示，</p><ul><li>忽略了不可感知的细节，保留了必要的语义信息。对于 likelihood-basedmodels（如 diffusion 模型），由于其 mode-covering特性，倾向于用很大的模型容量建模数据中高频、难以感知的细节，导致训练耗时。</li><li>维度更低。用于 diffusion 模型时，diffusion模型训练（梯度计算）、采样（迭代生成）更高效。</li><li>autoencoder 和 diffusion模型独立，不需要权衡重建质量与生成能力。</li></ul></li><li><p>条件生成</p><p>添加 crossattention，允许一般的条件输入，如class-conditional、文本、layout、图像等。</p></li><li><p>评价</p><ul><li>训练、生成速度更快，同时有很高的视觉保真度。</li><li>SOTA：图像修复、class-conditional 图像合成。</li><li>可比：无条件图像生成、text-to-image、super-resolution。</li><li>限制<ul><li>diffusion 模型迭代生成慢。</li><li>相较于像素级输入，latent表示的下采样可能限制需要细粒度准确性的任务的性能。</li></ul></li></ul></li></ul></div><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7a4202bfa6956a8d00757efc2bee1caf?method=download&shareKey=f32b9c20426b7dcdfbf2ccc5c6a06809" width="547"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB801376f71a4ac3ea2bd7666f48d297ee?method=download&shareKey=7c27805809afe6f5370266929a291d10" width="412"></p><h2 id="autoencoder">1.1. autoencoder</h2><ul><li>基于[23]。</li><li>输入 RGB 图像 <span class="math inline">\(x \in \mathbb{R}^{H \timesW \times 3}\)</span>；通过 encoder <span class="math inline">\(\mathcal{E}\)</span> 得到 latent 表示 <span class="math inline">\(z \in \mathbb{R}^{h \times w \timesc}\)</span>，下采样系数为 <span class="math inline">\(f=H / h=W /w\)</span>；通过 decoder <span class="math inline">\(\mathcal{D}\)</span> 重建 <span class="math inline">\(\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))\)</span>。保留了空间结构。</li><li>latent 空间正则，避免高方差。<ul><li>KL-reg：与标准正态分布的 KL 散度。<ul><li>rescale：在第一个 batch 上计算 component-wise variance。 <span class="math display">\[\hat{\sigma}^2=\frac{1}{b c h w} \sum_{b, c, h,w}\left(z^{b, c, h, w}-\hat{\mu}\right)^2\]</span> <span class="math display">\[\hat{\mu}=\frac{1}{b c h w} \sum_{b, c, h, w}z^{b, c, h, w}\]</span> <span class="math display">\[z \leftarrow\frac{z}{\hat{\sigma}}=\frac{\mathcal{E}(x)}{\hat{\sigma}}\]</span></li></ul></li><li>VQ-reg：decoder 第一层加向量量化层，学习大小为 <span class="math inline">\(|\mathcal{Z}|\)</span> 的码本。<ul><li>VQ-reg 生成质量更好，虽然 autoencoder 重建能力差于连续版本。</li></ul></li><li>latent空间的方差显著影响<font color="green">卷积采样</font>生成高分辨率图像的结果（参见<font color="green">附录B</font>），采用KL-reg + rescale，VQ-reg 方差接近1。效果对比见图15。</li></ul></li><li>优化目标：感知损失 + patch-based对抗损失。强调局部真实性；并避免仅依赖像素空间的损失（如 <span class="math inline">\(L_2\)</span> 或 <span class="math inline">\(L_1\)</span>）导致的模糊。</li></ul><h2 id="diffusion-model">1.2. diffusion model</h2><p>UNet<img src="https://note.youdao.com/yws/api/personal/file/WEB05461f56e29e26ea9efa8145e143c6fc?method=download&shareKey=a231d0a86d642b07e9cd19e519defe40" width="709"></p><h2 id="条件生成">1.3. 条件生成</h2><p>各种模态的条件输入 <span class="math inline">\(y\)</span>，采用特定领域的 encoder 转为表示。与diffusion 模型通过 cross attention 层连接，作为K、V；或者与输入拼接。</p><ul><li>class-conditional，encoder 可以为 1个 embedding 层。</li><li>text-to-image、layout-to-image：<font color="green">unmasked</font>transformer。</li></ul><h1 id="实验">2. 实验</h1><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>autoencoder 下采样系数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB704dd239e20fd750999038bcfd0cad12?method=download&shareKey=3a43478d07160e845a7c3ec3d10f2d14" width="553"></td><td><span class="math inline">\(f=1,2\)</span> 收敛慢；<br><span class="math inline">\(f=32\)</span> 采样质量差，较少训练步后 FID不下降；<br><span class="math inline">\(f=4-16\)</span>达到较好的效率、质量的均衡。</td></tr><tr><td>autoencoder 下采样系数、DDIM 采样步数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7713ab9f5a106dee4d8e1d1d50d3850f?method=download&shareKey=63540953b4a827497849c737ccf33752 " width="557"></td><td><font color="red"><span class="math inline">\(f=4,8\)</span>采样质量较好，且达到较好质量所需的采样步数较少。</font><br>下采样系数越大，吞吐越快。</td></tr><tr><td>无条件生成</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcdb9f4563aa02fc87e16d866870c0b4d?method=download&shareKey=b4780585cb60562b17b397970de44b5b" width="551"></td><td>相较于 LSGM（联合学习 latent 表示和 diffusion 模型），本文在固定latent 空间上训练 diffusion 模型，效果更好。<br> LDMs 优于 GANs。</td></tr><tr><td><strong>text-to-image</strong><br>文本：BERT-tokenizer +transformer<br>LDM: KL-regularized</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6b110695b4321c687a47cbc2550af4f7?method=download&shareKey=711dd2d87b8007660772bd70685c087f" width="552"><br>图5</td><td><font color="red">LDM-KL-8-G: classifier-free diffusion guidance显著地提升了采样质量。</font></td></tr><tr><td>layout-to-image</td><td>图8、Sec. D.3</td><td></td></tr><tr><td>class-conditional 图像生成</td><td>表3、Sec. D.4</td><td>优于 SOTA diffusion 模型 ADM，同时显著减少了计算量、参数量。</td></tr><tr><td>image-to-image。<br> <strong>条件：concat</strong></td><td>语义生成：图9<br>超分辨率：图10、表5、表4<br>图像修复：表6、表7、图11</td><td>超分辨率：SR3[72] 生成的细节结构更连贯。<br>图像修复：相较于LDM-1，LDM-4 训练更快、采样吞吐更高、同等训练 epoch 生成质量更好。</td></tr><tr><td>计算量</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB49386e37884464d20054ba5cee04737b?method=download&shareKey=5985d1698310cb1aadfe48b21bd2d8ad" width="1133"></td><td></td></tr></tbody></table><h1 id="其它">3. 其它</h1><ul><li>Rombach R, Blattmann A, Lorenz D, et al. High-resolution imagesynthesis with latent diffusion models[C]//Proceedings of the IEEE/CVFconference on computer vision and pattern recognition. 2022:10684-10695.</li><li>预训练模型 <a href="https://github.com/CompVis/latent-diffusion" class="uri">https://github.com/CompVis/latent-diffusion</a></li><li>相关工作<ul><li>其它 likelihood-based models，如自回归 transformers，参数量大。而diffusion模型迭代生成的过程，类似高度的参数共享，可以建模复杂分布。</li><li>GANs：由于对抗训练，难以扩展到建模复杂分布，适用于变化性有限的数据。存在mode-collapse、训练不稳定问题。</li></ul></li><li>UNet[71]：2D卷积，适用于图像等有空间结构的数据。可用于超分辨率。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 生成模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AudioLDM 2</title>
      <link href="/blog/llm/yin-pin-sheng-cheng/lun-wen/audioldm2/"/>
      <url>/blog/llm/yin-pin-sheng-cheng/lun-wen/audioldm2/</url>
      
        <content type="html"><![CDATA[<p>论文：Liu H, Yuan Y, Liu X, et al. Audioldm 2: Learning holisticaudio generation with self-supervised pretraining[J]. IEEE/ACMTransactions on Audio, Speech, and Language Processing, 2024.</p><ul><li>作者：Surrey 大学、字节跳动等。</li><li><font color="red">特点<ul><li>语音、音乐、音效 生成统一框架。有条件生成。</li><li>类似 MusicLM、AudioLM，分别采用语义 token、声学token，本文中分别对应 AudioMAE、VAE 表示。</li><li>生成流程<ul><li>采用多个 encoder，将多种条件拼接、输入 GPT-2；</li><li>GPT-2 生成语义 token（AudioMAE）；</li><li>采用 LDM，以语义 token 为条件，生成声学token（VAE 表示）；</li><li>采用 VAE decoder 重建 mel 谱；</li><li>采用 HiFiGAN vocoder 生成音频采样点。</li></ul></li><li>性能：text-to-audio、text-to-music SOTA，text-to-speech 显著优于 <a href="https://huggingface.co/facebook/fastspeech2-en-ljspeech">FastSpeech2</a>。</li></ul></font></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc0570aabe53121fdbfb3c238483c7c19?method=download&shareKey=80e274ee308805c24d26944870725bdb" width="921"></p><p><span class="math display">\[ \mathcal{H}_0=\mathcal{G} \circ\mathcal{M}: C \mapsto \hat{Y} \mapsto x \]</span> <span class="math display">\[ Y=\mathcal{A}(x) \]</span></p><p>其中，<span class="math inline">\(C\)</span> 为condition，可以是文本、音素、音频、图像等。<span class="math inline">\(\hat{Y}\)</span> 为音频中间表示，<span class="math inline">\(x\)</span> 为采样点。<span class="math inline">\(\mathcal{A}\)</span>为自监督表示学习预训练模型，如 AudioMAE。<span class="math inline">\(\mathcal{M}: C \mapsto \hat{Y}\)</span> 采用GPT-2。<span class="math inline">\(\hat{Y} \mapsto x\)</span> 采用latent diffusion 模型（VAE 表示） + VAE decoder（mel 谱） + HiFiGANvocoder（采样点）。</p><h2 id="audiomae">1.1. AudioMAE</h2><ul><li>相较于其它自监督预训练模型的优势<ul><li>相较于专注特定领域的模型，如 MERT（音乐）、HuBERT（语音），AudioMAE在 AudioSet 多种音频上进行了预训练，在通用音频分类任务上具有 SOTA性能。</li><li><font color="green">相较于采用对比 loss、next token预测等判别性损失，如 wav2vec，AudioMAE采用重建损失，可能更适合生成任务。</font></li></ul></li><li><a href="https://github.com/facebookresearch/AudioMAE">audio maskedautoencoder (AudioMAE)</a> encoder：将 log mel 谱，采用 <span class="math inline">\(k=s=P, c=D\)</span>的卷积，分割成 <span class="math inline">\(P \times P\)</span> 的 patch，encoder 输出 <span class="math inline">\(E \in \mathbb{R}^{T^{\prime} \times F^{\prime}\times D}\)</span>，<span class="math inline">\(T^{\prime}=T /P\)</span>，<span class="math inline">\(F^{\prime}=F / P\)</span>。</li><li>后处理：2D <font color="green">average-maxpooling[52]（保留时频关系）</font><span class="math inline">\(k=s=\lambda\)</span>，reshape，得到 embedding 序列<span class="math inline">\(L_\lambda \times D\)</span>，<span class="math inline">\(L_\lambda=T^{\prime} F^{\prime} /\lambda^2\)</span>。</li></ul><h2 id="vae">1.2. VAE</h2><ul><li>variational autoencoder (VAE) 用于 LDM。<span class="math inline">\(\mathcal{V}: X \mapsto z \mapsto\hat{X}\)</span>，压缩 mel 谱、重建。</li><li><font color="green">优化目标：重建损失 + 判别损失 + <span class="math inline">\(z\)</span> 和标准高斯的 KL 散度（约束 latent空间的方差）。</font></li></ul><h2 id="audiomae-vs-vae">1.3. AudioMAE vs VAE</h2><ul><li>相较于 AudioMAE，VAE更关注重建质量，<font color="green">压缩比更高</font>。</li><li>AudioMAE 包含更多语义信息。<img src="https://note.youdao.com/yws/api/personal/file/WEB3382540fdf27039037cade70eb98048c?method=download&shareKey=9afd9631dd94a9f9d113ba92a2a54b69" width="456"></li></ul><h2 id="gpt-2">1.4. GPT-2</h2><ul><li>condition<ul><li>可以是文本、音素、音频、图像等。采用多个 encoder +线性层，沿序列长度维度拼接多个 encoder 表示。<ul><li><a href="https://github.com/LAION-AI/CLAP">contrastive language andaudio pretraining (CLAP)</a>：音频字幕（不同于转写）采用 CLAP textencoder，否则采用 CLAP audio encoder，如 <font color="green">TTS（同AudioLDM）</font>。</li><li><a href="https://huggingface.co/google/flan-t5-large">FLAN-T5</a>（enhancedtext-to-text transfer transformer (T5)）：用于捕捉文本的时序信息。</li><li>音素 encoder：见 NaturalSpeech [2]。采用 <a href="https://github.com/espeak-ng/espeak-ng">Espeak phonemizers</a>将文本转为音素序列。</li></ul></li><li>除音素 encoder 无可用预训练权重外，其它预训练的 encoder 冻结。</li><li><font color="green">TTS 训练时，采用 CLAP audio encoder 计算condition，推理时采用 CLAP text encoder，有利于 prompt-based说话人控制。</font></li></ul></li><li>采用连续表示 AudioMAE 的 MSE loss 微调。</li><li><font color="green">由于序列长度变短，相较于MusicLM、SoundStream，减轻了推理计算成本高、错误累积的问题。</font></li></ul><h2 id="ldm">1.5. LDM</h2><ul><li>latent diffusion model (LDM) 以 AudioMAE 序列为条件，采用 VAE表示进行特征压缩。</li><li>采用 Transformer-UNet (T-UNet) 架构。为了扩大模型容量，在卷积 block后插入了 <span class="math inline">\(n_{\text {trans }}+1\)</span> 个transformer blocks，最后1个 transformer block 采用 cross attention 引入condition 信息，其它采用 self attention。</li><li>在 TTS 任务中，添加额外的 cross attention 层，引入 FLAN-T5condition。</li></ul><h3 id="classifier-free-guidance">1.5.1. Classifier-free guidance</h3><ul><li>训练时以一定概率（如0.1）丢弃 condition。生成时 <span class="math display">\[\mathcal{G}^{\prime}\left(z_t, t, Y ;\phi\right)=w \mathcal{G}\left(z_t, t ; \phi\right)+(1-w)\mathcal{G}\left(z_t, t, Y ; \phi\right)\]</span></li><li><font color="green">Denoising Diffusion Implicit Models (DDIM) [90]采样时 classifier-free guidance scale 设为 3.5。</font></li></ul><h1 id="数据集">2. 数据集</h1><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>数据集</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>AudioSet (AS)</td><td>音频分类</td><td>200万，527 类，10s</td></tr><tr><td>WavCaps</td><td>音频字幕</td><td>403050，ChatGPT 生成弱标签，平均68s</td></tr><tr><td>AudioCaps (AC)</td><td>音频字幕</td><td>46000，人工标注</td></tr><tr><td>VGGSound (VS)</td><td>音频分类，单标签</td><td>20万视频</td></tr><tr><td>Free Music Archive (FMA)</td><td>无标注</td><td>106574</td></tr><tr><td>Million Song Dataset (MSD) 有标注子集[85]</td><td>tag、标题、artist 等元数据</td><td>51万</td></tr><tr><td>LJSpeech (LJS)</td><td>音频、转写，单说话人</td><td>13100</td></tr><tr><td>GigaSpeech (GGS)</td><td>语音识别</td><td>1wh</td></tr></tbody></table><p>合计 29510h。</p><h1 id="训练">3. 训练</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0321be8daedc335de43427bba6e3ce77?method=download&shareKey=2002e2c181be21d052559192d174e6fc" width="455"></p><ul><li><p>AudioLDM 2 T-UNet <span class="math inline">\(n_{\text{trans}}=2\)</span>，AudioLDM 2-Large <span class="math inline">\(n_{\text {trans}}=6\)</span>。</p></li><li><p>AudioMAE：<span class="math inline">\(P=16\)</span>，10s音频序列长度 512，768 维。最后 16 个 transformer层输出求平均，作为最终的表示。</p></li><li><p>训练时，按一定概率动态选择 AudioMAE 特征来自 ground truth（<span class="math inline">\(P_{\mathrm{gt}}=0.25\)</span>）还是 GPT-2（<span class="math inline">\(P_{\text {pred }}=0.75\)</span>）。</p></li><li><p>GPT-2、LDM 分别训练，再联合微调。音频-文本配对数据训练GPT-2。所有音频数据训练 LDM。</p></li><li><p>LDM 预训练：用 10s片段训练，<font color="green">零填充到10.24s</font>。随机选择 <span class="math inline">\(\lambda \in\{1,2,4,8\}\)</span>以增强鲁棒性。</p></li></ul><h1 id="评价">4. 评价</h1><ul><li>评价指标<ul><li>客观<ul><li>Frechet Audio Distance (FAD)：VGGish模型提取的目标、生成音频的分布距离。</li><li>KL 散度：采用 Patch-out Transformer（音频 tag模型），评价目标、生成音频的相似度。</li><li>CLAP 评分：评价音频、文本之间的相似度。<span class="math inline">\(\operatorname{CLAPScore}(x, r)=\frac{\vec{e}_x\cdot \vec{e}_r}{\max\left(\left\|\vec{e}_x\right\|\left\|\vec{e}_r\right\|,\epsilon\right)}\)</span>。</li></ul></li><li>主观<ul><li>总体印象（OVL）、音频-文本相关性（REL）、MOS（TTS）。</li><li>众包标注，可靠性：<ul><li>worker: approval rate &gt;= 60%，approvals &gt;= 50。</li><li>每条音频至少10人标注。</li></ul></li></ul></li></ul></li></ul><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>任务</th><th>测试集</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>text-to-audio</td><td>AudioCaps</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBac7d937074d3cdfb37f1de7f51b69849?method=download&shareKey=04df36762de478a3e82cb660d3b81cd2" width="904"></td><td>相较于 -AC，-Full 效果变差，可能由于训练、测试不匹配。</td></tr><tr><td>text-to-music</td><td>MusicCaps</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB127b8c51aac5ab3ef1cdba38c8bb69cf?method=download&shareKey=5eee3f5d6be2a236acb9b0afdb9fa878" width="443"></td><td>相较于 -MSD，-Full 效果更好。<br>AudioLDM-M CLAP score 高，可能由于conditioned by 同一模型。</td></tr><tr><td>text-to-speech</td><td>LJSpeech</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBd9e31aa3da855664bfd630057d3f87f0?method=download&shareKey=e487cd4026f42b788bcb1f602dc65860" width="429"><br>AudioLDM2-LJS-Pretrained: 在 GigaSpeech 上预训练 GPT-2。</td><td>主观评价发现 AudioLDM 2-LJS-Pretrained在情感、标点符号、语调上有更好的起伏变化。</td></tr><tr><td>消融实验</td><td>AudioCaps</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBaf759f900f29a77e513465f58a0998a8?method=download&shareKey=2a5c3a4d512029bb2a9b453cce5c3ab9" width="373"></td><td>a. GPT-2、LDM 联合微调有明显的收益。</td></tr><tr><td>pooling 下采样倍数的影响</td><td></td><td>LDM的重建效果<br><img src="https://note.youdao.com/yws/api/personal/file/WEB8fb851a09d7af5961c25e62e62763c83?method=download&shareKey=c1b338f08badb24d53307e623c91b5ac" width="454"></td><td><span class="math inline">\(\lambda=1\)</span> 时最接近 groundtruth，<span class="math inline">\(\lambda=2, 4\)</span>时差异较小，虽然未保留所有细节，但准确地保留了语义信息。</td></tr></tbody></table><ul><li>相较于 AudioLDM，利用 GPT-2、diffusion模型，在生成质量、通用性、可懂度方面有提升。</li></ul><h1 id="代码">5. 代码</h1><ul><li><p><a href="https://audioldm.github.io/audioldm2">代码、预训练模型、demo（还包含Image-to-Audio）</a></p></li><li><p><a href="https://github.com/haoheliu/AudioLDM-training-finetuning">训练代码</a></p></li><li><p><a href="https://huggingface.co/blog/audioldm2">推理速度优化：采用 HuggingFace 🧨 Diffusers</a></p></li><li><p><a href="https://audioldm.github.io/audioldm2/#gigaspeech">prompt-basedspeaker control</a><img src="https://note.youdao.com/yws/api/personal/file/WEB222628bffd4881e9e021ee704e717f01?method=download&shareKey=4176bdbdfc7c99d935ccef6336e1d1c5" width="454"></p><pre class="line-numbers language-none"><code class="language-none">audioldm2 -t &quot;A female reporter is speaking full of emotion&quot; --transcription &quot;Wish you have a good day&quot;audioldm2 -t &quot;A female reporter is speaking&quot; --transcription &quot;Wish you have a good day&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频生成 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>tokenizer</title>
      <link href="/blog/nlp/tokenizer/"/>
      <url>/blog/nlp/tokenizer/</url>
      
        <content type="html"><![CDATA[<ul><li>子词：若采用所有单词、符号作为词表，太大；若采用字符，难以表达语义。</li></ul><h1 id="子词分割算法">1. 子词分割算法</h1><h2 id="bpe">1.1. BPE</h2><ul><li>Sennrich R. Neural machine translation of rare words with subwordunits[J]. arXiv preprint arXiv:1508.07909, 2015.</li><li>目的：解决罕见词和 OOV 词的翻译问题，支持开放词汇。</li><li>BPE (Byte Pair Encoding) 算法<ul><li>初始化词表为所有字符。每个词表示为字符序列加特殊的结束符（表示词边界）。为了提高效率，不考虑跨词边界的字节对。<font color="red">（需要先分词）</font></li><li>迭代计算所有<font color="red">连续符号对 (<span class="math inline">\(s_i\)</span>, <span class="math inline">\(s_{i+1}\)</span>)的频率、合并频率最高的符号对</font>。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEBe240fe1144ef451fecc569e3340b6a84?method=download&shareKey=e2407061ecc1197145159e1e08c816aa" width="439"></li><li>指定词表大小=初始词表大小+合并次数。</li><li>推理：对句子应用相同的合并操作。</li><li>效果：翻译质量更高。且提高了罕见词、OOV 词的 unigram-F1。</li></ul><h2 id="wordpiece">1.2. WordPiece</h2><ul><li>Schuster M, Nakajima K. Japanese and korean voice search[C]//2012IEEE international conference on acoustics, speech and signal processing(ICASSP). IEEE, 2012: 5149-5152.</li></ul><ol type="1"><li>初始化词表为基本 Unicode 字符。</li><li>采用上述词表在训练集上训语言模型。</li><li>组合词表中的两个单元，<font color="red">选择添加到词表后训练数据似然度增量最大的一个</font>（$P(bc|a)P(d|bc) - P(b|a) P(c|b) P(d|c) $）。</li><li>重复2-3，直到达到预定义的词表大小或似然性增量低于某个阈值。</li></ol><h2 id="unigram-lm">1.3. Unigram LM</h2><ul><li>Kudo T. Subword regularization: Improving neural network translationmodels with multiple subword candidates[J]. arXiv preprintarXiv:1804.10959, 2018.</li><li>采用 unigram LM，假设子词序列的概率为其中各子词出现概率的乘积。</li><li>从训练语料库启发式地创建一个合理大的词表。重复以下步骤直到达到指定的词表大小。<ul><li><font color="green">采用期望最大化算法估计子词概率，最大化训练语料库的边际似然。</font><span class="math display">\[\mathcal{L}=\sum_{s=1}^{|D|} \log\left(P\left(X^{(s)}\right)\right)=\sum_{s=1}^{|D|} \log\left(\sum_{\mathbf{x} \in \mathcal{S}\left(X^{(s)}\right)}P(\mathbf{x})\right)\]</span> 其中，<span class="math inline">\(|D|\)</span> 为语料库中句子的总数，<span class="math inline">\(X\)</span> 为句子，<span class="math inline">\(\mathcal{S}\left(X\right)\)</span>为所有可能的子词分割方案。</li><li>对每个子词，计算从当前词表移除该子词后，似然减小的量。</li><li>按似然减小量排序，保留 <span class="math inline">\(\eta \%\)</span>的子词（比如 80%）。</li></ul></li><li>子词正则化：由于同一句子可以有多种子词分割方式，训练中随机采样多个子词分割来提高模型的鲁棒性。</li></ul><h1 id="bpe-训练分词工具">2. BPE 训练、分词工具</h1><h2 id="sentencepiece">2.1. SentencePiece</h2><ul><li>Kudo T. Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing[J]. arXiv preprintarXiv:1808.06226, 2018.</li><li><a href="https://github.com/google/sentencepiece" class="uri">https://github.com/google/sentencepiece</a></li><li>特点<ul><li>支持两种子词分割算法：BPE、unigram LM。</li><li>无损：将文本视为 Unicode 字符序列，将空白视为普通符号（转义为“▁”（U+2581））。</li><li>语言无关、无需预分词、纯数据驱动：现有的子词分割工具通常假设输入已经预分词，依赖于语言特定的预处理和后处理，而SentencePiece 可以直接从原始句子训练。</li><li>指定词表大小。</li><li>实现了高效的分词和训练，且轻量。</li><li>自包含：只要使用相同的模型文件，就可以得到相同的标记化/去标记化。</li></ul></li></ul><h2 id="tiktoken">2.2. tiktoken</h2><ul><li><a href="https://github.com/openai/tiktoken" class="uri">https://github.com/openai/tiktoken</a> OpenAI</li><li>相较于 transformers TokenizerFast 更快。</li><li>应用：如 Llama 3、<a href="https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md">Qwen</a>、GLM-4-9b。</li></ul><h1 id="优化点">3. 优化点</h1><ul><li>最优词表大小</li><li>压缩比（char/token）</li><li>多语言支持</li></ul><h1 id="其它">4. 其它</h1><ul><li>Byte-level BPE：将所有 unicode 字符作为基础词表很大（<a href="https://en.wikipedia.org/wiki/Unicode#Versions">超过15万个</a>）。将字节作为基础字符仅256 个且能保证无 OOV。</li><li><a href="https://huggingface.co/docs/transformers/tokenizer_summary">transformers综述文档</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Dataset &amp; DataLoader</title>
      <link href="/blog/ji-qi-xue-xi/mo-xing-xun-lian/dataset-dataloader/"/>
      <url>/blog/ji-qi-xue-xi/mo-xing-xun-lian/dataset-dataloader/</url>
      
        <content type="html"><![CDATA[<h1 id="音频读取">1. 音频读取</h1><ul><li>采用 torchaudio，支持后缀为 wav、mp3（如 common voice）、flac（如Librispeech）、raw、WAV（如 TIMIT）、mp4（如 MELD）、m4a（如 Voxceleb）等的音频文件。</li><li>截取指定时间段</li><li>转单声道</li><li>重采样</li><li>加噪</li><li>降噪（匹配业务处理）</li><li>变速</li><li>音量缩放</li><li>……</li></ul><h1 id="特征提取">2. 特征提取</h1><ul><li>直接输入原始波形 或 FBANK</li></ul><h1 id="dataset-dataloader">3. Dataset &amp; DataLoader</h1><ul><li>相关实现<ul><li><a href="https://wenet.org.cn/wenet/UIO.html">WeNet UIO</a></li><li><a href="https://mp.weixin.qq.com/s/PBq3p9RR9LwENAdQQOJimw">Lhotse</a></li></ul></li></ul><h2 id="dataset">3.1. Dataset</h2><ul><li><p>dataset 存储音频路径。</p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>dataset</th><th>缺点</th></tr></thead><tbody><tr><td>音频路径</td><td>训练时读大量小文件，频繁地寻址，随机读性能慢。</td></tr><tr><td>原始波形或频谱特征</td><td>占用另外的存储空间；<br>对于输入不同的模型需要分别准备数据；<br>训练时shuffle，变回非连续存储，访问慢</td></tr></tbody></table><p>如 1wh 16kHz、16bit、单声道 采样点数据、80维 FBANK 均约为 1TB。<code>10000 h * 3600 s/h * 16000 samples/s * 2 byte/sample / (2 ** (10 * 4)) ~= 1.05 TB</code><code>10000 h * 3600 s/h * 100 frames/s * 80 dim/frame * 4 byte (float) / (2 ** (10 * 4)) ~= 1.05 TB</code></p></li><li><p>另外，WeNet UIO通过将数据集做成多个压缩包（含音频、文本），shuffle压缩包，顺序读取一个压缩包内的数据，优化随机读性能慢的问题。</p></li><li><p>dataset 采用 <a href="https://huggingface.co/docs/datasets/about_arrow">🤗Datasets，Arrow 格式</a>。</p><p>优点：从磁盘进行内存映射，允许在设备内存相对较小的机器上使用大型数据集。</p><p>存在的问题：<a href="https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable">🤗Datasets 基于 Arrow，训练时 shuffle 可达10x 慢。 优化：采用 🤗IterableDataset，shuffle shards。</a></p></li><li><p>特征提取在生成 batch 时计算。</p><ul><li>可通过 <code>Dataset.__getitem__</code> 或<code>DataLoader.collate_fn</code> 实现。通常，batch 处理更高效，如 <a href="https://huggingface.co/docs/datasets/v2.20.0/en/about_map_batch">🤗Tokenizers</a>、<code>torch.stft</code> 支持 batch。</li><li>FBANK 实现：🤗 transformers&gt;=4.40.0 支持 <code>torch.stft</code>GPU batch 计算。</li></ul></li></ul><h2 id="dataloader">3.2. DataLoader</h2><ul><li><font color="green">动态 batch</font></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型训练 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>denoising diffusion</title>
      <link href="/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/denoising-diffusion/"/>
      <url>/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/denoising-diffusion/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：Ho J, Jain A, Abbeel P. Denoising diffusion probabilisticmodels[J]. Advances in neural information processing systems, 2020, 33:6840-6851.</li><li><font color="red">核心思想<ul><li>将无条件生成近似为马尔可夫链过程，转移概率为条件高斯，反向过程由噪声<span class="math inline">\(p\left(\mathbf{x}_T\right) =\mathcal{N}\left(\mathbf{x}_T ; \mathbf{0}, \mathbf{I}\right)\)</span>渐进式地生成数据，正向过程为逐步向数据加高斯噪声直到信号被破坏。</li><li>将正向过程的分布参数固定为常数超参数，通过重参数化 <a href="#Eq8-1">Eq. (8-1)</a>、<a href="#Eq11">Eq. (11)</a>、<a href="#Eq7-3">Eq. (7-3)</a>，得到变分界 <a href="#Eq12">Eq.(12)</a>。进一步简化为 <a href="#Eq14">Eq. (14)</a>，类似于对较小的<span class="math inline">\(t\)</span> 降低了 loss权重，有利于采样质量。</li><li>对于图像离散数据，采用 <a href="#Eq13">Eq. (13)</a> 所示的离散decoder，使得在生成最终数据时不需要加噪。</li><li><a href="#训练、采样">训练、采样流程</a></li></ul></font></li></ul><h1 id="diffusion-model">1. diffusion model</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB541b78b0b3c2bff349127573a9fbcf73?method=download&shareKey=2916faa23b8813f19d8f54105ca2b16a" width="538"></p><p>latent 变量模型，其中 <span class="math inline">\(\mathbf{x}_1,\ldots, \mathbf{x}_T\)</span> 为 latents，与数据 <span class="math inline">\(\mathbf{x}_0\)</span>同维度。马尔可夫链，逐步向数据加高斯噪声直到信号被破坏，假设马尔可夫链的转移概率为条件高斯。</p><p>reverse process <span class="math display">\[p_\theta\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_T\right)\prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\]</span> <font color="red"><span class="math display">\[p\left(\mathbf{x}_T\right) =\mathcal{N}\left(\mathbf{x}_T ; \mathbf{0},\mathbf{I}\right)\]</span></font> <span class="math display">\[p_\theta\left(\mathbf{x}_{t-1} \mid\mathbf{x}_t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ;\boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right),\mathbf{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)\]</span></p><p>forward/diffusion process 近似后验 <span class="math display">\[q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right):=\prod_{t=1}^Tq\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)\]</span><font color="red"><span class="math display">\[ q\left(\mathbf{x}_t \mid\mathbf{x}_{t-1}\right):=\mathcal{N}\left(\mathbf{x}_t ;\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)\]</span></font></p><blockquote><p>备注： 重参数化 <span class="math inline">\(\mathbf{x}_t=\sqrt{1-\beta_t}\mathbf{x}_{t-1}+\epsilon_t\)</span>，其中 <span class="math inline">\(\epsilon_t \sim \mathcal{N}\left(0, \beta_t\mathbf{I}\right)\)</span>，即噪声的方差为 <span class="math inline">\(\beta_t\)</span>。</p><p><span class="math inline">\(\operatorname{Var}\left(\mathbf{x}_t\right)=\operatorname{Var}\left(\sqrt{1-\beta_t}\mathbf{x}_{t-1}+\epsilon_t\right) = \left(1-\beta_t\right)\sigma^2+\beta_t\)</span>，其中 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 的方差为 <span class="math inline">\(\sigma^2\)</span>。</p><p>若原始数据的方差为1，则所有 latents 的方差保持 1 不变。</p><p>可以理解为对 <span class="math inline">\(t-1\)</span> 时刻的数据<span class="math inline">\(\mathbf{x}_{t-1}\)</span>进行一定程度的“确定性”变换得到 <span class="math inline">\(\sqrt{1-\beta_t}\mathbf{x}_{t-1}\)</span>，再在每个维度上独立地添加均值为0、方差为 <span class="math inline">\(\beta_t\)</span> 的高斯噪声。其中，<span class="math inline">\({1-\beta_t}\)</span> 决定了 <span class="math inline">\(\mathbf{x}_t\)</span> 中 <span class="math inline">\(\mathbf{x}_{t-1}\)</span> 所占的比例，<span class="math inline">\(\beta_t\)</span> 决定噪声比例。</p></blockquote><p>令 <span class="math inline">\(\alpha_t:=1-\beta_t\)</span>，<span class="math inline">\(\bar{\alpha}_t:=\prod_{s=1}^t\alpha_s\)</span>，则 <span id="Eq4"><span class="math display">\[q\left(\mathbf{x}_t \mid\mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ;\sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right)\mathbf{I}\right) \tag{4}\]</span></span></p><blockquote><p>备注： <span class="math display">\[ \begin{aligned}\mathbf{x}_t&amp;=\sqrt{\alpha_t} \mathbf{x}_{t-1}+\epsilon_t  \\&amp;=\sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}}\mathbf{x}_{t-2}+\epsilon_{t-1}\right)+\epsilon_t \\&amp;=\sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}} \left(\sqrt{\alpha_{t-2}}\mathbf{x}_{t-3}+\epsilon_{t-2}\right) +\epsilon_{t-1}\right)+\epsilon_t\\&amp;= \sqrt{\alpha_{t}\alpha_{t-1}\alpha_{t-2}}\mathbf{x}_{t-3}+\sqrt{\alpha_{t}\alpha_{t-1}}\epsilon_{t-2} +\sqrt{\alpha_{t}}\epsilon_{t-1} + \epsilon_t \\&amp;=\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sum_{i=0}^{t-1}\sqrt{\alpha_t \alpha_{t-1} \cdots \alpha_{t-i+1}} \epsilon_{t-i}\end{aligned} \]</span> <span class="math inline">\(\mathbb{E}\left[\mathbf{x}_t\right]=\sqrt{\bar{\alpha}_t}\mathbb{E}\left[\mathbf{x}_0\right]\)</span></p><p><font color="green">由于 <span class="math inline">\(\beta_t\)</span>较小，<span class="math inline">\(\mathbb{E}\left[\mathbf{x}_0\right]\)</span>接近0时，才能使 <span class="math inline">\(\mathbb{E}\left[\mathbf{x}_T\right]\)</span>接近0？ <span class="math display">\[\begin{aligned}\operatorname{Var}\left(\mathbf{x}_t\right)&amp;=\bar{\alpha}_t\operatorname{Var}\left(\mathbf{x}_0\right)+ \sum_{i=0}^{t-1} \alpha_t\alpha_{t-1} \cdots \alpha_{t-i+1}\beta_{t-i} \\&amp;= \bar{\alpha}_t + (1-\alpha_t) + \alpha_t(1-\alpha_{t-1}) +\alpha_t\alpha_{t-1}(1-\alpha_{t-2}) + \cdots + \alpha_t\alpha_{t-1}\cdots \alpha_2(1-\alpha_1) \\&amp;= 1\end{aligned}\]</span> 存在的问题：哪里推导得不对？</font></p></blockquote><p>训练：通过 Variational Inference 训练，优化变分下界 <span class="math display">\[ \mathbb{E}\left[-\logp_\theta\left(\mathbf{x}_0\right)\right] \leq \mathbb{E}_q\left[-\log\frac{p_\theta\left(\mathbf{x}_{0: T}\right)}{q\left(\mathbf{x}_{1: T}\mid \mathbf{x}_0\right)}\right]=\mathbb{E}_q\left[-\logp\left(\mathbf{x}_T\right)-\sum_{t \geq 1} \log\frac{p_\theta\left(\mathbf{x}_{t-1} \mid\mathbf{x}_t\right)}{q\left(\mathbf{x}_t \mid\mathbf{x}_{t-1}\right)}\right]=: L \]</span></p><p>重写 <span class="math inline">\(L\)</span>（推导过程详见论文附录）<span class="math display">\[\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid\mathbf{x}_0\right) \|p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t&gt;1}\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid\mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid\mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\logp_\theta\left(\mathbf{x}_0 \mid\mathbf{x}_1\right)}_{L_0}]  \tag{5}\]</span> <font color="green"><span class="math display">\[ q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t,\mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ;\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right),\tilde{\beta}_t \mathbf{I}\right) \tag{6}\]</span> <span id="Eq7-1"><span class="math display">\[\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t,\mathbf{x}_0\right):=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}\mathbf{x}_t  \tag{7-1}\]</span></span> <span class="math display">\[\tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t\tag{7-2}\]</span></font> 其中，高斯分布之间的 KL 散度可以直接计算。</p><h1 id="forward-process">2. Forward process</h1><p>相较于通过重参数化学习 <span class="math inline">\(\beta_t\)</span>，本文将其固定为常数超参数，因此近似后验<span class="math inline">\(q\)</span> 没有可学习的参数，<span class="math inline">\(L_T\)</span> 为常数，训练时可忽略。</p><h1 id="reverse-process">3. Reverse process</h1><p>令 <span id="Eq7-3"><span class="math display">\[\boldsymbol{\Sigma}_\theta\left(\mathbf{x}_t,t\right)=\sigma_t^2 \mathbf{I} \tag{7-3}\]</span></span>为时间相关的常数。</p><p>则 <span class="math display">\[ L_{t-1}=\mathbb{E}_q\left[\frac{1}{2\sigma_t^2}\left\|\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t,\mathbf{x}_0\right)-\boldsymbol{\mu}_\theta\left(\mathbf{x}_t,t\right)\right\|^2\right]+C \tag{8}\]</span> 其中，<span class="math inline">\(C\)</span> 为$ $ 无关的常数。</p><p>重参数化 <a href="#Eq4">Eq. (4)</a> <span id="Eq8-1"><span class="math display">\[ \mathbf{x}_t\left(\mathbf{x}_0,\boldsymbol{\epsilon}\right)=\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon} \quad \text{for} \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I}) \tag{8-1}\]</span></span> 和 <a href="#Eq7-1">Eq.(7-1)</a>、<span class="math inline">\(\alpha_t:=1-\beta_t\)</span>一起带入上式得到 <span id="Eq10"><span class="math display">\[L_{t-1}-C=\mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}}\left[\frac{1}{2\sigma_t^2}\left\|\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t\left(\mathbf{x}_0,\boldsymbol{\epsilon}\right)-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}\right)-\boldsymbol{\mu}_\theta\left(\mathbf{x}_t\left(\mathbf{x}_0,\boldsymbol{\epsilon}\right), t\right)\right\|^2\right]\tag{10}\]</span></span></p><p>参数化 <span id="Eq11"><span class="math display">\[\boldsymbol{\mu}_\theta\left(\mathbf{x}_t,t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)\tag{11}\]</span></span> 其中，<span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> 为从 <span class="math inline">\(\mathbf{x}_t\)</span> 预测 <span class="math inline">\(\boldsymbol{\epsilon}\)</span> 的函数近似。 <span class="math display">\[\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)+\sigma_t\mathbf{z}, \text { where } \mathbf{z} \sim \mathcal{N}(\mathbf{0},\mathbf{I})\]</span></p><p>因此，<a href="#Eq10">Eq. (10)</a>简化为 <span id="Eq12"><span class="math display">\[\mathbb{E}_{\mathbf{x}_0,\boldsymbol{\epsilon}}\left[\frac{\beta_t^2}{2 \sigma_t^2\alpha_t\left(1-\bar{\alpha}_t\right)}\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon},t\right)\right\|^2\right] \tag{12}\]</span></span></p><h1 id="简化训练目标">4. 简化训练目标</h1><p><span id="Eq14"><span class="math display">\[ L_{\text {simple}}(\theta):=\mathbb{E}_{t, \mathbf{x}_0,\boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon},t\right)\right\|^2\right] \tag{14}\]</span></span> 相较于 <a href="#Eq12">Eq. (12)</a>，<font color="green">类似于对较小的 <span class="math inline">\(t\)</span> 降低了 loss权重，有利于采样质量。</font></p><h1 id="l_0离散-decoder">5. \(L_0\)、离散decoder</h1><ul><li><p>将图像数据 [0，255] 缩放到 [-1, 1]。</p></li><li><p>离散解码器/离散 likelihood</p><p><span id="Eq13"><span class="math display">\[p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right) =\prod_{i=1}^D\int_{\delta_{-}\left(x_0^i\right)}^{\delta_{+}\left(x_0^i\right)}\mathcal{N}\left(x ; \mu_\theta^i\left(\mathbf{x}_1, 1\right),\sigma_1^2\right) dx \\\delta_{+}(x) =\left\{\begin{array}{ll}\infty &amp; \text { if } x=1 \\ x+\frac{1}{255} &amp; \text { if }x&lt;1\end{array} \quad \delta_{-}(x)= \begin{cases}-\infty &amp; \text { if }x=-1 \\ x-\frac{1}{255} &amp; \text { if } x&gt;-1\end{cases}\right.\tag{13}\]</span></span></p><p>其中，<span class="math inline">\(D\)</span> 表示数据维度，<span class="math inline">\(i\)</span>表示坐标。可以替换为更强大的解码器，如条件自回归模型。</p><p><font color="green">使得对离散数据是无损编码，且在生成最终数据时不需要加噪。</font></p></li></ul><h1 id="训练采样">6. 训练、采样</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa03ec53bf0a5e84468db19290149e775?method=download&shareKey=e33d664f7355ba00d042e6aa44dcfd98" width="712"></p><h1 id="渐进有损编码压缩">7. 渐进有损编码/压缩</h1><p>对于 <span class="math inline">\(\mathbf{x}_0 \simq\left(\mathbf{x}_0\right)\)</span>，依次发送 <span class="math inline">\(\mathbf{x}_T, \ldots,\mathbf{x}_0\)</span>，接收端渐近地估计 <span class="math display">\[\mathbf{x}_0 \approx\hat{\mathbf{x}}_0=\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta\left(\mathbf{x}_t\right)\right) /\sqrt{\bar{\alpha}_t}\]</span> 失真为 <span class="math display">\[\sqrt{\left\|\mathbf{x}_0-\hat{\mathbf{x}}_0\right\|^2/ D}\]</span> <font color="green">rate：假设传输1个样本 <span class="math inline">\(\mathbf{x} \sim q(\mathbf{x})\)</span> 平均需要<span class="math inline">\(D_{\mathrm{KL}}(q(\mathbf{x}) \|p(\mathbf{x}))\)</span> bit，则 rate（总编码长度）为 <span class="math inline">\(L_T+\cdots+L_1\)</span>。</font></p><h1 id="实验">8. 实验</h1><p><span class="math inline">\(T=1000\)</span>。前向过程的方差线性增加<span class="math inline">\(\beta_1=10^{-4}\)</span>，<span class="math inline">\(\beta_T=0.02\)</span>；<font color="green">当<span class="math inline">\(\beta_t\)</span>较小时，前向过程、反向过程有相同的函数形式；另外，使得 <span class="math inline">\(\mathbf{X}_T\)</span> 时的信噪比</font> <span class="math inline">\(D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid\mathbf{x}_0\right) \| \mathcal{N}(\mathbf{0}, \mathbf{I})\right)\approx 10^{-5}\)</span> 尽可能小。</p><p><font color="green">反向过程采用 U-Net + groupnormalization。各时间步参数共享 + Transformer 正弦位置 embedding。分辨率<span class="math inline">\(16 \times 16\)</span>。</font></p><p>评价指标</p><ul><li>Inception score</li><li>FID score</li><li>负 log likelihood(lossless codelengths)（对比基于 likelihood的模型） </li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>采样质量</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB488ad36f737fd0d4f85f86660b846714?method=download&shareKey=af4b5424fa2603d8127e3696e78e1a90" width="547"></td><td>采用真实变分界，NLL 更好；采用简化版本，采样质量更好。</td></tr><tr><td>反向过程参数化、训练目标</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBabd8aa5bdc8f8821f563971b32c00d5f?method=download&shareKey=251eae700aef3aea210c4e7a3803131e" width="396"></td><td>1. 预测 <span class="math inline">\(\tilde{\boldsymbol{\mu}}\)</span> 时，采用 MSEloss（类似于<a href="#Eq14">Eq.(14)</a>）训练不稳定，可采用真实变分界；<br> 2.相较于固定反向过程方差，学习方差导致样本质量较差或训练不稳定；<br> 3.相较于预测 <span class="math inline">\(\tilde{\boldsymbol{\mu}}\)</span>，预测 <span class="math inline">\(\boldsymbol{\epsilon}\)</span>，采用真实变分界、固定方差时性能相当，采用简化版本时性能更好。</td></tr><tr><td>渐进编码</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe0d2f1389ceba4712de3ef1a97ff5276?method=download&shareKey=081b6d71ed8f5438c3997996fc00c0ec" width="716"></td><td>绝大多数 bit 用于表征不易感知的失真/图像细节。</td></tr><tr><td>渐进式图像生成</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB003d41aa650b5db01f30d3721b95bf2f?method=download&shareKey=6e001c87d2c47bb3b89ee2281205e7d3" width="1362"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB2e92e297542059b5d0cc27900e476c2f?method=download&shareKey=820860d80f4b2e57130b5e1d4ac54c0c" width="1248"></td><td>先生成大尺度的图像特征，再生成细节。</td></tr><tr><td>latent 插值</td><td>示例样本详见论文图8、图9</td><td></td></tr></tbody></table><h1 id="其它">9. 其它</h1><ul><li><a href="https://github.com/hojonathanho/diffusion" class="uri">https://github.com/hojonathanho/diffusion</a></li><li><font color="green">生成模型：GAN、自回归、flows、VAE、基于能量的模型、scorematching。</font><ul><li><p>score matching</p><blockquote><ul><li>随机变量的 score（或梯度）是其概率密度函数的梯度。对于数据点 <span class="math inline">\(x\)</span>, 其在分布 <span class="math inline">\(p(x)\)</span> 下的得分为 <span class="math inline">\(\nabla_x \log p(x)\)</span></li><li>目标函数：<span class="math display">\[ L=\mathbb{E}_{x \simp}\left\|\nabla_x \log q(x ; \theta)-\nabla_x \log p(x)\right\|^2\]</span> 其中, <span class="math inline">\(\theta\)</span> 是模型参数,<span class="math inline">\(p\)</span> 是真实数据分布, <span class="math inline">\(q\)</span> 是模型分布。</li><li>梯度估计：取训练样本上的梯度均值。</li><li>相较于 likelihood-based model直接建模样本分布，学习数据分布的局部梯度特征，对未见数据的泛化性能更好；对数据分布中的噪声、异常值鲁棒性更好。-- kimi</li></ul></blockquote></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 生成模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语音对话数据集-合成</title>
      <link href="/blog/llm/yu-yin-dui-hua/shu-ju-ji/he-cheng/"/>
      <url>/blog/llm/yu-yin-dui-hua/shu-ju-ji/he-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="文本指令数据集-tts">1. 文本指令数据集 TTS</h1><ul><li><p>过滤或 LLM 改写：单次消息不超过 200 个词；剔除包含URL、数学表达式、代码、不常见符号的。删除括号中的注释内容。</p><p>或者 LLM 改写使符合口语对话。简短、语法结构简单，用户指令添加“uh”、“um” 等填充词。如 GLM-4-Voice</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa861c66cb68aab97da1984263f7083f6?method=download&shareKey=a315f86f5d5eb182c9c91846f006138e" width="607"></p></li><li><p>TTS。</p></li><li><p>ASR：保留 TTS 发音准确度较高的。</p></li><li><p><font color="green">组织为双声道音频：<a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/omniflatten/">对话由用户开始，随后交替用户与助手的对话。用户语音结束后，助手立即响应；助手语音结束后，采样暂停时长，以模拟自然的对话节奏。</a><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/moshi/">对用户音频流进行数据增强，如随机增益调整、加噪、回声（将助手音频流缩放后加到用户流上，缩放系数[0, 0.2]，延迟 [100, 500] ms、用户流 reverb-likeecho）。</a></font></p></li></ul><blockquote><ul><li>OmniFlatten 过滤含大量非文本元素（如代码、数学表达式）、中英文超过200 个单词、包含不常见的符号的。</li><li><font color="green"><a href="https://github.com/gpt-omni/mini-omni/issues/132">VoiceAssistant-400K疑似未合成括号里的内容，枚举示例、步骤的句子。</a></font></li><li>Moshi 认为文本指令数据集中的部分内容（如URL）、形式（如项目符号、长枚举）不适用于口语对话，采用在文本指令数据集、真实对话转写上微调LLM 来生成口语对话。</li><li>Llama 3.1 筛选提示相对较短、结构简单、没有非文本符号的对话。</li><li>AnyGPT 未介绍具体的筛选规则。</li></ul></blockquote><h1 id="对话生成">2. 对话生成</h1><ul><li><a href="https://nanyang2015.github.io/blog/llm/yu-yin-dui-hua/lun-wen/moshi/#%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90">Moshi</a></li><li>尽量包含简短的适合口语的对话、打断、backchannel。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>VAE</title>
      <link href="/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/vae/"/>
      <url>/blog/ji-qi-xue-xi/sheng-cheng-mo-xing/vae/</url>
      
        <content type="html"><![CDATA[<p>论文：Kingma D P, Welling M. Auto-encoding variational bayes[J].arXiv preprint arXiv:1312.6114, 2013.</p><p><font color="red">核心思想：通过 VariationalInference，联合学习数据的 latent 表示、生成过程。重参数化 latent后验分布，如高斯分布，引入独立的随机噪声 <span class="math inline">\(\epsilon\)</span>，latent <span class="math inline">\(z=\mu+\sigma \epsilon\)</span>，<span class="math inline">\(\epsilon \sim\mathcal{N}(0,1)\)</span>。可用随机梯度方法直接优化（<a href="#AEVB算法">AEVB算法</a>）。encoder、decoder预测变分分布的参数，如高斯分布中的均值、方差。latent、数据重建通过采样得到。</font></p><h1 id="问题场景">1. 问题场景</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB11563930f8a4c84b9ea1cbc334aa6b2d?method=download&shareKey=ac5dca106e2bc8def89852403177a5aa" width="918"></p><ul><li>有向概率图模型，独立同分布（大型）数据集，每个数据点有连续 latent变量，后验分布难以处理，如何高效地学习、推理？</li><li>数据集 <span class="math inline">\(\mathbf{X}=\left\{\mathbf{x}^{(i)}\right\}_{i=1}^N\)</span>，<span class="math inline">\(N\)</span> 个样本，连续/离散变量 <span class="math inline">\(\mathbf{x}\)</span>。</li><li>先验 <span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z})\)</span></li><li>likelihood <span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{x} \mid\mathbf{z})\)</span></li><li>后验 <span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})\)</span></li><li>marginal likelihood <span class="math inline">\(p_\theta(\mathbf{x})\)</span></li><li>真实后验的变分近似/识别模型/概率编码器：<span class="math inline">\(q_\phi(\mathbf{z} \mid \mathbf{x})\)</span></li><li>学习目标<ul><li>生成模型参数 <span class="math inline">\(\theta\)</span>，可用于 AI生成。</li><li><span class="math inline">\(q_\phi(\mathbf{z} \mid\mathbf{x})\)</span>，可用于编码、数据表示、可视化等。</li><li><span class="math inline">\(p(\mathbf{x})\)</span>，可用于图像去噪、修复、超分辨率等。</li></ul></li></ul><h1 id="变分下界">2. 变分下界</h1><p>目标函数 <span class="math display">\[\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ;\mathbf{x}^{(i)}\right)=-D_{KL}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid\mathbf{x}^{(i)}\right) \|p_{\boldsymbol{\theta}}(\mathbf{z})\right)+\mathbb{E}_{q_{\boldsymbol{\phi}}\left(\mathbf{z}\mid \mathbf{x}^{(i)}\right)}\left[\logp_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid\mathbf{z}\right)\right] \]</span></p><p>由于 KL 散度非负，所以 <span class="math inline">\(\mathcal{L}\left(\boldsymbol{\theta},\boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)\)</span> 称为 <span class="math inline">\(\logp_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\right)\)</span>的变分下界。</p><p>存在的问题：变分下界关于 <span class="math inline">\(\phi\)</span>的梯度 <span class="math inline">\(\nabla_\phi\mathbb{E}_{q_\phi(\mathbf{z})}[f(\mathbf{z})]\)</span>，若采用 MonteCarlo gradient estimator 通过采样的样本梯度的均值来近似，方差较大。</p><h1 id="重参数化">3. 重参数化</h1><ul><li>重参数化 <span class="math inline">\(\widetilde{\mathbf{z}} \simq_\phi(\mathbf{z} \mid \mathbf{x})\)</span> 为 <span class="math inline">\(\widetilde{\mathbf{z}}=g_\phi(\boldsymbol{\epsilon},\mathbf{x})\)</span> <span class="math inline">\(\quad\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})\)</span>，其中 <span class="math inline">\(\epsilon\)</span> 为独立的辅助噪声变量，<span class="math inline">\(g_\phi(\boldsymbol{\epsilon}, \mathbf{x})\)</span>可微分。</li><li>选择 <span class="math inline">\(g_\phi(.)\)</span> 和 <span class="math inline">\(\epsilon\)</span> 的 3 种基本方法：<ul><li><span class="math inline">\(\boldsymbol{\epsilon} \sim\mathcal{U}(\mathbf{0}, \mathbf{I})\)</span>，其中 <span class="math inline">\(I\)</span> 为单位向量，<span class="math inline">\(\mathcal{U}\)</span> 为均匀分布，<span class="math inline">\(g_\phi(\boldsymbol{\epsilon}, \mathbf{x})\)</span>为 <span class="math inline">\(q_\phi(\mathbf{z} \mid\mathbf{x})\)</span> 的逆 CDF（Cumulative DistributionFunction，累积分布函数）。</li><li><span class="math inline">\(\epsilon \sim\mathcal{N}(0,1)\)</span>，<span class="math inline">\(z=\mu+\sigma\epsilon\)</span>，<span class="math inline">\(z \sim p(z \midx)=\mathcal{N}\left(\mu, \sigma^2\right)\)</span>。</li><li><span class="math inline">\({\epsilon}\)</span>的多种变换的组合。</li></ul></li></ul><h1 id="sgvb-estimator">4. SGVB estimator</h1><ul><li>版本一 <span class="math display">\[ \begin{aligned}\tilde{\mathcal{L}}^A\left(\boldsymbol{\theta}, \boldsymbol{\phi} ;\mathbf{x}^{(i)}\right) &amp; =\frac{1}{L} \sum_{l=1}^L \logp_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}, \mathbf{z}^{(i,l)}\right)-\log q_{\boldsymbol{\phi}}\left(\mathbf{z}^{(i, l)} \mid\mathbf{x}^{(i)}\right) \\\text { where } \quad \mathbf{z}^{(i, l)} &amp;=g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}^{(i, l)},\mathbf{x}^{(i)}\right) \quad \text { and } \quad\boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})\end{aligned} \]</span></li><li>版本二 <span class="math display">\[ \begin{aligned}&amp; \widetilde{\mathcal{L}}^B\left(\boldsymbol{\theta},\boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=-D_{KL}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid\mathbf{x}^{(i)}\right) \|p_{\boldsymbol{\theta}}(\mathbf{z})\right)+\frac{1}{L}\sum_{l=1}^L\left(\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}\mid \mathbf{z}^{(i, l)}\right)\right) \\&amp; \text { where } \quad \mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}^{(i, l)},\mathbf{x}^{(i)}\right) \quad \text { and } \quad\boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})\end{aligned} \]</span><ul><li>KL 散度可解释为正则项。该 SGVB estimator 通常比一般的 estimator方差小。</li><li>第二项可理解为自编码器中的负重建误差期望。</li></ul></li><li><span class="math inline">\(g_\phi(\boldsymbol{\epsilon},\mathbf{x})\)</span> 可微分，SGVB 可用随机梯度方法直接优化。</li></ul><h1 id="aevb算法">5. AEVB算法</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7b6c74f67b0640b9303f5948dc9d61c9?method=download&shareKey=9b674c2c8117816d8b55fa94e88c25c3" width="916"></p><ul><li>只要 batch size 足够大（本文采用 <span class="math inline">\(M=100\)</span>），<span class="math inline">\(L\)</span> 可以为1。</li></ul><h1 id="vae">6. VAE</h1><ul><li><p>采用 1个隐藏层的 MLP 建模 encoder <span class="math inline">\(q_\phi(\mathbf{z} \mid\mathbf{x})\)</span>、decoder <span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{x} \mid\mathbf{z})\)</span>，通过 AEVB 算法联合优化参数 <span class="math inline">\(\phi\)</span>、<span class="math inline">\(\theta\)</span>。</p></li><li><p>假设先验为 centered 各向同性的多元高斯 <span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}) =\mathcal{N}(\mathbf{z} ; \mathbf{0}, \mathbf{I})\)</span>，通过 weightdecay 约束。</p></li><li><p>encoder：对角协方差结构的多元高斯输出。</p></li><li><p>decoder</p><ul><li><p>多元二进制输出 <span class="math display">\[\begin{aligned}\log p(\mathbf{x} \mid \mathbf{z}) &amp; =\sum_{i=1}^D x_i \logy_i+\left(1-x_i\right) \cdot \log \left(1-y_i\right) \\\text { where } \mathbf{y} &amp; =f_\sigma\left(\mathbf{W}_2 \tanh\left(\mathbf{W}_1 \mathbf{z}+\mathbf{b}_1\right)+\mathbf{b}_2\right)\end{aligned}\]</span></p><p>其中 <span class="math inline">\(f_\sigma(.)\)</span> 为 elementwisesigmoid 函数。</p></li><li><p>对角协方差结构的多元高斯（实数）输出 <span class="math display">\[ \begin{aligned}\log p(\mathbf{x} \mid \mathbf{z}) &amp; =\log\mathcal{N}\left(\mathbf{x} ; \boldsymbol{\mu}, \boldsymbol{\sigma}^2\mathbf{I}\right) \\\text { where } \boldsymbol{\mu} &amp; =\mathbf{W}_4\mathbf{h}+\mathbf{b}_4 \\\log \boldsymbol{\sigma}^2 &amp; =\mathbf{W}_5 \mathbf{h}+\mathbf{b}_5\\\mathbf{h} &amp; =\tanh \left(\mathbf{W}_3\mathbf{z}+\mathbf{b}_3\right)\end{aligned} \]</span></p><ul><li>高斯 MLP 作为 encoder，与上述类似。</li><li>其中，<span class="math inline">\(\boldsymbol{\mu}^{(i)}\)</span>、<span class="math inline">\(\boldsymbol{\sigma}^{(i)}\)</span> 为encoder、decoder 输出。</li><li>采用 sigmoid 将 decoder 均值约束到区间 (0,1)。</li></ul></li></ul></li><li><p>采样 <span class="math inline">\(\mathbf{z}\)</span>：<span class="math inline">\(\mathbf{z}^{(i,l)}=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)} \odot\boldsymbol{\epsilon}^{(l)}\)</span> where <span class="math inline">\(\boldsymbol{\epsilon}^{(l)} \sim\mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> 其中<span class="math inline">\(\odot\)</span> 表示 element-wiseproduct。</p></li><li><p>上述假设下的 KL 散度计算 <span class="math display">\[\begin{aligned}-D_{KL}\left(\left(q_{\boldsymbol{\phi}}(\mathbf{z}) \|p_{\boldsymbol{\theta}}(\mathbf{z})\right)\right. &amp; =\intq_{\boldsymbol{\theta}}(\mathbf{z})\left(\logp_{\boldsymbol{\theta}}(\mathbf{z})-\logq_{\boldsymbol{\theta}}(\mathbf{z})\right) d \mathbf{z} \\&amp; =\int q_{\boldsymbol{\theta}}(\mathbf{z}) \log p(\mathbf{z}) d\mathbf{z} - \int q_{\boldsymbol{\theta}}(\mathbf{z}) \logq_{\boldsymbol{\theta}}(\mathbf{z}) d \mathbf{z} \\&amp; =\int \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu},\boldsymbol{\sigma}^2\right) \log \mathcal{N}(\mathbf{z} ; \mathbf{0},\mathbf{I}) d \mathbf{z} - \int \mathcal{N}\left(\mathbf{z} ;\boldsymbol{\mu}, \boldsymbol{\sigma}^2\right) \log\mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu},\boldsymbol{\sigma}^2\right) d \mathbf{z} \\&amp; = -\frac{J}{2} \log (2 \pi)-\frac{1}{2}\sum_{j=1}^J\left(\mu_j^2+\sigma_j^2\right)+\frac{J}{2} \log (2\pi)+\frac{1}{2} \sum_{j=1}^J\left(1+\log \sigma_j^2\right) \\&amp; =\frac{1}{2} \sum_{j=1}^J\left(1+\log\left(\left(\sigma_j\right)^2\right)-\left(\mu_j\right)^2-\left(\sigma_j\right)^2\right)\end{aligned}\]</span></p><p>其中，<span class="math inline">\(J\)</span> 为 <span class="math inline">\(\mathbf{z}\)</span> 的维数，<span class="math inline">\(\mu_j\)</span>、<span class="math inline">\(\sigma_j\)</span> 为向量的第 <span class="math inline">\(j\)</span> 个元素。</p></li><li><p>本文对全局参数采用最大似然/后验推理，对潜变量采用变分推理。也可推广至对参数、潜变量都采用变分推理，见附录。</p></li><li><p><font color="green">可用于流式、非平稳数据。</font></p></li></ul><h1 id="评价">7. 评价</h1><p>图像生成，对比变分下界或 <span class="math inline">\(p_\theta(\mathbf{x})\)</span>（latent维度极低时）。</p><h1 id="术语">8. 术语</h1><ul><li><p>变分：选择简化的、可微分的变分分布，来近似复杂的真实后验分布，并通过最小化两者的差异来优化变分分布的参数。</p></li><li><p>Bayesian Inference：<span class="math display">\[p(\theta \midD)=\frac{p(D \mid \theta) \cdot p(\theta)}{p(D)}\]</span></p><p>其中，<span class="math inline">\(\theta\)</span> 是参数，<span class="math inline">\(D\)</span> 是数据，<span class="math inline">\(p(\theta)\)</span> 是先验知识，<span class="math inline">\(p(\theta \mid D)\)</span> 是后验分布。</p></li><li><p>VB：Variational Bayes，变分贝叶斯</p></li><li><p>SGVB：Stochastic Gradient VB</p></li><li><p>AEVB：Auto-Encoding VB</p></li><li><p>VAE：variational auto-encoder</p></li><li><p>ML：maximum likelihood</p></li><li><p>MAP：maximum a posteriori</p></li><li><p>i.i.d.：independently and identicallydistributed，独立同分布</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 生成模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【综述】MM-LLMs</title>
      <link href="/blog/llm/duo-mo-tai/lun-wen/mm-llms/"/>
      <url>/blog/llm/duo-mo-tai/lun-wen/mm-llms/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：腾讯</li><li>MM-LLMs：利用预训练的LLM，添加多模态理解、生成能力，以节省训练成本。</li></ul><h1 id="模型架构">1. 模型架构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcb12cf55d3ce2b9bf66210923e52e1d7?method=download&shareKey=a3e24e6823eb1eeec7ec5c1c22385282" width="1568"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2d2861e84459234e2659bef4e9c5c3a3?method=download&shareKey=ceced6a9c135f61e2aa83705ba1b932c" width="2096">通常模态 encoder、LLM backbone、模态生成器参数冻结，重点优化输入、输出投影层。</p><h2 id="模态encoder">1.1. 模态encoder</h2><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>模态</th><th>encoder</th></tr></thead><tbody><tr><td>图像</td><td>ViT、CLIP ViT、Eva-CLIPViT、BEiT-3、Open-CLIP、Grounding-DINO-T、DINOv2、SAM-HQ、RAM++、InternViT、VCoder等</td></tr><tr><td>视频</td><td>采样为多帧图像，与图像的预处理相同</td></tr><tr><td>音频</td><td>C-Former、HuBERT、BEATs、Whisper、CLAP 等</td></tr><tr><td>3D 点云</td><td>ULIP-2 (+ Point-BERT)</td></tr><tr><td>图像、视频、文本、音频、heat map、inertial measurementunits、depth</td><td>ImageBind</td></tr></tbody></table><h2 id="输入投影层">1.2. 输入投影层</h2><ul><li><p>线性层、MLP、cross-attention、Q-Former、P-Former、MQ-Former等。</p></li><li><p>训练：采用 X-text 数据集，最小化文本生成损失。 <span class="math display">\[ \boldsymbol{P}_X=\boldsymbol{\Theta}_{X\rightarrow T}\left(\boldsymbol{F}_X\right) \]</span></p><p><span class="math display">\[ \underset{\Theta_{X \rightarrowT}}{\arg \min } \mathcal{L}_{\text{txt-gen}}\left(\operatorname{LLM}\left(\boldsymbol{P}_X,\boldsymbol{F}_T\right), t\right) \]</span></p><p>其中，<span class="math inline">\(\boldsymbol{F}_X\)</span>为其它模态 encoder 的特征，<span class="math inline">\(\boldsymbol{P}_X\)</span>为输入投影层的表示，<span class="math inline">\(\boldsymbol{F}_T\)</span> 为输入的文本特征，<span class="math inline">\(t\)</span> 为文本 label。</p></li></ul><h2 id="llm">1.3. LLM</h2><ul><li>Parameter-Efficient Fine-Tuning(PEFT)：Prefix-tuning、Adapter、LoRA、LayerNorm tuning 等。</li></ul><h2 id="输出投影层">1.4. 输出投影层</h2><ul><li>训练：采用 X-text 数据集，最小化输出投影层的表示、模态生成器的textual condition encoder 的表示的距离。 <span class="math display">\[\underset{\boldsymbol{\Theta}_{T \rightarrow X}}{\arg \min }\mathcal{L}_{\mathrm{mse}}\left(\boldsymbol{H}_X, \tau_X(t)\right)\]</span></li></ul><h2 id="模态生成器">1.5. 模态生成器</h2><ul><li><p>Latent Diffusion Models (LDMs)</p><ul><li>图像：Stable Diffusion</li><li>视频：Zeroscope</li><li>音频：AudioLDM-2</li></ul></li><li><p>训练 将 ground truth 通过预训练的 VAE 转换为 latent 特征 <span class="math inline">\(z_0\)</span>，加噪 <span class="math inline">\(\epsilon\)</span>，得到噪声 latent 特征 <span class="math inline">\(z_t\)</span>，采用预训练的 Unet，通过优化损失</p><p><span class="math display">\[ \mathcal{L}_{\mathrm{X} \text{-gen}}:=\mathbb{E}_{\epsilon \sim \mathcal{N}(0,1),t}\left\|\epsilon-\epsilon_X\left(z_t, t,\boldsymbol{H}_X\right)\right\|_2^2 \]</span></p><p>来优化参数 <span class="math inline">\(\Theta_{X \rightarrowT}\)</span>、<span class="math inline">\(\Theta_{T \rightarrowX}\)</span>。</p></li></ul><h1 id="训练">2. 训练</h1><h2 id="模态对齐预训练">2.1. 模态对齐预训练</h2><p>对于多模态生成模型，输入投影层的优化目标还包含 ground truth 多模态token 序列。</p><h2 id="指令微调">2.2. 指令微调</h2><ul><li>增强对话交互能力，更符合人类意图。</li><li>Supervised Fine-Tuning(SFT)：将部分预训练阶段的数据转换为指令格式。</li><li>Reinforcement Learning from Human Feedback (RLHF)。</li></ul><h2 id="经验">2.3. 经验</h2><ul><li>高质量的 SFT 数据可以显著提升特定任务的性能。</li><li>更高的分辨率有利于需要视觉细节的任务，如对富文本图像、表格、文档内容的理解。另外，需要处理高分辨率导致的序列长度问题。</li><li>VILA<ul><li>对 LLM 进行 PEFT，可以促进深层表示对齐，特别是 In-Context Learning(ICL)。</li><li>相较于图像-文本对，图文交错的数据更有益。</li><li>SFT中，纯文本指令数据与多模态指令数据混合，不仅可以缓解纯文本任务的退化，还能提升多模态任务的性能。</li></ul></li></ul><h1 id="数据集">3. 数据集</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB70dd2ec2a24445592a2d301e6f9e06a4?method=download&shareKey=f5df9a327d406f34d530cd591875dc95" width="1518"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB921a8568c36affbda4eb3c25e3fefcfc?method=download&shareKey=c8eee3f623fac1841335d5e30926bc70" width="2080"></p><h1 id="benchmark">4. benchmark</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB46c638f8fc83e29f66da98657d9f0e77?method=download&shareKey=f03cf409b22a6b8caefa59e382d27a0d" width="2082"></p><h1 id="未来的方向">5. 未来的方向</h1><ul><li>研究趋势<ul><li>多模态理解 -&gt; 多模态生成 -&gt; any-to-any 生成。</li><li>训练 pipeline 逐步完善：预训练 -&gt; SFT -&gt; RLHF。</li><li><font color="green">多样化的模态扩展。</font></li><li>更高质量的训练数据集。</li><li>更高效的模型架构，如 Q/P-Former -&gt; 线性输入投影层。</li></ul></li><li>更通用、智能的模型<ul><li>扩展模态，如图表、网页等。</li><li>各种类型、规模的LLMs，使得可以根据具体需求选择。</li><li>高质量的指令集，多样化的指令。</li><li>增强多模态生成能力。</li></ul></li><li>benchmark</li><li>移动设备/轻量级部署。如MobileVLM、TinyGPT-V、Vary-toy、Mobile-Agent、MoE-LLaVA、MobileVLMV2。</li><li>具身智能。如 PaLM-E、EmbodiedGPT。</li><li>持续学习。如<ul><li>He J, Guo H, Tang M, et al. Continual instruction tuning for largemultimodal models[J]. arXiv preprint arXiv:2311.16206, 2023.</li><li>挑战：灾难性遗忘；negative forward transfer学习新任务时在未见任务上性能下降。</li></ul></li><li>减少幻觉。如<ul><li>Han Z, Bai Z, Mei H, et al. Skip <span class="math inline">\(\backslash n\)</span> : A simple method to reducehallucination in Large Vision-Language Models[J]. arXiv preprintarXiv:2402.01345, 2024.</li></ul></li><li>偏见和伦理。</li></ul><h1 id="其它">6. 其它</h1><ul><li><p>Zhang D, Yu Y, Li C, et al. Mm-llms: Recent advances inmultimodal large language models[J]. arXiv preprint arXiv:2401.13601,2024.</p></li><li><p><a href="https://mm-llms.github.io/" class="uri">https://mm-llms.github.io/</a></p></li><li><p>MM-LLMs</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe4ffd5049651e7dd9fa60101c531f7e9?method=download&shareKey=ea622ff90b539918b59cf471b9ef1102" width="988"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdd8dd8d6495a66dfe3a1a910c88583bc?method=download&shareKey=23775301830797435d585d02fdd686c2" width="1564"></p><ul><li>Video-LLaMA：多分支跨模态预训练，在对话中同时处理视频、音频。</li><li>BuboGPT：学习图像、音频、文本 共享的语义空间。</li></ul></li><li><p>相关工作</p><ul><li>Song S, Li X, Li S. How to bridge the gap between modalities: Acomprehensive survey on multimodal large language model[J]. arXivpreprint arXiv:2311.07594, 2023.</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>X-VILA</title>
      <link href="/blog/llm/duo-mo-tai/lun-wen/x-vila/"/>
      <url>/blog/llm/duo-mo-tai/lun-wen/x-vila/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：NVIDIA 等</li><li>功能：支持 文本、图像、视频、音频 理解、推理与生成。</li><li>创新点<ul><li>在 vision-to-vision 生成任务中，由于将 visual encoder 表示投影到 LLM的表示空间, 而 LLM 更倾向于表征语义概念，丢失了视觉细节。提出 VisualEmbedding Highway (VEH)，连接 visual encoder 与 decoder，以增强 visual输入、输出的一致性。</li><li><a href="#instruct_dataset">any-to-any 模态交错的指令集</a>。</li><li><a href="#训练">3 阶段对齐训练、指令微调。</a></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb19b3e46908c9bb83c27339692e38f64?method=download&shareKey=fe233bdf660dfb24a41a92f6d16653bc" width="688"></p><ul><li>encoder<ul><li>文本：tokenizer</li><li>音频、图像、视频：<font color="green">ImageBind</font></li></ul></li><li>LLM：Vicuna-7B-1.5</li><li><font color="red">decoder：扩充 LLM 词表，添加特定模态的token。提取生成的 token 序列对应的 embedding 序列，通过 transformer层投影到 decoder 的文本 encoder 特征空间。decoder 为 diffusion模型</font>，cross attention 文本 encoder 表示。<ul><li><font color="green">音频：AudioLDM</font></li><li><font color="green">图像：Stable Diffusion 1.5</font></li><li><font color="green">视频：VideoCrafter2</font></li></ul></li></ul><h2 id="veh">1.1. VEH</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB22b5a0fcbaedbb990060b172b6279699?method=download&shareKey=402f196c43f5336e10dde76d3acbb1bd" width="348"></p><ul><li><p>VEH embedding <span class="math inline">\(\mathbf{E}^{\mathrm{vis}}\)</span>：visual encoderfeature map 逐层相加。</p></li><li><p>RB：2个残差卷积。</p></li><li><p>ZC：全0初始化，生成输出控制信号。</p></li><li><p>visual decoder 在 reverse step <span class="math inline">\(t\)</span>： <span class="math display">\[\epsilon^p=\left\{\begin{array}{ll}\mathrm{U-Net}_m\left(z(t),\operatorname{VisCtrl}_m\left(\mathbf{E}^{\text {vis}}\right),\mathbf{E}_m^{\text {text}}\right) &amp; \text { if } t&lt;T \times\alpha \\ \mathrm{U-Net}_m\left(z(t), \text {Null}, \mathbf{E}_m^{\text{text}}\right) &amp; \text { if } t \geq T \times \alpha\end{array}, m\in\{\text {&#39;image&#39;, &#39;video&#39; }\}\right. \]</span></p><p>其中，<span class="math inline">\(\epsilon^p\)</span> 为预测的noise，<span class="math inline">\(z(t)\)</span> 为输入 latent，<span class="math inline">\(T\)</span> 为 diffusion 步数，NULL 表示不输入 VEH特征。</p></li><li><p>效果<img src="https://note.youdao.com/yws/api/personal/file/WEBfcb2d7243d6b8c522f392b43c32e48f5?method=download&shareKey=f66528da1d69b3ab2a9e812780a224cf" width="345"></p></li><li><p><font color="green">参考文献</font></p><ul><li>Mou C, Wang X, Xie L, et al. T2i-adapter: Learning adapters to digout more controllable ability for text-to-image diffusionmodels[C]//Proceedings of the AAAI Conference on ArtificialIntelligence. 2024, 38(5): 4296-4304.</li><li>Zhang L, Rao A, Agrawala M. Adding conditional control totext-to-image diffusion models[C]//Proceedings of the IEEE/CVFInternational Conference on Computer Vision. 2023: 3836-3847.</li></ul></li></ul><h1 id="训练">2. 训练</h1><ul><li>encoder-LLM-decoder 文本对齐训练<ul><li>数据集：LLaVA-pretrain, cc3m, WebVid, AudioCaps, WavCaps，X-text对，字幕。</li><li>可训练参数：输入投影层、LLM embedding、输出投影层。进行X-to-text、text-to-X 训练。</li><li>训练目标：text-to-X：最小化输出投影层表示、diffusion模型原始预训练的 text encoder 的表示的距离。</li></ul></li><li>模态交错数据 预训练<ul><li>数据集：MMC4 图文交错、ActivityNet Captions<ul><li><p>ActivityNetCaptions：视频，自建多模态交错数据，构建方式如下图。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa3ccea66726fe842f39040d5d087b095?method=download&shareKey=7cc2c07cac8ce92b2956346088ebc51e" width="583"></p></li></ul></li><li>训练：输入投影层、输出投影层、LLM LoRA。</li><li><font color="green">作用：减轻灾难性遗忘、长上下文理解能力。</font></li></ul></li><li>X-to-X 跨模态指令微调<ul><li><p><span id="instruct_dataset">数据集</span>：基于视频字幕数据集构建。每个对话包含至少1个跨模态问答对。</p><table><thead><tr><th>数据集</th><th>模态</th><th>训练集样本量</th><th>验证集样本量</th></tr></thead><tbody><tr><td>WebVid <br> 随机挑选 500k 训练样本</td><td>image-to-video</td><td>499915</td><td>100</td></tr><tr><td></td><td>video-to-image</td><td>499915</td><td>100</td></tr><tr><td></td><td>video-to-video</td><td>499915</td><td>100</td></tr><tr><td></td><td>audio-to-video</td><td>32874</td><td>62</td></tr><tr><td></td><td>video-to-audio</td><td>32874</td><td>62</td></tr><tr><td></td><td>image+audio-to-video</td><td>32874</td><td>62</td></tr><tr><td>ActivityNet Captions</td><td>image-to-video</td><td>10009</td><td>100</td></tr><tr><td></td><td>video-to-image</td><td>10009</td><td>100</td></tr><tr><td></td><td>video-to-video</td><td>10009</td><td>100</td></tr><tr><td>LLaVA</td><td></td><td></td><td></td></tr><tr><td>VideoChat</td><td></td><td></td><td></td></tr><tr><td>NextGPT-instructions</td><td></td><td></td><td></td></tr><tr><td>Alpaca</td><td></td><td></td><td></td></tr></tbody></table><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8ab6731ea1ed8be193524079981dd8a6?method=download&shareKey=caff80afacfc351248f2ab9a71d2e21f" width="680"></p></li><li><p>训练：两阶段</p><ul><li><p>文本对齐</p><ul><li>对于图像、视频、音频输出，优化模型生成的 <span class="math inline">\(\mathbf{E}_m^{\text {text}}\)</span> 与 diffusion模型 encoder 表示的距离。</li><li>可训练参数：输入投影层、输出投影层、LLM embedding、LoRA。</li></ul></li><li><p>视觉对齐</p><ul><li>可训练参数：视觉控制模块、视觉 decoder。</li></ul></li></ul></li></ul></li></ul><h1 id="评价">3. 评价</h1><ul><li>评价指标：X-to-X Alignment Score (<span class="math inline">\(\mathrm{X}^2 \mathrm{A}\)</span> Score)<ul><li>采用 ImageBind transformer 分别提取 ground truth、预测的embedding，计算余弦相似度。</li></ul></li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>多模态 benchmark</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB54d5bdba1496d0df7462af0fc904f899?method=download&shareKey=80351389a42037c388d6e3fef707d998" width="346"></td><td>与专门的 VLM 性能接近。</td></tr><tr><td>VEH 有效性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2d329a4ef43a6d7cc3fa41c07a9f9d29?method=download&shareKey=47746f46dae7861d8946734c4cc87256" width="1368"><img src="https://note.youdao.com/yws/api/personal/file/WEB9addfdcc7366d6aff522ed73794b2641?method=download&shareKey=dfacbf22168dea9c6fb4d605d8866525" width="1370"></td><td>相较于 Next-GPT，有显著的性能提升。<br> 添加VEH，图像、视频生成的一致性增强。</td></tr><tr><td>VEH <span class="math inline">\(\alpha\)</span></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB11999459a95d5a026a0aa5774734dae9?method=download&shareKey=7526a857502253d23b9a685ec80bbe0e" width="1364"></td><td>随 <span class="math inline">\(\alpha\)</span>增大，生成图像的一致性增强。</td></tr><tr><td>LLM 输出与 decoder 对齐<br>1. 文本对齐：LLM生成文本描述；<br>2.Text-Embed-Aligned: LLM 生成特定模态的token，取对应的 embedding；<br>3.+ VEH</td><td>结果见上图</td><td>文本对齐：纯文本描述很难传达视觉风格、对象外观、精确的人类动作等具体细节。embedding提供了更大的信息量。</td></tr></tbody></table><ul><li>涌现能力<ul><li>组合不同模态的多个输入，生成一致的内容。</li><li>图像-音频、音频-图像 生成。<img src="https://note.youdao.com/yws/api/personal/file/WEB9dcee95ecfbee6f329ee2b83e9a9ae28?method=download&shareKey=5eaebb20c7d59f98b27d75d1b6afc4b3" width="692"></li></ul></li></ul><h1 id="其它">4. 其它</h1><ul><li>Ye H, Huang D A, Lu Y, et al. X-VILA: Cross-Modality Alignment forLarge Language Model[J]. arXiv preprint arXiv:2405.19335, 2024.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AnyGPT</title>
      <link href="/blog/llm/duo-mo-tai/lun-wen/anygpt/"/>
      <url>/blog/llm/duo-mo-tai/lun-wen/anygpt/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：复旦大学等。</li><li>贡献<ul><li>基于离散 token，实现了 any-to-any 多模态LLM，即支持理解、生成各种模态，包含语音、文本、图像、音乐。以文本为中心，通过将其它模态与文本对齐，实现所有模态之间的相互对齐。</li><li><font color="red">any-to-any 多模态交叉指令数据集AnyInstruct-108k：108k 多轮对话样本，交替各种模态，约 205k 图像、503k语音、113k 音乐。</font></li><li>zero-shot 性能接近各模态的专业模型（可能的优化方案：MOE）。</li><li><font color="red">指令集、模型、推理代码 <a href="https://junzhan2000.github.io/AnyGPT.github.io/" class="uri">https://junzhan2000.github.io/AnyGPT.github.io/</a></font></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB436e3f864d535f6b821f2f970b44e0ef?method=download&shareKey=c11e72ae89e80223520d040165dd091b" width="715"></p><h2 id="tokenization">1.1. Tokenization</h2><ul><li><font color="red">图像：SEED + diffusion</font></li><li><font color="red">语音：SpeechTokenizer + SoundStorm</font><ul><li>SpeechTokenizer<ul><li>residual vector quantization (RVQ)结构，<font color="green">第一层捕捉语义，第 2-8 层编码副语言细节。LLM只对语义 token 进行建模，语音词表大小 1024。副语言信息采用 voice clone模型。AnyGPT 可以根据 3s 的语音提示复制任意说话人的声音。</font></li><li>在 Commonvoice、Librispeech 上预训练。20ms 帧率。</li></ul></li><li>SoundStorm<ul><li>MLM，根据 SpeechTokenizer 的语义 token 生成声学 token。然后SpeechTokenizer decoder 将所有语音 token 转换为 raw 音频。</li><li>在 Multilingual LibriSpeech 上训练。</li></ul></li></ul></li><li><font color="red">音乐：<a href="https://huggingface.co/facebook/encodec_32khz">Encodec</a></font>，卷积自编码器，encoder+ decoder，RVQ 潜空间量化。<ul><li>在 2w 乐曲上预训练，输入 32k 单声道音乐，20ms 帧率。RVQ 量化，4 *2048。</li><li><font color="green">音乐生成限制为 5s。</font></li></ul></li><li>离散 token过滤了高频的、特定模态的感知信息，保留了必要的低频语义信息。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2fa1bf645d5680429cfba93adc3cafc2?method=download&shareKey=067c38c16000dadd9331d733a4e8f494" width="258"></p><h2 id="llm">1.2. LLM</h2><p><font color="red">LLaMA-2 7B</font></p><h1 id="数据集">2. 数据集</h1><h2 id="多模态对齐预训练">2.1. 多模态对齐预训练</h2><table style="width:100%;"><colgroup><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"></colgroup><thead><tr><th>模态</th><th></th><th>数据集</th><th>数据量</th><th>描述</th><th>采样概率</th></tr></thead><tbody><tr><td>图片 - 文本</td><td>低质量</td><td>LAION-2B</td><td>2B</td><td>从网上获取的 noisy alt-text</td><td>0.3</td></tr><tr><td></td><td></td><td>LAION-COCO</td><td>600M</td><td>LAION-2B 的子集，用 BLIP 生成字幕</td><td></td></tr><tr><td></td><td>高质量<br>用于提高图像生成的保真度</td><td>JouneyDB</td><td>4429K</td><td>Midjourney 生成</td><td></td></tr><tr><td></td><td></td><td>LAION-Aesthetics</td><td>-</td><td>LAION 5B 的高质量子集</td><td></td></tr><tr><td></td><td>模态交错</td><td>MMC4-core-ff (Multimodal-C4)</td><td>7.3M 文档</td><td>从 Common Crawl 收集的 101M 图文交错的文档</td><td>0.05</td></tr><tr><td>语音-文本</td><td>英语 ASR</td><td>Multilingual LibriSpeech</td><td>4.4wh</td><td></td><td>0.13</td></tr><tr><td></td><td></td><td>Common Voice</td><td>3kh</td><td></td><td>0.27</td></tr><tr><td></td><td></td><td>Gigaspeech</td><td>1wh</td><td></td><td></td></tr><tr><td>音乐-文本</td><td></td><td>Youtube-Music-1M</td><td>100M</td><td>Spotify，收集音乐音频、视频标题、描述、关键词、播放列表名称、歌词。将上述元数据输入GPT-4，生成简洁、连贯的标题，减少幻觉。</td><td>0.25</td></tr><tr><td></td><td></td><td>MusicGen-Synthesis</td><td>20k</td><td>来自 AnyInstruct-108k 数据集</td><td></td></tr></tbody></table><ul><li>图片 - 文本 低质量数据集 数据清洗 根据文本质量、图像 aspectratio、clip score 等，产生 300M 对。</li></ul><h2 id="dataloader">2.2. dataloader</h2><ul><li>对数据量少的模态过采样。</li><li>同一数据集的样本拼接为长序列，直到最大序列长度。</li><li>模板<ul><li>由 GPT-4 生成数百条双向指令，多样化，如<code>Please generate an image based on the provided text.</code>。</li><li>格式<code>[Human]: &#123;I&#125;.&#123;S&#125;&lt;eoh&gt;. [AnyGPT]: &#123;T&#125;&lt;eos&gt;</code><code>[Human]: &#123;I&#125;. This is input:&#123;T&#125;&lt;eoh&gt;. [AnyGPT]: &#123;S&#125;&lt;eos&gt;</code>其中，I 表示指令，S 表示非文本 token，T 表示文本。<ul><li>图文交错的文档，直接用 token 序列替换非文本内容。</li></ul></li><li>非文本模态 token 首尾有 special token。</li></ul></li></ul><h2 id="指令集">2.3. 指令集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB60ebd3a144c7cf7c44a0bc0148a7d925?method=download&shareKey=d653d84b187f916aece00b1519b24c7a" width="1810"></p><ul><li>【主题】基于与视听相关的100个元主题，采用 GPT-4 扩展到 2w 个。<img src="https://note.youdao.com/yws/api/personal/file/WEBdcd33d92f7fc311374afbc87ff508c33?method=download&shareKey=47cb053ecdd3bba7dfad5d6fe2910556" width="711"></li><li>【场景】准备包含尽可能多的模态组合的示例，采样，作为示例提示 LLM生成基于主题的对话场景。<img src="https://note.youdao.com/yws/api/personal/file/WEBf5dabb56f4f3541e19537c261990be15?method=download&shareKey=bdc90db057c01773833b46d22d633c47" width="712"></li><li>【对话】提供示例，用 GPT-4生成基于场景的多轮对话，图像、音乐等用详细的文本描绘。<img src="https://note.youdao.com/yws/api/personal/file/WEB4276f1e30d2eb13305f5960512634fd5?method=download&shareKey=0731046843fa6e3ac456f88f0ea9e0d4" width="536"></li><li>【Text-to-Multimodality 转换】<ul><li>OpenAI DALL-E-3 生成图像</li><li>MusicGen 音乐作曲</li><li>Microsoft Azure TTS 合成用户指令、模型的响应。</li></ul></li><li>从现有的纯文本指令数据集中提取适合口语的对话，TTS，100k。</li></ul><h1 id="训练">3. 训练</h1><p>由于训练数据含噪声，影响生成质量，在预训练后，采用高质量数据继续预训练4k 步。</p><ul><li>文本 - 图片：JouneyDB、LAION-Aesthetics。</li><li>图片 - 文本：LAION-COCO。</li><li>音乐：AnyInstruct-108k<img src="https://note.youdao.com/yws/api/personal/file/WEBbbb79fe94bfc41ec8a7113ba7364bac5?method=download&shareKey=fac4dccc7555914869ae0cf95384561e" width="474"></li></ul><h1 id="评价">4. 评价</h1><ul><li>解码策略<img src="https://note.youdao.com/yws/api/personal/file/WEB2f2b9467db62d315cdd387f572d53592?method=download&shareKey=a81f36c8b17e86b51d8557dfecbb98cb" width="532"></li><li>zero-shot</li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>任务</th><th>测试集</th><th>结果</th></tr></thead><tbody><tr><td>图像理解</td><td>MS-COCO 2014 captioning benchmark Karpathy split testset</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2e82bda53cca725a5866d7ffea287c8c?method=download&shareKey=64b7b3a67bd313eee2fa202482eca5db" width="570"></td></tr><tr><td>图像生成</td><td>MS-COCO validation set，随机选 30k 图像</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB665e85fd658b9c419d393aa4e2f133f0?method=download&shareKey=a5bd2764ef2fe772ef3a19a54032da47" width="569"><br>*采用CLIP-ViT-L</td></tr><tr><td>ASR</td><td>LibriSpeech test-clean</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcf059c83c071d9a8c3d44afb529f8bdf?method=download&shareKey=9a01191a580d1e231699bd38d77f7720" width="507"></td></tr><tr><td>TTS</td><td>VCTK<br>3s 的说话人语音提示</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa9ca38ee011dfd8f2c22d4efd8e0843b?method=download&shareKey=f3666e9f88ba06e029ca837cc69bf3a4" width="277"><br><em>WER：采用Whisper medium 识别<br></em>SIM：说话人相似度，采用 <a href="https://github.com/yangdongchao/UniAudio/blob/main/UniAudio/tools/evaluation/compute_similarity_vc.py">WavLM-TDNN</a>提取说话人 embedding，计算余弦相似度。</td></tr><tr><td>音乐理解、生成</td><td>MusicCaps</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBdf9303d7cff4d6dd1d0bf4cab1ca5316?method=download&shareKey=ca0b11bbd70ef45bfae6b116704c5f86" width="571"></td></tr></tbody></table><h1 id="未来工作">5. 未来工作</h1><ul><li>相较于单模态，性能有损。可能的优化方案：MOE。</li><li><font color="red">tokenizer的质量设定了模型理解、生成潜力的上限。</font></li></ul><h1 id="其它">6. 其它</h1><ul><li>Zhan J, Dai J, Ye J, et al. AnyGPT: Unified Multimodal LLM withDiscrete Sequence Modeling[J]. arXiv preprint arXiv:2402.12226,2024.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 多模态 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DeepSpeed Compression</title>
      <link href="/blog/ji-qi-xue-xi/gong-ju-bao/deepspeed/compression/"/>
      <url>/blog/ji-qi-xue-xi/gong-ju-bao/deepspeed/compression/</url>
      
        <content type="html"><![CDATA[<p><font color="green"><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/" class="uri">https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/</a></font></p><p>支持的量化技术包含：</p><h1 id="减少层数">1. 减少层数</h1><ul><li>可选是否采用知识蒸馏。</li><li>收益：推理延迟线性减小。</li></ul><h1 id="权重量化">2. 权重量化</h1><ul><li>支持分组权重量化。<ul><li>Shen S, Dong Z, Ye J, et al. Q-bert: Hessian based ultra lowprecision quantization of bert[C]//Proceedings of the AAAI Conference onArtificial Intelligence. 2020, 34(05): 8815-8821.</li></ul></li></ul><h1 id="激活量化">3. 激活量化</h1><ul><li><font color="green"><a href="https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b" class="uri">https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b</a></font></li></ul><h1 id="剪枝">4. 剪枝</h1><p>通过删除模型连接来减少计算量。</p><h2 id="稀疏剪枝">4.1. 稀疏剪枝</h2><ul><li>将权重矩阵中的某些元素置为 0。可以<a href="https://arxiv.org/abs/1506.02626">基于权重的绝对值</a> 或<a href="https://arxiv.org/abs/1810.02340">基于权重被 mask 时 loss的变化</a>。</li><li><font color="green">支持的方法：L1 norm, topk,snip_momentum。</font></li></ul><h2 id="行剪枝">4.2. 行剪枝</h2><ul><li>将权重矩阵中某些行的所有元素置为零。</li><li>相较于稀疏剪枝，有利于硬件加速，但准确率损失可能更大。</li></ul><h2 id="head-剪枝">4.3. head 剪枝</h2><blockquote><p>训练模型后，测试时删除许多head，没有显著影响性能，甚至在某些情况下，删除少量 head 性能更好。</p><p><a href="https://arxiv.org/abs/1905.09418" class="uri">https://arxiv.org/abs/1905.09418</a></p></blockquote><ul><li>涉及 multi-head attention 中的 QKV、输出矩阵。</li></ul><h2 id="channel-剪枝">4.4. channel 剪枝</h2><p>应用于卷积层。</p><h1 id="zero-cost-quantization-zeroquant">5. zero-cost quantization(ZeroQuant)</h1><ul><li>包含权重、激活量化。支持 INT8、INT4/INT8 混合精度量化。</li><li>逐层知识蒸馏。</li><li><font color="red">训练后量化，不需要量化感知训练。</font></li><li><font color="green"><a href="https://arxiv.org/abs/2206.01861" class="uri">https://arxiv.org/abs/2206.01861</a></font></li></ul><h1 id="extreme-compression-xtc">6. extreme compression (XTC)</h1><ul><li><font color="red">极低 bit 量化。训练中量化。</font>比如对激活应用8bit 量化，QKV、feedforward 权重、embedding 应用 1bit 量化。</li><li>支持层减少，1bit、2bit 量化。</li><li><font color="red">发现 FP16 训练下，<code>quantize_groups</code>较小时（如1、2）训练可能不稳定，建议设为较大值，如 64。</font></li><li><font color="green"><a href="https://arxiv.org/abs/2206.01859" class="uri">https://arxiv.org/abs/2206.01859</a></font></li></ul><h1 id="mixture-of-quantization-moq">7. Mixture-of-Quantization (MoQ)</h1><ul><li><font color="red">在量化感知训练中，先以高精度量化模型，然后按预定义的计划降低精度，直到达到目标量化位数。</font></li><li>支持使用模型参数的二阶信息分别动态调整各层（可指定部分层）的量化计划，以充分地适应性训练。默认为指定减少bit 数的训练步数 <code>quantize_period</code>，每减少 1bit上述步数加倍。<ul><li>训练速度将变慢。</li><li>动态调整至多将所需的步数增大到5倍，当与原本的“每减少 1bit步数加倍”结合时，可能变得非常大。因此，建议设置相对较小的<code>quantize_period</code>，以在训练结束前经历所有的精度减少阶段。</li></ul></li><li><font color="green"><a href="https://www.deepspeed.ai/2021/05/04/MoQ.html" class="uri">https://www.deepspeed.ai/2021/05/04/MoQ.html</a></font></li></ul><h1 id="配置">8. 配置</h1><ul><li>权重量化<ul><li><font color="green">对称或非对称量化</font></li></ul></li><li>激活量化<ul><li><font color="green">动态或静态。</font></li></ul></li></ul><h1 id="其它">9. 其它</h1><p><font color="green">结合 DeepSpeed高度优化的推理引擎，消除量化/反量化开销。</font></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 工具包 </category>
          
          <category> DeepSpeed </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>transformers Streamer</title>
      <link href="/blog/ji-qi-xue-xi/gong-ju-bao/transformers/streamer/"/>
      <url>/blog/ji-qi-xue-xi/gong-ju-bao/transformers/streamer/</url>
      
        <content type="html"><![CDATA[<p>不能用于 beam search。</p><p><font color="red"> <a href="https://github.com/huggingface/transformers/blob/v4.42.0/src/transformers/generation/streamers.py#L87">不支持batch size &gt; 1</a></font></p><h1 id="textstreamer">1. TextStreamer</h1><p>若输出 token 构成完整单词，立即打印到 stdout。 skip_prompt：若为True，则第一次调用 put(input_ids) 不打印。</p><ul><li>put<ul><li>若生成 text 末尾为 ，打印新增文本，清空 cache。</li><li>若最后一个 token 为 CJK 字符，打印新增文本。</li><li>否则，打印新增文本中最后一个空格及之前的内容。</li></ul></li><li>end：打印剩余生成文本及换行符。</li></ul><h1 id="textiteratorstreamer">2. TextIteratorStreamer</h1><p>在另外的线程中运行generate，新增的可打印文本可以迭代器的方式非阻塞地获取。可用于交互式Gradio demo 中。</p><p><a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer">接口、代码用例</a></p><p><a href="https://medium.com/@shrinath.suresh/building-an-interactive-streaming-chatbot-with-langchain-transformers-and-gradio-93b97378353e" class="uri">https://medium.com/@shrinath.suresh/building-an-interactive-streaming-chatbot-with-langchain-transformers-and-gradio-93b97378353e</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 工具包 </category>
          
          <category> transformers </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>dGSLM</title>
      <link href="/blog/yin-pin/yu-yin-dui-hua/dgslm/"/>
      <url>/blog/yin-pin/yu-yin-dui-hua/dgslm/</url>
      
        <content type="html"><![CDATA[<ul><li><p>作者：Meta AI 等。</p></li><li><p>功能：生成自然的口语对话音频（双通道）。</p><p>示例：<a href="https://speechbot.github.io/dgslm/" class="uri">https://speechbot.github.io/dgslm/</a></p></li><li><p>特点</p><ul><li><font color="red">采用 cross attention结构，能同时听+说。</font></li><li>能同时生成语音、backchannels、笑声、其它副语言信号。</li><li>相较于 ASR+LM+TTS 级联模型，生成的对话 turn更自然、流畅，但语义连贯性较差。</li><li>textless: 训练数据不包含文本。</li></ul></li><li><p>发现</p><ul><li><font color="red">相较于 next token 预测，同时学习内容+时长，edgeunit 预测 + 时长预测 性能更好。且 next token预测容易重复单元生成，导致长静音。</font></li><li><font color="red">相较于朗读语音，对话语音包含随意的表达（如 hmm等填充词）、非语言声音（如笑声），在对话数据上训练HuBERT，以学习合适的语音表示。</font></li><li><font color="red">说话人内的停顿（pause）通常比说话人之间的停顿（gap）持续时间长。因此，停顿不是表明turn change 的充分信号。</font></li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB55a14acc3b1c2a30d02061ba230a4a9d?method=download&shareKey=d9d28e36de91c3acfa464cd1309ee15b" width="254"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBff57a2a82bec9b1b2e8e183e77e7cd92?method=download&shareKey=a6de0282150ada47bdfb8e47f2be5150" width="1039"></p><ul><li><font color="green">权重共享：speaker independent。</font></li><li><font color="red">Cross-attention。</font></li></ul><h2 id="训练目标">1.1. 训练目标</h2><p>由于</p><ul><li>直接预测非去重的声学 unit，同时学习内容、时长，模型性能较差。</li><li>unit 时长与 unit 本身高度相关。</li></ul><p>采用 Edge Unit Prediction (CE loss) +<font color="red">Delayed</font> Duration Prediction（连续值，L1损失）。</p><p><span class="math display">\[\mathcal{L}_{EU}=\sum_{c=1}^2\sum_{\substack{t \\u_t^{(c)} \neq u_{t-1}^{(c)}}} \log p\left(u_t^{(c)} \mid u_{1:t-1}^{(1,2)} ; \theta\right)\]</span> <span class="math display">\[\mathcal{L}_{ED}=\sum_{c=1}^2 \sum_{\substack{t\\ u_t^{(c)} \nequ_{t-1}^{(c)}}}\left|d_t^{(c)}-\hat{d}_t^{(c)}\left(u_{1:t-1+\Delta}^{(1,2)} ; \theta\right)\right|\]</span> <span class="math display">\[\mathcal{L}_{DLM}=\mathcal{L}_{EU}+\mathcal{L}_{ED}\]</span></p><p>其中，<span class="math inline">\(c\)</span> 表示通道，<span class="math inline">\(t\)</span> 表示时间，<span class="math inline">\(\theta\)</span>表示模型参数，<font color="red"><span class="math inline">\(\Delta\)</span> 表示延迟因子</font>，<span class="math inline">\(d_t^{(c)}\)</span> 为边界 unit <span class="math inline">\(u_t^{(c)}\)</span> 重复的次数。</p><p><font color="green">若 <span class="math inline">\(\Delta\)</span>足够大，则 <span class="math inline">\(u_{1:t-1+\Delta}^{(1,2)}\)</span> 已知时长。并且，若 <span class="math inline">\(\Delta&gt;2\)</span> 则来不及修改预测的unit，如：<span class="math inline">\(\Delta=3\)</span>，unit 预测：t时刻为 A，t+1 时刻为B，t+2时刻为C，时长预测需要输入ABC，若预测值=1，已无法修改 unit自回归序列中的B，但仍可修改输入 audio decoder 的 unit 序列。</font></p><h2 id="生成">1.2. 生成</h2><p><font color="red">依照预测的时长（四舍五入），修改预测的 unit，作为LM 自回归的输入。</font></p><h1 id="数据集">2. 数据集</h1><p>Fisher: 双通道电话对话音频，英语、含转写，16000多个对话，2000h，各种主题。</p><h1 id="训练">3. 训练</h1><ul><li>HuBERT+kmeans、HifiGAN 在 Fisher 的单通道语音段上训练。</li><li><a href="https://nanyang2015.github.io/blog/yu-yin/mo-xing/hubert/">HuBERT</a><ul><li>模型结构同 HuBERT Base，12 层 Transformer，训练同 HuBERTLarge，3次迭代。训练结束后，将最后一层 Transformer 特征量化为<font color="red">500 units</font>。</li></ul></li><li>modified HifiGAN <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><ul><li>输入为 HuBERT 离散单元 + 数据集中的 one-hot<font color="red">说话人</font>。生成时，说话人选自 HifiGAN训练集。</li><li>训练集：Fisher 子集，120 个说话人，每人10分钟。 说话人挑选：用在100h Librispeech clean 上训练的音素识别器 <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>，基于平均困惑度，挑选可懂度高的说话人。</li></ul></li><li>DLM (Dialogue Transformer Language Model)<ul><li>6 层 Transformer，head 8，embedding 512，cross attention加到前4层。</li><li>具体训练参数详见论文。</li></ul></li><li>Multi-Stream Transformer Language Model (MS-TLM) <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a><ul><li>embedding 层 concat 两个 channel 的表示，两个softmax 输出层。</li></ul></li><li>ASR+LM+TTS 级联模型<ul><li>ASR<ul><li>Switchboard 上微调的 wav2vec2-large + Switchboard 上训练的 4-gramKenLM。</li></ul></li><li>LM<ul><li>6层 Transformer。</li><li>训练时忽略被其它 turn 完全包含的 turn。样本示例：<code>&lt;A&gt; hi &lt;B&gt; hi how you doing &lt;A&gt; great &lt;B&gt; good good my name is marine</code></li></ul></li><li>Google TTS API</li></ul></li></ul><h1 id="实验">4. 实验</h1><ul><li>turn 定义<ul><li>对对话音频进行 VAD（pyannote）。</li><li>IPU：语音段，两侧有超过 200ms 的静音。</li><li>silence：两个通道都没有语音。<ul><li>gaps：不同说话人 IPU 之间。</li><li>pauses: 同一说话人 IPU 之间。</li></ul></li><li>overlap：两个通道都为语音。<ul><li>backchannel：短 IPU，在另一个说话人的语音段内。</li><li>interruption：在另一个说话人的语音段内开始，在其结束后仍继续。</li><li>确切的定义取决于语言内容。</li></ul></li><li>turn：同一说话人，由 pause 间隔的连续 IPUs。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB1326ad8836e5da1613c8e0280978bee2?method=download&shareKey=e0fce1af11ab415e5580dca3c7093254" width="592"></li><li>评价 对从开发集选取的117个 30s 的提示生成 90s 的continuations。temperature=1，topK=20。</li></ul><h2 id="结果">4.1. 结果</h2><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>实验</th><th>评价指标</th><th>结果</th><th>分析</th></tr></thead><tbody><tr><td>内容 + 时长建模</td><td>NLL: Negative Log-Likelihood<br>MAE: 1 代表 20ms</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB16c0d8a9434fbd2c5822bb39c1c7e801?method=download&shareKey=334f61a18237825d2e3ae897dc4a560f" width="398"><br>未采用时长预测的模型均包含next token 预测；否则不包含。</td><td><font color="red">相较于 next token 预测，同时学习内容+时长，edgeunit 预测 + 时长预测 性能更好。</font></td></tr><tr><td>Turn-taking Event 统计</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1ec9c8be6658fa2141a7b94eacfc9940?method=download&shareKey=e413c92e9822f3cf0a2e0b841d7b6119" width="1622"><br><img src="https://note.youdao.com/yws/api/personal/file/WEBebb06cb4ccb47181c30f5ffcb42d473f?method=download&shareKey=ea672c6d50c69f1fd12cc67601886787" width="811"></td><td>1. DLM-1 没有 cross attention 层，生成的对话两个通道间有较大的 gaps和 重叠。<br> <font color="red">2. MS-TLM、DLM-2 采用 next token预测训练，生成的对话有较长的pause、gap，可能由于重复单元生成。</font><br>3. 除 DLM-1外，所有模型都学习到了事实：pause 通常比 gap 持续时间长。<br>4.级联系统生成的对话几乎没有重叠、停顿。</td></tr><tr><td>Turn-taking Event 一致性</td><td>prompt、continuation 中 turn-taking events 时长的 Pearsoncorrelation。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB400306b257bd9075d40c20b2141bfe76?method=download&shareKey=9e8240f8c0090e8e1edd92474d85bf19" width="814"></td><td>1. 级联系统：TTS 与提示无关，gap 与 turn 数成正比。<br>2. 除 DLM-1外，其它模型相关性较好。</td></tr><tr><td>自然对话事件统计</td><td>1. WPM, words per minute<br>2. LPM, laughs per minute<br> <a href="https://github.com/jrgillick/laughter-detection">laugh检测</a><br>3. FWR, filler words per 100 words<br> filler: uh, um,like, i mean, you know</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB60ba6477214883d49caa5befe9028bc3?method=download&shareKey=509b61613b5fb78264053021533bc4d1" width="794"></td><td>没有 edge unit 预测的模型（MS-TLM、DLM 1-2）比有 edge unit预测的模型（DLM 3-5）更倾向于生成 语速更慢、笑声更少、filler更多的对话。<br> DLM 3-&gt;5，逐渐接近 ground truth。</td></tr><tr><td></td><td>4. Floor Transfer Offset：两个说话人连续 turn之间的时间间隔，正数表示 gap，负数表示 overlap</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa5a9f85b28cd904c224f4bfef7cadcd6?method=download&shareKey=7c21366753efc7951bb675cbfdd4eeba" width="393"></td><td></td></tr><tr><td>语义</td><td>1. 识别生成的对话，采用 <a href="https://huggingface.co/microsoft/DialoGPT-medium">DialoGPT</a>计算文本 PPL。<br> 说话人 token 替换为 &lt;endoftext&gt;。<br>对于条件生成，concat prompt 与生成文件，计算PPL。<br><font color="green">2. VERT-4、self-BLEU、 auto-BLEU<br>由于对话文本有较多重复，采用 VERT-4 指标而不是 VERT-2。</font></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB65bda35114b4377602b6137193abb90d?method=download&shareKey=50d8f63335b51756d742d486f984538f" width="806"><br><img src="https://note.youdao.com/yws/api/personal/file/WEB54756f354878e4007d4f7a3e8fdde74b?method=download&shareKey=d98ccc776571f207454816b78ca547b0" width="794"></td><td>1. 部分模型无法计算PPL@GT，由于它们生成重复单元，合成音频全为噪声。<br>2.相较于级联模型，对话模型生成的语音 PPL 较高，语义不连贯。</td></tr><tr><td>主观评价</td><td>1. N-MOS：对话的自然度、流畅、turn-takingconversationality，说话人的表现力。<br>2.M-MOS：语义连贯、有意义。<br>1-5（非常好）。<br>众包标注，CrowdMOSpackage 统计，检测和丢弃不准确的分数。<br>排除与平均分相关系数&lt;<font color="green">0.6</font>的评分员。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3ffba27af7a30f51b72b2483073a6291?method=download&shareKey=221560e47809cbb17dc777593509d9b4" width="800"></td><td>1. 级联模型生成的音频自然度较差。<br>2. 所有对话模型中，DLM-5效果最好。<br>3. DLM-5 生成的对话语义连贯性较差，但自然度相较于 groundtruth 差距最小。</td></tr><tr><td>HuBERT 训练集</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3b8ad7c6bde7de0229f8e70772d1c94e?method=download&shareKey=7c7d3e4f269c84e3425f901eb189f474" width="792"></td><td><font color="green">相较于在有声读物上训练的开源 HuBERT，在 Fisher上训练的 HuBERT 能学到更适用于对话的语音信息。</font></td></tr><tr><td>cross attention 层数的影响</td><td></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5b9e0d244a75443c0e8aca4cfeba04a7?method=download&shareKey=0b7c34eea5c6788aa6a31bc690883073" width="400"></td><td>4 层、6层效果相当。</td></tr></tbody></table><h1 id="其它">5. 其它</h1><ul><li>Nguyen T A, Kharitonov E, Copet J, et al. Generative spoken dialoguelanguage modeling[J]. Transactions of the Association for ComputationalLinguistics, 2023, 11: 250-266.</li><li><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/textless_nlp/dgslm">代码、模型</a></li><li>扩展：每个说话人有自己的通道。</li><li>文本对话生成<ul><li>【长上下文、多方对话】Xu J, Szlam A, Weston J. Beyond goldfishmemory: Long-term open-domain conversation[J]. arXiv preprintarXiv:2107.07567, 2021.</li><li>【利用互联网最新信息进行增强】Komeili M, Shuster K, Weston J.Internet-augmented dialogue generation[J]. arXiv preprintarXiv:2107.07566, 2021. Shuster K, Komeili M, Adolphs L, et al. Languagemodels that seek for knowledge: Modular search &amp; generation fordialogue and prompt completion[J]. arXiv preprint arXiv:2203.13224,2022.</li></ul></li></ul><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>Polyak A, Adi Y, Copet J, et al. Speech resynthesis fromdiscrete disentangled self-supervised representations[J]. arXiv preprintarXiv:2104.00355, 2021.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>Rivière M, Dupoux E. Towards unsupervised learning ofspeech features in the wild[C]//2021 IEEE Spoken Language TechnologyWorkshop (SLT). IEEE, 2021: 156-163.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3"><p>Kharitonov E, Lee A, Polyak A, et al. Text-freeprosody-aware generative spoken language modeling[J]. arXiv preprintarXiv:2109.03264, 2021.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音对话 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Whisper</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/lun-wen/whisper/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/lun-wen/whisper/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：OpenAI</li><li><font color="red">wav2vec 2.0等无监督预训练的语音编码器，需要有监督训练解码器。Whisper采用多语种、多任务训练，以及 68wh大规模低质量弱监督训练集，可以在广泛的语音环境中鲁棒地工作，不需要特定数据集的微调、zero-shot性能也较好。</font></li><li><font color="red">创新点</font><ul><li><font color="red">通过在 decoder 端添加任务提示，实现 multitask训练。且验证了多任务、多语种训练有知识迁移正收益。</font></li><li><font color="red">训练数据量大。</font></li><li><font color="red">训练时以 50%的概率将当前音频之前的转写文本添加到解码器的上下文中。实际应用中，推理时可输入上一窗口的识别结果用于长音频识别，或特定领域、特定任务的提示。</font></li><li><font color="red">直接预测原始的转写文本，而不需要单独的 ITN模块，简化了流程。</font></li></ul></li><li><font color="red">发现</font><ul><li><font color="red">训练数据每增加16倍，WER 减半。</font></li><li>采用对数概率作为得分。从温度0开始，即始终选择最高概率的token。当生成token 的平均对数概率低于 -1，或生成文本的 gzip压缩率大于2.4时，将温度提高 0.2 直到 1.0，最终为 5 beams 的 beamsearch。当温度低于 0.5时，将前一个窗口的转写文本作为上文输入。<font color="red">可以减少贪婪解码中更频繁出现的重复循环。</font></li></ul></li></ul><h1 id="模型">1. 模型</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbd20803d649ee1ac60ace53dfad2bb8e?method=download&shareKey=e472e659f0649f8ee4ad233b75a56240" width="991"></p><table style="width:100%;"><colgroup><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"></colgroup><thead><tr><th>Size</th><th>Layers</th><th>Width</th><th>Heads</th><th>Parameters</th><th>Required VRAM</th><th>Relative speed</th></tr></thead><tbody><tr><td>tiny</td><td>4</td><td>384</td><td>6</td><td>39 M</td><td>~1 GB</td><td>~32x</td></tr><tr><td>base</td><td>6</td><td>512</td><td>8</td><td>74 M</td><td>~1 GB</td><td>~16x</td></tr><tr><td>small</td><td>12</td><td>768</td><td>12</td><td>244 M</td><td>~2 GB</td><td>~6x</td></tr><tr><td>medium</td><td>24</td><td>1024</td><td>16</td><td>769 M</td><td>~5 GB</td><td>~2x</td></tr><tr><td>large/large-v2/large-v3</td><td>32</td><td>1280</td><td>20</td><td>1550 M</td><td>~10 GB</td><td>1x</td></tr></tbody></table><ul><li><p>音频：30s。没有语音的片段 <span class="math inline">\(10\times\)</span> 下采样也加入训练。16kHz，80维FBank，25ms 窗长，10mshop。</p></li><li><p>特征归一化：[-1, 1]，零均值。</p></li><li><p><span class="math inline">\(2 \times Conv1D\)</span>：kernel3，第2层 stride 2。</p></li><li><p>encoder output + layer norm</p></li><li><p>文本：BPE tokenizer。</p></li><li><p><font color="red">以 50%的概率将当前音频之前的转写文本添加到解码器的上下文中。</font></p></li><li><p>时间戳预测：训练集中的时刻被量化为20ms分辨率。</p></li><li><p>存在音频截断时</p><ul><li>预测时间戳：最后只预测起始时刻token。</li><li>不预测时间戳：截断音频，使其不包含不完整的token。</li></ul></li><li><p><font color="red">多任务训练：口语语种识别、多语种语音识别、语音翻译、VAD。</font></p></li><li><p>超参数</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBac5a0219964b788e71002aeeb36a0ed2?method=download&shareKey=57995ce43c68a48fde48a34814cbb555" width="360"></p></li><li><p>Large V2</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB35781e40821ba56bc40ec46a18412a5a?method=download&shareKey=f15bc329d64997e2dbaeb9c11fcd173b" width="335"></p></li><li><p>Large V3</p><ul><li>音频特征：128 维 FBank。</li><li>训练集：100wh 弱标注数据 + 400wh 采用 Large V2生成的伪标签数据。</li><li>2 epoch。</li><li><a href="https://github.com/openai/whisper/discussions/1762" class="uri">https://github.com/openai/whisper/discussions/1762</a></li></ul></li></ul><h1 id="数据集">2. 数据集</h1><ul><li>43.8wh 英语 ASR 数据（约占65%）</li><li>11.7wh 其它 96 个语种的识别数据。</li><li>12.6wh <span class="math inline">\(x \rightarrow \text {en}\)</span>翻译数据。</li><li>各语种数据量详见附录E。</li></ul><h1 id="数据处理">3. 数据处理</h1><ul><li><font color="red">直接预测原始的转写文本，而不需要单独的 ITN模块，简化了流程。</font></li><li>转写文本筛选<ul><li>删除机器生成的转写文本（如不包含逗号、感叹号、问号等复杂的标点符号，全大写或全小写）。在人工和机器混合生成的数据集上训练，会显著地损害翻译系统的性能。</li><li>语种检测。剔除音频、文本语种不匹配的，但保留文本为英语的作为翻译样本。</li><li>文本模糊去重。</li></ul></li><li>训练得到初始模型后，对训练集中的数据源根据 WER、数据大小排序，人工检查，剔除低质量数据源。</li><li>避免污染：对训练集和重叠风险很高的测试集 TED-LIUM 3进行文本去重。</li><li>存在的问题：许多转写文本包含当前说话人的名字。Whisper在不存在上述情况的子集上进行了微调。</li></ul><h1 id="zero-shot-评价">4. zero-shot 评价</h1><h2 id="tn">4.1. TN</h2><ul><li>计算 WER 前进行 TN，以减少非语义差异的惩罚。</li></ul><h3 id="英语">4.1.1. 英语</h3><ol start="0" type="1"><li>转小写</li><li>删除[]中的内容</li><li>删除()中的内容</li><li>删除 hmm, mm, mhm, mmm, uh, um</li><li>删除 ' 前的 whitespace</li><li>将标准的或非正式的缩略形式转换为原始形式</li><li>删除数字间的逗号</li><li>删除其后不为数字的句号</li><li>删除除句号、百分比、货币符号之外的 Unicode 类别以 M、S、P开头的字符<ul><li>M：MiscellaneousSymbols，包含箭头、几何形状、星号、圆圈等，如➡、★、○ 等</li><li>S：Symbols，包含货币符号、标点符号、数学符号、技术符号等，如$、%、℃等</li><li>P：Punctuation，标点符号</li></ul></li><li>将数字、货币转换为阿拉伯数字，如 Ten thousand dollars <span class="math inline">\(\rightarrow\)</span> $10000</li><li>将英式拼写转换为美式拼写</li><li>删除前/后不为数字的百分比、句号、货币符号</li><li>将连续的 whitespace 替换为 1个空格</li></ol><h3 id="非英语">4.1.2. 非英语</h3><ol type="1"><li>删除[]中的内容</li><li>删除()中的内容</li><li>将 NFKC-normalized 字符串中 Unicode 类别以 M、S、P开头的字符替换为1个空格</li><li>转小写</li><li>将连续的 whitespace 替换为 1个空格</li><li>对于不以空格分隔单词的语种，每个 Unicode 字符间插入1个空格</li></ol><h2 id="英语语音识别">4.2. 英语语音识别</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd35e737cea04cd547ac7bf9c27a2f70a?method=download&shareKey=8ff960ceb46e3ee800afe7f0f0cfecae" width="346"></p><ul><li><font color="red">虽然在 LibriSpeech test-clean 上的 WER 高于SOTA，但相较于在 LibriSpeech上监督训练的模型，在其它测试集上的泛化性能更好。</font></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB836ace73368a699aefd86ad561fadfd7?method=download&shareKey=5dc9e9a441ae3044b1319d39750e2934" width="912"></p><h2 id="多语种识别">4.3. 多语种识别</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB849dfce200ed0c6e3d11466f25ca73d8?method=download&shareKey=987641e44832d417d9fefe85fe71cef4" width="408"></p><ul><li>VoxPopuli数据集上效果较差，可能由于它具有更多的监督数据，有利于微调。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB70ac88b46df672e6102bdd2b686554b3?method=download&shareKey=d23033c50f7911dad31a5f71d216021a" width="418"></p><ul><li><font color="red">训练数据每增加16倍，WER减半。</font>训练数据大多为印欧语系，中文（ZH）、韩语（KO）等语种与上述规律偏离较远，可能的原因：语系迁移、tokenizer、数据质量。</li></ul><h2 id="语音翻译">4.4. 语音翻译</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB772cfc893a6086de48d06fa99f5d6062?method=download&shareKey=5716b9ec8d97207bc03856217ba4bd8a" width="409"></p><ul><li><font color="red"><span class="math inline">\(x \rightarrow \text{en}\)</span>翻译，在低资源语种上翻译效果显著提升，但在高资源语种上略差于SOTA。</font></li><li>由于训练集创建时语种识别错误，导致部分英语音频转写任务被标记为翻译任务。</li></ul><h2 id="对噪声的鲁棒性">4.5. 对噪声的鲁棒性</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf45f39c8413c859841e3dae7ef688dae?method=download&shareKey=e6777c38d08297ef607a589a1bf6ec85" width="414"></p><ul><li><font color="red">在 LibriSpeech test-clean上，高信噪比时，性能差于用 LibriSpeech预训练或微调的模型，但在低信噪比时（如10dB pub 噪声），Whisper性能较好。</font></li></ul><h2 id="长音频转写">4.6. 长音频转写</h2><ul><li><font color="red">连续转写 30s的片段，根据模型预测的时间戳来移动窗口。</font></li><li>采用对数概率作为得分。从温度0开始，即始终选择最高概率的token。当生成token 的平均对数概率低于 -1，或生成文本的 gzip压缩率大于2.4时，将温度提高 0.2 直到 1.0，最终为 5 beams 的 beamsearch。当温度低于 0.5时，将前一个窗口的转写文本作为上文输入。可以减少贪婪解码中更频繁出现的重复循环。</li><li>结合 no-speech 概率阈值 0.6、平均对数概率阈值-1，可以更可靠地检测没有语音的片段。</li><li>约束第一个时间戳在 0～1s，避免模型忽略前几个词。<img src="https://note.youdao.com/yws/api/personal/file/WEB76755ca27d1951139ef4c367ac9656bb?method=download&shareKey=c5e47638f97e0a25399aca971e2852a9" width="392"></li><li>在所有数据集上，均优于 SOTA 开源模型 NVIDIA STT Conformer-CTC Largemodel (Nemo)；在大部分数据集上，优于商业 ASR系统（训练集可能包含这些公开测试集）。</li></ul><h2 id="与人类比较">4.7. 与人类比较</h2><ul><li>由于歧义、不清晰的语音、label错误等，每个数据集都有不同程度的错误。仅采用 WER很难衡量还有多少改进空间。</li><li>从 Kincaid46数据集中挑选了25条音频，录音环境涵盖有/无脚本的广播、电话、VoIP通话、会议。获得5份专业转写员的转写，其中4份完全由人工转写，1份提供了机器辅助转写。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaafd7b2add354903a7ef71e0cc969dbb?method=download&shareKey=173a38db52d391d654fec5057da63e62" width="380"></p><ul><li>计算机辅助转写的 WER 最低。Whisper 的英语 ASR性能接近人类水平，只比纯人工转写的 WER 差不到1个百分点。</li></ul><h2 id="语种识别">4.8. 语种识别</h2><h2 id="vad">4.9. VAD</h2><h1 id="消融实验">5. 消融实验</h1><h2 id="模型数据规模">5.1. 模型、数据规模</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB76db9ac4e5dc83608d5834e22c85f9c3?method=download&shareKey=321535b080bd0292a0690a1dcff41728" width="953"></p><ul><li>随着模型大小的增加，英语语音识别的性能接近饱和，可能由于其已接近人类水平。多语种语音识别、语音翻译、语种识别的性能不断提高。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8c5929fe495c4089b905cd581b9a0cd6?method=download&shareKey=dcaa4abc5367d7f32a7bd20e0e4085a3" width="387"></p><p>* Whisper medium +通过更长的训练时间、更大的模型，可以进一步提高性能。</p><ul><li><font color="green">其它语种、任务是否需要和英语 ASR相当的数据量来达到最佳性能？</font></li><li>数据分布变化后，是否损伤英语 ASR 的性能？</li><li>接近人类水平是否是模型的上限？</li><li>在当前数据集上，扩大模型容量的收益已接近饱和。继续增加英语 ASR的数据量是否有意义？数据量增加后进一步扩大模型容量是否有收益？</li></ul><h2 id="多任务多语种迁移">5.2. 多任务、多语种迁移</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc6e4ba8f86c64ac0c4f6822672788ab2?method=download&shareKey=23f20f11fb6137e67baa5297d501e93e" width="389"></p><ul><li><font color="red">相较于仅在英语 ASR 任务上训练的模型，相同的英语ASR训练计算量，采用多任务、多语种训练，小模型性能较差，但扩展到更大的模型时可以超越单一任务模型的性能，从其它任务中获得知识迁移正收益。</font></li></ul><h1 id="展望">6. 展望</h1><ul><li><font color="red">存在的问题：陷入重复循环、不转写音频片段的前几个或后几个单词、输出与音频完全无关的文本等，特别是在长音频上。可能的改进方案：在高质量监督数据集上微调Whisper；强化学习。</font></li><li>增加低资源语种的训练数据。</li><li>微调。</li><li>通过消融实验研究 encoder、decoder分别的收益：比如训练无解码器的CTC模型、对比现有语音识别 encoder (如wav2vec 2.0) + LM 的性能。</li><li>添加无监督预训练或 self-teaching。</li></ul><h1 id="缺点">7. 缺点</h1><ul><li>不支持流式</li></ul><h1 id="todo代码">8. TODO：代码</h1><ul><li><font color="red">提示输出简体中文，而非繁体字</font><code>whisper --language Chinese --model large audio.wav --initial_prompt "以下是普通话的句子。"  # simplified</code></li><li>huggingface<ul><li><font color="green">时间戳预测<code>return_timestamps=True # 句子级时间戳</code><code>return_timestamps=word # 词级时间戳</code></font></li><li>速度、内存优化<ul><li>长音频转写：chunked 算法 Gandhi S, von Platen P, Rush A M.Distil-Whisper: Robust Knowledge Distillation via Large-Scale PseudoLabelling[J]. arXiv preprint arXiv:2311.00430, 2023.</li><li>flash_attention_2</li><li>Torch Scale-Product-Attention (SDPA)</li></ul></li><li>微调：<a href="https://huggingface.co/blog/zh/fine-tune-whisper" class="uri">https://huggingface.co/blog/zh/fine-tune-whisper</a></li></ul></li></ul><h1 id="其它">9. 其它</h1><ul><li><a href="https://github.com/openai/whisper" class="uri">https://github.com/openai/whisper</a></li><li>Radford A, Kim J W, Xu T, et al. Robust speech recognition vialarge-scale weak supervision[C]//International Conference on MachineLearning. PMLR, 2023: 28492-28518.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅老师 《GPT-4o 背后可能的语音技术猜测》</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/gpt-4o-li-hong-yi/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/gpt-4o-li-hong-yi/</url>
      
        <content type="html"><![CDATA[<p>视频地址 <a href="https://www.youtube.com/watch?v=CgQ3lUOpXgc&amp;ab_channel=Hung-yiLee" class="uri">https://www.youtube.com/watch?v=CgQ3lUOpXgc&amp;ab_channel=Hung-yiLee</a></p><h1 id="audio-tokenization">1. audio tokenization</h1><ul><li>audio encoder +codebook，表征语音、人声（喘息声、笑声等）、声音事件等各类声音。</li><li>【宏毅老师猜想】混合 token：text token + 非语音 audio token<ul><li>出发点：若 speech token、text token 独立，有冗余。</li><li><font color="green">【个人想法】若要对齐语音 token 与 texttoken，需要转写音频数据集，强制对齐音频 token、texttoken，且会丢失语音段内的非文本信息。</font></li></ul></li><li>speaker diarization<img src="https://note.youdao.com/yws/api/personal/file/WEBae527fb0aa9425b44ef473c9e772e3bd?method=download&shareKey=3ed39e1c632228d0f7022ee3b35a49b3" width="634"></li></ul><h1 id="audio-encoder-decoder">2. audio encoder-decoder</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe699e9ea92a55945fe75445390f396ef?method=download&shareKey=21783fbd1e3ff344b285cf74182711f7" width="1806"></p><h1 id="预训练">3. 预训练</h1><ul><li>音频自回归生成 LM由于预训练所需音频量较大、音频的录音环境等非常多样，生成音频可能自带BGM（it is not a bug, it is a feature）。</li><li>结合文本数据<ul><li>由于相较于文本数据，现有的音频数据包含的文本内容（知识）较少，LLM预训练仍需要结合文本数据。</li><li>结合方式：用 LLM 初始化；音频、文本混合训练等。<ul><li><a href="https://arxiv.org/abs/2310.08715" class="uri">https://arxiv.org/abs/2310.08715</a></li><li><a href="https://arxiv.org/abs/2402.05755" class="uri">https://arxiv.org/abs/2402.05755</a></li></ul></li></ul></li></ul><h1 id="alignment">4. alignment</h1><ul><li>训练数据：用户-AI 对话数据<ul><li>若想生成固定音色，可能不需要大量该音色的训练数据。也可以采用语音转换生成大量指定音色的数据。</li></ul></li><li>语音合成<ul><li><p>当有大量的合成训练数据时，合成的声音不再平淡，而是能根据文本内容调整音色。</p><p>BASE TTS (Amazon)：100kh，1B 参数</p></li></ul></li><li>开始说话、停止说话的判断停止说话：听到人声就停止不是最佳方案，比如人与 AI 合唱。<ul><li>同时输入麦克风输入、历史生成，用户说话时生成silence，自动预测何时开始说话生成声音 token，用户打断时停止说话。<img src="https://note.youdao.com/yws/api/personal/file/WEBa91a9bc7be198bae13fc8b07f3071511?method=download&shareKey=97b422cb52543f4f04e7b8b506e70981" width="610"></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB404e59229702978d347f587d8590746b?method=download&shareKey=d4fe70c6a16687f7db1826c264dee318" width="603"></li></ul><h1 id="其它">5. 其它</h1><ul><li><p>Speech/Audio LLM 论文合集</p><p><a href="https://github.com/ga642381/speech-trident" class="uri">https://github.com/ga642381/speech-trident</a></p></li><li><p>LLM 训练的3阶段</p><ul><li>无标注数据 预训练</li><li>有标注数据 微调（对齐）</li><li>RLHF 用户反馈微调（对齐）</li></ul></li><li><p>ASR + 情绪识别 + LLM</p><ul><li><a href="https://arxiv.org/abs/2402.12786" class="uri">https://arxiv.org/abs/2402.12786</a></li><li><a href="https://arxiv.org/abs/2312.15316" class="uri">https://arxiv.org/abs/2312.15316</a></li></ul></li><li><p>带指令的合成，控制音色</p><ul><li>suno-ai/bark</li><li>audiobox meta</li><li><a href="https://arxiv.org/abs/2309.14324" class="uri">https://arxiv.org/abs/2309.14324</a></li><li><a href="https://arxiv.org/abs/2211.12171" class="uri">https://arxiv.org/abs/2211.12171</a></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AudioLM</title>
      <link href="/blog/llm/yin-pin-sheng-cheng/lun-wen/audiolm/"/>
      <url>/blog/llm/yin-pin-sheng-cheng/lun-wen/audiolm/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：Google, 2022.09。</li><li><font color="red">功能：输入 prompt音频（3s），生成延续，在语法、语义上连贯，同时保持说话人的音色、韵律、录音环境。还能扩展到钢琴音乐延续。</font></li><li>创新点<ul><li>音频表示同时采用语义 token、声学token，兼顾了长时结构的一致性（如语音中的语法、语义，音乐中的旋律、和弦、节奏）、高合成质量。</li><li>采用3阶段 decoder-only TransformerLM，分别建模语义、粗粒度声学表示、细粒度声学表示。每个阶段的 token序列长度更小。</li></ul></li><li>优点<ul><li>不需要带标注的训练音频。</li></ul></li></ul><h1 id="系统结构">1. 系统结构</h1><ul><li>音频表示：离散 token 语义 token：采用自监督表示学习w2v-BERT，表征局部内容、长时结构； 声学 token：采用 neural audio codecSoundStream，实现高质量的合成。<img src="https://note.youdao.com/yws/api/personal/file/WEB7e9ed734208fcc5c20f2e283870071c9?method=download&shareKey=48506717ee8cd986aeb067812c47179c" width="584"></li><li>w2v-BERT XL 0.6B<ul><li>基于 Conformer，MLM loss + 对比 loss。40ms 帧率。</li><li>中间层表示（MLM 第7层）k-means 聚类。聚类前进行归一化（各维0均值、单位方差）可以显著地提高音素区分性。</li></ul></li><li>SoundStream<ul><li>residual vector quantizer (RVQ) 由 Q 个词表为 N的量化器组成。16kHz、20ms 帧率，比特率为 <span class="math inline">\(50* Q * \log_{2}{N} \mathrm{bps}\)</span>。</li><li>重建损失 + 对抗损失，端到端训练。</li></ul></li><li>decoder-only Transformer 自回归 LM。</li></ul><h2 id="阶段建模">1.1. 3 阶段建模</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5478fde1b8235d245d20aa9cfba1f22e?method=download&shareKey=b38bac8bd4dceef3a7d5dd174c1b2a58" width="1184"></p><p>3 个 decoder-only Transformer。</p><ol type="1"><li><p>语义建模：自回归预测语义 token <span class="math inline">\(p\left(z_t \midz_{&lt;t}\right)\)</span>。</p></li><li><p>粗声学建模：以语义 token 为条件，只建模粗声学 token（前 <span class="math inline">\(Q^{\prime}\)</span>个量化器），恢复说话人、录音环境等声学特性。token 序列为 <span class="math inline">\(\left(z_1, z_2, \ldots, z_{T_S}, y_1^1, y_1^2,\ldots, y_1^{Q^{\prime}}, y_2^1, y_2^2, \ldots,, y_2^{Q^{\prime}},\ldots, y_{T_A}^{Q^{\prime}}\right)\)</span>，建模 <span class="math inline">\(p\left(y_t^q \mid z, y_{&lt;t}^{\leq Q^{\prime}},y_t^{&lt;q}\right)\)</span> <span class="math inline">\(q \leqQ^{\prime}\)</span>，其中，<span class="math inline">\(y\)</span> 序列为<span class="math inline">\(y+o\)</span>，<span class="math inline">\(o_i=(i-1 \bmod Q^{\prime}) \cdotN\)</span>。<font color="green">训练时 <span class="math inline">\(y_1^1\)</span> 为第一个预测的token。</font></p></li><li><p>细粒度声学建模：<span class="math inline">\(p\left(y_t^q \midy^{\leq Q^{\prime}}, y_{&lt;t}^{&gt;Q^{\prime}},y_t^{&lt;q}\right)\)</span> <span class="math inline">\(q &gt;Q^{\prime}\)</span>，去除第二阶段有损压缩导致的 artifacts。由于细粒度声学 token 由粗粒度 token 局部确定，在 3s的非重叠块上执行。因此，第3阶段不受目标音频序列长度影响，且可以采用更大的Q 来实现更高的质量。</p></li></ol><ul><li>条件独立假设：语义 token 与 声学 token 条件独立；细粒度声学 token与语义 token 条件独立。</li><li>优点：相较于语义 token、声学 token交替，合并第2、第3阶段，该方案每个阶段的 token序列长度减小了，计算更高效。</li></ul><h2 id="推理">1.2. 推理</h2><ul><li>无条件生成。无条件采样语义token。可以生成具有不同说话人、韵律、声学环境特征的，语法、语义一致的，多样化的语音。</li><li>声学生成。给定语义token，合成语音的文本一致，具有不同的声学特征。</li><li>延续。输入简短的 prompt 音频。</li></ul><h1 id="实验">2. 实验</h1><ul><li>训练集 AudioLM所有组件（w2v-BERT、k-means、SoundStream、decoder-onlyTransformer）的训练集都为 Libri-Light unlab-60k（英语）。</li><li>SoundStream <span class="math inline">\(Q=12\)</span>，<span class="math inline">\(N=1024\)</span>，<span class="math inline">\(Q^{\prime}=4\)</span>。4个卷积 block stride(2,4,5,8)，音频采样率 16kHz，帧率 20ms。</li><li>decoder-only Transformer<ul><li>3个阶段的模型结构一致，各 0.3B，参数详见论文。</li><li>3个阶段，音频随机裁剪到 30s、10s、3s。删除语义 token的连续重复。</li></ul></li><li>训练 训练音频 encoder、decoder -&gt; 冻结，训练LM。解耦、简化训练流程。</li><li>推理3个阶段均采用采样，温度系数分别为0.6、0.8、0.6，权衡生成语音的多样性、语义一致性。</li><li>对比语义 token、声学 token<ul><li>音素区分性<ul><li>方式：将 w2v-BERT k-means 聚类质心表示、SoundStream encoder输出，转换为 RVQ embedding，计算 ABX 错误率。</li><li>评价指标：ABX 错误率。考虑仅中心 phone 不同的 triphone，ABX 错误率为X 更接近 B（中心 phone 不同）而不是 A（中心 phone 相同）的比例。<ul><li>within-speaker: A、B、X 来自同一说话人。</li><li>across-speaker: A、B 来自同一说话人，X 来自另一说话人。</li></ul></li></ul></li><li>重建质量<ul><li>方式：训练 SoundStream decoder，输入 token 重建音频。</li><li>评价指标：ViSQOL score，参考音频、重建音频之间的感知相似度。</li></ul></li><li>训练 decoder-only Transformer，采用输出 token 合成音频。<ul><li>采用语义 token，音频合成质量很差。</li><li>采用声学 token，保留了说话人特征、录音环境，但内容往往类似于babbling。</li></ul></li></ul></li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>对比语义 token、声学 token</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB83e195cc1dc022a6ca8f7ed21ba07560?method=download&shareKey=ac47314155debfbd09bb877c610ad6c1" width="1176"></td><td>语义 token 包含了长时结构，声学 token保留了说话人特征、录音环境。<br> 相较于 <span class="math inline">\(Q=4\)</span>（2000bps），<span class="math inline">\(Q=12\)</span>（6000bps）显著提高了生成音频的质量。</td></tr><tr><td>语义建模超参数选择</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB02ac67da6a5c3ae42d72104b22d776ba?method=download&shareKey=81e7f0785a75b8a07730ff360d98fd14" width="582"><br>*另外，主观评价不同超参数下生成的音频。</td><td>采用 w2v-BERT XL 0.6B MLM 第7层的表示；<br>k-means 聚类 <span class="math inline">\(K=1024\)</span>。</td></tr><tr><td>语义 token<br>给定语义 token 生成音频，ASR</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1f0c712205e17dfe0dd6bf60b00b639f?method=download&shareKey=181dadfdba4ff4d82fc6d21b074b6155" width="550"></td><td>CER、WER 较低：语言内容被语义 token 完全表示。<br> AudioLM WER/CER相对较高，而SoundStream 重建音频与原始音频的 WER/CER 接近：AudioLMWER/CER 主要来源于语义 token 到声学 token 的映射。<br>WER的主要来源：专有名词合成、&lt;eos&gt;。</td></tr><tr><td>声学 token<br>说话人分类</td><td>上一实验中，给定语义token，多次采样，说话人、录音环境变化较大，韵律、语调变化较小。<br><img src="https://note.youdao.com/yws/api/personal/file/WEB4b8cffc907ad2ecacc2d2f04142be561?method=download&shareKey=4fdf85b21ff4a1f7d34537a84c9a97e3" width="564"></td><td>说话人、录音环境信息由声学 token 表示。</td></tr><tr><td>语义 token LM 的语言学知识<br>sWUGGY：给定单词、发音相近的non-word，模型是否对正确单词有更高的概率。<br>sBLIMP：给定语法正确的句子、语法不正确的相似的句子，模型对前者概率更高的比例。</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBdc9cb9b175df27d911ee93bd08b6c3c8?method=download&shareKey=03cee79773d8ba262c19f6cacdcae155" width="580"></td><td>不需要文本有监督数据，语义 token LM 词法、句法判断能力较强。</td></tr><tr><td>主观评价<br>区分原始录音与AudioLM 生成的延续。原始录音也经过SoundStream 压缩。</td><td>人工区分正确率 51.2%，接近随机猜。</td><td></td></tr><tr><td>钢琴延续<br>在钢琴音乐数据集上重训所有组件。Q=3,N=16384，不需要第3阶段。<br>主观评价：对仅基于声学 token生成的延续、AudioLM 生成的偏好</td><td>83.3% 偏好 AudioLM 的生成。</td><td></td></tr></tbody></table><h1 id="安全性">3. 安全性</h1><p>训练了分类器区分 AudioLM 生成的语音、压缩后的真实录音，准确率98.6%。</p><h1 id="其它">4. 其它</h1><ul><li>Borsos Z, Marinier R, Vincent D, et al. Audiolm: a language modelingapproach to audio generation[J]. IEEE/ACM Transactions on Audio, Speech,and Language Processing, 2023.</li><li><a href="https://google-research.github.io/seanet/audiolm/examples" class="uri">https://google-research.github.io/seanet/audiolm/examples</a></li><li><a href="https://github.com/lucidrains/audiolm-pytorch" class="uri">https://github.com/lucidrains/audiolm-pytorch</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频生成 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AudioPaLM</title>
      <link href="/blog/llm/yin-pin-sheng-cheng/lun-wen/audiopalm/"/>
      <url>/blog/llm/yin-pin-sheng-cheng/lun-wen/audiopalm/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：Google，2023.06。</li><li>功能：speech-to-speech translation、speech-to-texttranslation、ASR、MT、TTS，多语种。</li></ul><h1 id="系统结构">1. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf8e7c81e1d1e383223684f3e4e69f8b9?method=download&shareKey=6f0854513c1b21d961fdf2ba3169b85e" width="572"></p><ul><li>输入：在输入前加任务 tag前缀，以及输入语种、输出语种（可选，若与输入语种不一致）。任务 tag 采用normal text，而非 special tokens。输入语种对低资源语种有益。</li><li>audio encoder：USM-v2，相较于 USM-v1 2B，<font color="green">用辅助ASR 损失训练、微调</font>。</li><li>文本 tokenizer：SentencePiece。</li><li>LLM：PaLM-2 8B。</li><li>audio 合成：SoundStorm 或 AudioLM。</li></ul><h2 id="audio-encoder">1.1. audio encoder</h2><ul><li>从 audio encoder 中提取 embedding，并通过 k-means 量化为离散token。25Hz，词汇表大小 1024。</li><li>与原始 AudioLM 的区别：在多语种数据上训练 w2v-BERT；k-means聚类前未对 embedding 归一化，归一化降低性能。</li></ul><h2 id="llm">1.2. LLM</h2><ul><li>用 PaLM-2 8B 初始化，扩充 embedding 矩阵维度，audio embedding 全 0初始化，输入、输出 embedding 矩阵共享，训练所有模型参数。</li></ul><h2 id="audio-合成">1.3. audio 合成</h2><ul><li>自回归解码：AudioLM stage2+3，stage2 输入 LLM 生成的 token及说话人语音 condition，自回归生成粗粒度 SoundStream token；stage3进一步生成细粒度 SoundStream token。</li><li>非自回归解码：SoundStorm，并行处理所有 token。相较于AudioLM，音质相当，语音、声学环境的一致性更高，合成速度快2个数量级。</li><li>上述两种方案，均先将音频 token 转换为 SoundStreamtoken，然后采用卷积 decoder 转换为波形。</li><li>将 3s 长的语音作为condition，以保留说话人的声音；若语音小于3s，重复音频。</li><li>在 Multilingual LibriSpeech 上训练。</li></ul><h1 id="数据集">2. 数据集</h1><h2 id="训练集">2.1. 训练集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9e357b70a5077239eaf78eb3ab5835dc?method=download&shareKey=e70deebc5c4a4e7022dc884177bff018" width="777"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB87c96e0ca1227cf785897d6239419c11?method=download&shareKey=ec681948f088f08a2de5f2cc41220d51" width="533"></p><ul><li>CVSS：扩展了CoVoST2，合成了目标文本的语音。CVSS-C：采用1个标准说话人的声音；CVSS-T：采用源语音的声音。</li><li>VoxPopuli：转写翻译音频。</li><li>conversational dataset：众包朗读 西班牙语-英语机器翻译中的西班牙语部分。<ul><li>Leveraging weakly supervised data to improve end-to-endspeech-to-text translation</li></ul></li><li>YouTube ASR：采用 USM-2B ASR 模型转写。</li><li>WMT/TED TTS：采用 WMT、TED 文本翻译数据集，TTS引擎合成成对音频。</li><li>PaLM MT TTS：采用 PaLM-2 将 YouTube、Common Voice、Babel数据集翻译为英文，再采用 AudioPaLM 8B S2ST 模型合成语音。</li></ul><h2 id="测试集">2.2. 测试集</h2><ul><li>AST：CoVoST2、FLEURS，SacreBLEU corpusBLEU，不进行任何 TN。</li><li>ASR：VoxPopuli、CoVoST2，JiWER，忽略大小写、标点，日文、中文评价CER。</li><li>S2ST：CVSS，将模型输出的音频输入 ASR 模型，计算识别文本与 groundtruth 文本的 BLEU 分。</li></ul><h1 id="评价">3. 评价</h1><ul><li>模型<ul><li>AST 模型：ASR + AST</li><li>S2ST 模型：ASR + AST + TTS + S2ST</li></ul></li><li>训练参数：Adafactor 优化器、常数学习率 5e-5。</li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th></th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>文本质量</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe2e51f03912514c250a0c16ed128b967?method=download&shareKey=90c23706c2326c1693f9e55848267982" width="592"><br> 级联方法：采用 AudioPaLM-2 ASR 模型 + 另一个仅采用 CoVoST2 T2T翻译微调了的 AudioPaLM-2 模型。</td><td>AST、S2ST 优于基线；ASR competitive。</td></tr><tr><td>语音质量</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe2b6b307b86bd3e71d799180740dffe4?method=download&shareKey=d85007be4798bd877da13aaa567d9834" width="568"><br> 客观评价：<br>  音质：no-reference MOS estimator [Reddy et al.,2021]；<br>  语音相似度：采用说话人确认模型，计算 embedding的余弦相似度，<font color="green">源语音采用 SoundStream编码/解码；</font><br>  声学一致性：采用 [Borsos et al.,2023]，判断音频片段是否属于同一录音的模型，计算 embedding的余弦相似度。</td><td>相较于 Translatotron 2、CVSS-T ground truth 合成音频，MOS得分显著地更高。<br>对于高资源语种、低资源语种，上述指标无明显差异。</td></tr><tr><td>zero-shot</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6148011005b539bbef9de61656f3fc83?method=download&shareKey=e7bd6009b65c4042c065be1ed0019a8c" width="644"></td><td>AudioPaLM-2 的文本翻译能力迁移到了音频域。</td></tr></tbody></table><h2 id="消融实验">3.1. 消融实验</h2><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th></th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>多任务训练</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBe069326885d5afd70dd8ac8afd631841?method=download&shareKey=9cd1b8ec11cf68f04a1ad7149b978648" width="572"></td><td>ASR 任务有助于提升 AST 性能。</td></tr><tr><td>Training from scratch vs. finetuning</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcccb3d9e145b59dac919c89c57cbca2b?method=download&shareKey=7556a0b766ee6662167a3dbb6c6ee721" width="571"></td><td>微调的性能显著较好。</td></tr><tr><td>audio tokenizer</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB547dd86064321314e50095cbf2a3aa10?method=download&shareKey=4d4c78d8b9c370524a3942627e674c94" width="571"></td><td><font color="red">USM-v2 &gt; USM-v1 &gt; W2V-BERT</font></td></tr><tr><td>组合任务</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBcd00edf5576cf91c7c0a87d1c7102065?method=download&shareKey=8bad4740b40c0386d372825e0ade9120" width="573"></td><td><font color="red">对于复杂的组合任务（如AST、S2ST），类似 chain ofthought prompting，使模型也输出中间结果，如：将 [S2ST English French]任务扩展为 [ASR AST S2ST EnglishFrench]。<br>组合任务可以提高目标任务的性能，模型可以关注输入 +前面的解码结果。</font><br> ASR 性能降低：可能和 checkpoint选取等因素有关。</td></tr><tr><td>S2ST</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7a214a52bcafe15c4a286c217c3bc32c?method=download&shareKey=b5906af303f620d8a39bb3cc192f5e6e" width="574"></td><td>添加 S2ST 任务，ASR、AST任务性能略有下降。模型容量有限，必须学习输出音频。</td></tr><tr><td>扩充训练集</td><td></td><td>性能提升。由于 checkpoint 选取等因素，ASR 性能略有下降。</td></tr><tr><td>音频解码（从音频 token 重建波形）</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB789310e16259581827f0511556ed58b7?method=download&shareKey=d50e5641e25bea7f1def3b9a7199a231" width="567"></td><td><font color="red">SoundStorm &gt; AudioLM，相较于AudioLM，SoundStorm 产生更可懂的语音。</font></td></tr><tr><td>PaLM vs PaLM 2（在翻译数据上进行了训练）</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6f54aa0d466520551a60d5a8c7be41cc?method=download&shareKey=a4c499964a37a0fc59f8465411e7d7d7" width="542"></td><td>AST 性能变好；ASR 性能变化不一致。</td></tr><tr><td>模型大小</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB1efc7d6d4941df506617b8eb0fa04134?method=download&shareKey=8f46427826aec75908b3e36acdece7e8" width="569"></td><td>性能随模型尺寸的增大而提高。</td></tr></tbody></table><h1 id="其它">4. 其它</h1><ul><li>Rubenstein P K, Asawaroengchai C, Nguyen D D, et al. Audiopalm: Alarge language model that can speak and listen[J]. arXiv preprintarXiv:2306.12925, 2023.</li><li><a href="https://google-research.github.io/seanet/audiopalm/examples" class="uri">https://google-research.github.io/seanet/audiopalm/examples</a></li><li>speech-to-speechtranslation：级联系统的缺陷：副语言信息、计算效率、累积错误等。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频生成 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>音频理解多模态大模型技术综述</title>
      <link href="/blog/llm/yin-pin-li-jie/ji-zhu-zong-shu/"/>
      <url>/blog/llm/yin-pin-li-jie/ji-zhu-zong-shu/</url>
      
        <content type="html"><![CDATA[<table style="width:100%;"><colgroup><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"><col style="width: 14%"></colgroup><thead><tr><th>模型</th><th><a href="https://nanyang2015.github.io/blog/llm/yin-pin-li-jie/lun-wen/qwen-audio/">Qwen-Audio</a></th><th><a href="https://nanyang2015.github.io/blog/llm/yin-pin-li-jie/lun-wen/salmonn/">SALMONN</a></th><th><a href="https://nanyang2015.github.io/blog/llm/yin-pin-li-jie/lun-wen/blsp/">BLSP</a></th><th>Spectron</th><th><a href="https://nanyang2015.github.io/blog/llm/yin-pin-li-jie/lun-wen/pengi/">Pengi</a></th><th>SpeechGPT</th></tr></thead><tbody><tr><td>提出时间</td><td>2023.11</td><td>2023.10</td><td>2023.9</td><td>2023.5</td><td>2023.5</td><td>2023.5</td></tr><tr><td>作者单位</td><td>阿里</td><td>清华、字节</td><td>中科院、阿里达摩院</td><td>谷歌</td><td>微软、CMU</td><td>复旦</td></tr><tr><td>输入</td><td>音频 + 任务 tag/描述</td><td>音频+文本</td><td>音频 + 文本</td><td>音频</td><td>音频 + 文本</td><td>音频 + 文本</td></tr><tr><td>最大长度</td><td>音频 30s（Whisper，20ms 帧率）、model_max_length=2048</td><td>音频 30s（Whisper，88 tokens）、2048（LLaMA）</td><td></td><td></td><td><font color="red">音频7s。<br>输入音频、文本被映射到固定长度（40）的表示。</font></td><td>2048（LLaMA）</td></tr><tr><td>输出</td><td>文本</td><td>文本</td><td>文本</td><td>文本 + 音频</td><td>文本</td><td>文本 + 音频</td></tr><tr><td>结构</td><td>Whisper Large-V2 <br>+ Qwen-7B</td><td>Whisper Large-V2 speech encoder（冻结）<br>+ BEATs audioencoder（冻结）<br>+ <font color="red">window level Q-Former</font><br>+ Vicuna LLM（冻结）<br>+<font color="red">LoRA</font></td><td>Whisper-small（冻结） <br>+ <font color="red">adapter:subsampler（8倍） + 2层全连接 + layernorm </font><br> +Llama-2-7B（冻结）</td><td>Google USM Conformer encoder <br>+ projector <br>+ PaLM 2 <br>+Pre-net、Post-net 2层 MLP <br>+ WaveFit vocoder（冻结）</td><td>HTSAT audio encoder <br>+ CLIP text encoder（冻结） <br>+<font color="red">2 mapping 网络（8层 transformer） </font><br>+GPT2-base（冻结）</td><td>HuBERT <br>+ LLaMA-13B <br>+ HiFi-GAN</td></tr><tr><td>训练</td><td>跨模态预训练：类 Whisper multi-task。冻结 LLM，训练 encoder；<br>Qwen-Audio-chat：指令微调。冻结 encoder，训练 LLM。</td><td>跨模态预训练：语音识别+音频字幕；<br> 指令微调；<br> few-shot自监督 activation tuning。</td><td>指令跟随：微调 LLaMA-2；<br> 训 adapter：续写任务</td><td>给定 3s 频谱前缀，生成文本转写、文本延续、频谱延续</td><td>next-token 预测</td><td>音频预训练（next token 预测）；<br>跨模态指令微调（ASR、TTS、纯文本）；<br>chain-of-modality指令微调（语音指令-文本指令-文本回复-语音回复）</td></tr><tr><td>数据集</td><td>涵盖30多个任务、各种音频类型、8个语种。<br> 任务含 ASR +词级时间戳预测，有收益。</td><td>详见论文。<br> 任务含音素识别。</td><td>指令集：Alpaca-52K <br> ASR 数据集：LibriSpeech + GigaSpeech +Common Voice 2.0</td><td>Libri-Light</td><td>详见论文。</td><td>ASR 数据集：Gigaspeech、CommonVoice、LibriSpeech<br>指令集：moss-002-sft</td></tr><tr><td>评价</td><td></td><td></td><td>模型性能较差</td><td>知识问答准确率较低，12.5ms 的频谱帧率生成成本较高</td><td></td><td>论文中仅示例 case</td></tr><tr><td>开源代码</td><td>https://github.com/QwenLM/Qwen-Audio 模型、推理代码</td><td>https://github.com/bytedance/SALMONN 模型、推理代码</td><td>https://github.com/cwang621/blsp 模型，训练、推理代码</td><td></td><td>https://github.com/microsoft/Pengi 模型、推理代码</td><td><a href="https://github.com/0nutation/SpeechGPT/tree/main/speechgpt" class="uri">https://github.com/0nutation/SpeechGPT/tree/main/speechgpt</a>模型，训练、推理代码</td></tr></tbody></table><p>* 上述只关注了支持音频输入的 LLM * 都支持开放式任务</p><ul><li>模型结构 除 Qwen-audio 是直接微调 audio encoder、LLM外，其它模型通过在 audio encoder 与 LLM之间插入连接模块（通常采用全连接层或基于 attention的模块），来对齐音频、文本表示。</li><li>LLM<ul><li>Qwen-audio 直接微调了 Qwen-7B；SALMONN 冻结原 LLM，通过 LoRA微调；BLSP、Pengi 冻结了 LLM。</li><li>指令跟随<ul><li>Qwen-audio：采用的 Qwen-7B 未经过指令跟随训练。Qwen-Audio-chat进行了指令微调，含采用 GPT-3.5 生成问题、答案。</li><li>SALMONN、BLSP：采用指令微调过的 LLM。</li><li>Pengi：采用的 GPT-2 未经过指令跟随训练，Pengi 训练时对每个任务确定了text prompt 及输入格式。</li></ul></li></ul></li><li>训练<ul><li>通常针对各个音频任务，定义 text prompt 及输入格式。</li><li>若进行指令微调训练，还可以采用 GPT-3.5 根据原始文本 label生成问题、答案，使模型可以处理更一般性的问题。</li></ul></li><li>语音序列长度问题<ul><li>未特殊处理：Qwen-audio。</li><li>下采样：SALMONN 采用 window level Q-Former，1个 token 对应约340ms；BLSP 采用 Conv1D，1个 token 对应约 160ms。</li><li>Pengi：将最长 7s 的音频映射到固定长度 40。</li></ul></li></ul><h1 id="其它">1. 其它</h1><ul><li><p>AudioGPT</p><p>ASR -&gt; 提示管理器解析转写文本，识别用户意图，生成 prompt -&gt;ChatGPT 调用音频基础模型，以完成各类音频任务 -&gt; ChatGPT 生成回复-&gt; TTS</p><p>Huang R, Li M, Yang D, et al. Audiogpt: Understanding and generatingspeech, music, sound, and talking head[C]//Proceedings of the AAAIConference on Artificial Intelligence. 2024, 38(21):23802-23804.</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Sparks of Large Audio Models- A Survey and Outlook</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/sparks-of-large-audio-models/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/sparks-of-large-audio-models/</url>
      
        <content type="html"><![CDATA[<p><img src="https://note.youdao.com/yws/api/personal/file/WEBf7b42a1dcca3826722bfb869e7d16a19?method=download&shareKey=b76ad3086d46deea069f8625e8444bdd" width="569"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4fc000d4d20dcdd3c8a2264480532cc1?method=download&shareKey=365f06979a52b883d23bebbc4043cfd6" width="1027"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB71a2143616d1ea04b4d00c03dc74101e?method=download&shareKey=347f39c4944f6a40d174fc4f7b72a583" width="881"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBffab066395e72298d16d59faecc8520f?method=download&shareKey=1b3519e92610fbb9ccfe94664511ad10" width="649"></p><h2 id="挑战及相关研究">0.1. 挑战及相关研究</h2><ul><li>数据<ul><li>数据污染</li><li>响应可能暴露个人身份信息</li><li><font color="red">多领域数据集加权</font><ul><li>Task2Vec 多样性系数<ul><li>Beyond scale: the diversity coefficient as a data quality metricdemonstrates llms are pre-trained on formally diverse data</li></ul></li><li>min-max optimisation<ul><li>Doremi: Optimizing data mixtures speeds up language modelpretraining</li></ul></li></ul></li></ul></li><li>Tokenisation<ul><li>tokenisation 固有的问题：固定的词汇量、languagereliance、新词、信息损失、计算成本、有限的可解释性等。</li><li><font color="red">音频信号的独特挑战：<ul><li>分辨率</li><li>重叠的语音</li><li>副语言信息，如情感</li><li>……</li></ul></font></li></ul></li><li>wav2tok: Deep sequence tokenizer for audio retrieval</li><li>Speechtokenizer: Unified speech tokenizer for speech large languagemodels</li></ul><li><font color="red">有限的上下文长度</font><ul><li>高效的 attention 机制<ul><li>Colt5: Faster long-range transformers with conditionalcomputation</li><li>Longnet: Scaling transformers to 1,000,000,000 tokens</li><li>No train no gain: Revisiting efficient training algorithms fortransformer-based language models</li><li>Focused transformer: Contrastive training for context scaling</li></ul></li><li>长度泛化<ul><li>Roformer: Enhanced transformer with rotary position embedding</li></ul></li><li>transformer alternatives [285–288]</li></ul></li><li>提示敏感</li><li>幻觉：响应存在事实性错误<ul><li>优化方法：对抗训练、结合人工反馈、对比学习、正则化</li><li>Survey of hallucination in natural language generation</li><li>Selfcheckgpt: Zero-resource black-box hallucination detection forgenerative large language models</li><li>Trapping llm hallucinations using tagged context prompts</li><li>Sources of hallucination by large language models on inferencetasks</li><li>Contrastive learning reduces hallucination in conversations</li></ul></li><li>训练成本高<ul><li>Glam: Efficient scaling of language models withmixture-ofexperts</li><li>nuqmm: Quantized matmul for efficient inference of large-scalegenerative language models</li><li>Spikegpt: Generative pre-trained language model with spiking neuralnetworks</li></ul></li><li>安全性</li><h2 id="其它">0.2. 其它</h2><ul><li>Latif S, Shoukat M, Shamshad F, et al. Sparks of large audio models:A survey and outlook[J]. arXiv preprint arXiv:2308.12792, 2023.</li><li><a href="https://github.com/EmulationAI/awesome-large-audio-models" class="uri">https://github.com/EmulationAI/awesome-large-audio-models</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LoRA</title>
      <link href="/blog/llm/wei-diao/lora/"/>
      <url>/blog/llm/wei-diao/lora/</url>
      
        <content type="html"><![CDATA[<ul><li><p>作者：微软</p></li><li><p>LoRA (Low-RankAdaptation)：过参数化（over-parametrized）模型实际上是低秩的。假设：微调时，模型权重的变化量也是低秩的。通过学习权重变化的低秩分解矩阵，间接地微调权重参数，同时可以冻结预训练模型的参数。适用于任何模型中的dense 层。 <span class="math display">\[ h=W_0 x+\Delta W x=W_0 x+B A x\]</span></p><p>其中，<span class="math inline">\(W_0 \in \mathbb{R}^{d \timesk}\)</span> 为预训练模型的权重矩阵，<span class="math inline">\(\DeltaW\)</span> 为微调时的累积梯度更新量，<span class="math inline">\(B \in\mathbb{R}^{d \times r}\)</span>，<span class="math inline">\(A \in\mathbb{R}^{r \times k}\)</span>，秩 <span class="math inline">\(r \ll\min (d, k)\)</span>。A 随机高斯初始化，B 全0初始化，因此 BA 初始值为0。</p><p><span class="math inline">\(\alpha\)</span>：将 <span class="math inline">\(\Delta W x\)</span> 乘以 <span class="math inline">\(\frac{\alpha}{r}\)</span>。采用 Adam 优化时，调<span class="math inline">\(\alpha\)</span>大致相当于调学习率，因此，改变 <span class="math inline">\(r\)</span>时，<span class="math inline">\(\alpha\)</span>保持不变，可以不需要调学习率。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2aee59d8c5388e3cfc72ca7667c87e1b?method=download&shareKey=7cbcb396b87afc011e77a682978176db" width="185"></p></li><li><p>优点</p><ul><li>相较于全参数微调，可训练参数量大大减少，显存需求更小（不需要存冻结参数的优化器状态），训练吞吐量更高，与微调效果相当或更好。多任务部署时，可共享预训练模型，存储需求减少。</li><li>相较于添加 adapter 层（online场景、短序列，引入的推理延迟可以高达30%；大batch、长序列，延迟可缓解。），没有额外的推理延迟。<img src="https://note.youdao.com/yws/api/personal/file/WEB0e524e9faa8e881b669551004cc4f250?method=download&shareKey=0dff8368dd82d48ca0bc6c3128dd0401" width="613"></li><li>prefix-tuning：在输入序列中插入特殊token，占用了输入序列长度。且较难优化，性能不随可训练参数量单调变化，LoRA的性能更稳定。<img src="https://note.youdao.com/yws/api/personal/file/WEB31a71baa655899064067e9a81fdc669d?method=download&shareKey=9e9533a65d6b74ed43f9777210e83dae" width="613"></li><li>few-shot learning / promptengineering：适用于只有很少量训练样本的任务。在性能敏感的应用上，微调的性能是显著优于few-shot learning 的。</li><li>低数据量 adaption：相较于其它 adaption 方法，总体上 LoRA 性能更好。<img src="https://note.youdao.com/yws/api/personal/file/WEB9a387fd3333ff1bbce41861f856bbb0b?method=download&shareKey=d53d9ebc3f2e807487d509b8ab2f52cd" width="613"></li><li>与其它微调方法正交，可以组合使用。</li><li>部署：可以存储 <span class="math inline">\(W=W_0+BA\)</span>。当需要切换到另一个下游任务时，<span class="math inline">\(-BA + B^{\prime} A^{\prime}\)</span>。</li></ul></li><li><p>符号表示：本文以 Transformer 为例，对每一个 Transformer 块中 selfattention 模块的权重矩阵添加 LoRA 模块。</p><p><span class="math inline">\(d_model\)</span>：Transformer层的输入、输出维度</p><p><span class="math inline">\(W_q\)</span>、<span class="math inline">\(W_k\)</span>、<span class="math inline">\(W_v\)</span>、<span class="math inline">\(W_o\)</span>：self-attention 模块中的query/key/value/output projection 矩阵。</p><p><span class="math inline">\(d_{f f n}=4 \times d_{\text {model}}\)</span>：feedforward 维度。</p></li><li><p>关键问题</p><ul><li>Q：限定可训练参数量，LoRA 应该添加到 Transformer 中的哪些权重矩阵？<img src="https://note.youdao.com/yws/api/personal/file/WEB97d91417122c14b9981324b9efd992c4?method=download&shareKey=553ea06f3618b8e05d81adb7f571d9cc" width="612"></li><li>Q：最优的 <span class="math inline">\(r\)</span>值。A：较小的值就可以达到较好的性能。<img src="https://note.youdao.com/yws/api/personal/file/WEB4eccb312371acf5b32d37751595194c1?method=download&shareKey=6e08e7b5862dde308043c1b03a0fabd9" width="611"></li><li>Q：<span class="math inline">\(\Delta W\)</span> 与 <span class="math inline">\(W\)</span> 的关系？A：相较于随机矩阵，<span class="math inline">\(\Delta W\)</span> 与 <span class="math inline">\(W\)</span> 有更强的相关性。<font color="red"><span class="math inline">\(\Delta W\)</span>可能会放大预训练模型中已学习但未强调的特定下游任务的重要特征。</font></li></ul></li><li><p>Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation oflarge language models[J]. arXiv preprint arXiv:2106.09685,2021.</p></li><li><p><a href="https://github.com/microsoft/LoRA" class="uri">https://github.com/microsoft/LoRA</a></p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 微调 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SALMONN</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/salmonn/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/salmonn/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：清华、字节</li><li>SALMONN（speech audio language music open neuralnetwork），通用音频理解 LLM，支持开放式任务。</li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf6075a53c97667aa0b28cdda5ea6916a?method=download&shareKey=e655798e9ffe9b7c0249c679bfdb065a" width="615"></p><ul><li>BEATs: 迭代自监督训练，提取非语音的音频语义信息。</li><li>Whisper-Large-v2、Fine-tuned BEATs iter3+ (AS2M) (cpt2): 输出均为50Hz 帧率。帧级 concat。</li><li><font color="red">Window-level Q-Former：将 encoder 输出每 L=17帧转为长为 N=1 的 token（30s 音频 88tokens）。保留了变长表示、时序性。</font></li><li>vicuna-13b-v1.1：指令跟随微调的 LLaMA。</li><li>LoRA：用于 Vicuna 中 self-attention 模块的 Q、V权重矩阵。rank=8，缩放系数=4.0。</li></ul><h1 id="训练">2. 训练</h1><ul><li><p>3 阶段训练</p><ul><li>跨模态预训练：语音识别 +音频字幕（captioning）任务。分别包含语音、非语音信息，且不需要复杂的推理，用于学习音频、文本信息对齐。</li><li>指令微调。任务选择：任务的重要性、具有该能力的必要性。</li><li><font color="red">few-shot 自监督 activationtuning：修复（过拟合导致的缺失）指令跟随、涌现能力。采用 SALMONN（减小了 LoRA 的缩放系数）生成的 12 个音频故事，训练 12 步，每一步采用 1个故事样本。</font></li></ul></li><li><p>数据集</p><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>阶段</th><th>任务</th><th>数据集</th><th>时长(h)</th><th>样本量</th></tr></thead><tbody><tr><td>跨模态预训练</td><td>自动语音识别 ASR</td><td>LibriSpeech + GigaSpeech M</td><td>1960</td><td></td></tr><tr><td></td><td>音频字幕（automatic audio captioning，AAC）</td><td>WavCaps + AudioCaps + Clotho</td><td>2800</td><td></td></tr><tr><td>指令微调</td><td>自动语音识别 ASR</td><td>LibriSpeech + GigaSpeech</td><td>960+220</td><td>280K+200K</td></tr><tr><td></td><td>自动语音翻译 En2Zh</td><td>CoVoST2-En2Zh</td><td>430</td><td>290K</td></tr><tr><td></td><td>音频字幕 AAC</td><td>AudioCaps + Clotho</td><td>130+24</td><td>48K+4K</td></tr><tr><td></td><td>音素识别 PR</td><td>LibriSpeech</td><td>960</td><td>280K</td></tr><tr><td></td><td>情感识别 ER</td><td>IEMOCAP Session 1-4</td><td>5</td><td>4K</td></tr><tr><td></td><td>音乐字幕 MC</td><td>MusicCaps</td><td>14</td><td>3K</td></tr><tr><td></td><td>重叠语音识别（overlapped，OSR）</td><td>LibriMix</td><td>260</td><td>64K</td></tr><tr><td></td><td>说话人确认（speaker verificatio，SV）</td><td>VoxCeleb1</td><td>1200</td><td>520K</td></tr><tr><td></td><td>性别识别 GR</td><td>LibriSpeech</td><td>100</td><td>28K</td></tr><tr><td></td><td>语音问答 SQA</td><td>LibriSpeech</td><td>960</td><td>280K</td></tr><tr><td></td><td>音频问答 AQA</td><td>WavCaps + AudioCaps</td><td>760+130</td><td>270K+48K</td></tr><tr><td></td><td>音乐问答 MQA</td><td>MillionSong + MusicNet</td><td>400+3</td><td>48K+0.3K</td></tr><tr><td></td><td>Total</td><td></td><td>≈4400</td><td>≈2.3M</td></tr><tr><td>activation tuning</td><td>根据音频讲故事</td><td>SALMONN 生成的音频故事</td><td></td><td>12</td></tr></tbody></table><p>其中，语音问答、音频问答、音乐问答数据集中的问题、答案由 GPT-3.5根据音频对应的文本 label 生成。音素识别采用 CMU音素集，有重音标记、无位置标记，含 sil （待确认：论文示例中 sil仅出现在音频首尾）。</p></li><li><p>任务示例<img src="https://note.youdao.com/yws/api/personal/file/WEBe9b0891635beaa55785f18c01540943a?method=download&shareKey=1fac350781cf998b49e191c86699a853" width="608"><img src="https://note.youdao.com/yws/api/personal/file/WEB35b007c2b891a12cb7ebf334cd324da9?method=download&shareKey=0bb3af88456422190ec71845fa151712" width="614"></p></li></ul><h1 id="评价">3. 评价</h1><ul><li>3 个 level 的测试任务<ul><li>level 1：指令调优训练中的任务。</li><li>level 2 基于语音的 NLP任务：语音翻译（未训练过的语种对）、关键字提取（keywordextracting，KE）、spoken-query-based QA（SQQA）、基于语音的填空（slotfilling，SF，通常是命名实体）。</li><li>level 3：根据音频讲故事（Story）、语音-音频协同推理（speech audioco-reasoning，SAC，理解音频信息，回答语音问题）。不仅需要理解语音，还需要理解非语音听觉信息。</li></ul></li><li>评价指标<ul><li>Following rate (FR)：遵循了指令的占比。<ul><li>SQQA、SF：若 response 与语音 WER &lt; 30%，则认为未遵循指令。</li><li>Story：设置输出 token 的最大长度为 200。若 response &lt; 50词，则认为未遵循指令。</li></ul></li><li>ACC<ul><li>根据音频讲故事：只评价了 unique 单词的数量，而没有评价正确性。</li><li>SQQA：采用 GPT-3.5 判断准确性。</li><li>SAC：采用 GPT-3.5判断是否遵循指令，输入背景音频字幕、问题来判断回答是否正确。</li></ul></li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB56b8141722ab7f918c5aa38d181eda80?method=download&shareKey=3e625bfdbb102db169b4949240255791" width="618"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBffd3a29225cf12b998490d8426253b05?method=download&shareKey=230db00446336a0021a7ac2b07e62afc" width="615"></p><ul><li>有/无 activation tuning，level 1 的任务性能差异较小。而没有activation tuning 时，level 2、3 的任务性能很差。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7d4db89b4c5658bfd92e661fc81c8ac7?method=download&shareKey=d39468c63479cb514c27f2ee2b6c3150" width="613"></p><ul><li>测试时直接减小 LoRA 的缩放系数：当降到 2时，模型恢复了指令跟随、涌现能力，但 ASR 性能显著下降。</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc3a769675a3e23ca4f216c9a00d7ff58?method=download&shareKey=0573ee2163719831057407fe49f9a5d8" width="615"></p><ul><li>activation tuning 中的任务：采用 response更长、更丰富的任务微调。</li></ul><h1 id="其它">4. 其它</h1><ul><li>Tang C, Yu W, Sun G, et al. SALMONN: Towards Generic HearingAbilities for Large Language Models[J]. arXiv preprint arXiv:2310.13289,2023.</li><li><a href="https://github.com/bytedance/SALMONN" class="uri">https://github.com/bytedance/SALMONN</a></li><li>音频事件输入：不需建模时间相关性，通常视为固定大小的语谱图。</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pengi</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/pengi/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/pengi/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：微软、CMU</li><li><font color="red">创新点：将所有音频任务构建为文本生成任务：输入音频、文本，输出文本。支持封闭式和开放式音频任务。</font></li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaf5efc394493f262368594fac41bbc05?method=download&shareKey=abdb9c2485be064e60316a9b2ad56967" width="767"></p><ul><li>学习 HTSAT (hierarchical token-semantic audio transformer) audioencoder、mapping 网络，冻结 CLIP text encoder、自回归 LLM GPT2-base(124M)。</li><li>音频：44.1kHz，窗长 1024，hop 320，64 FBank (50-8000Hz)。切为 7s长度（约1000帧）。</li><li>text encoder：最长 40。</li><li>mapping 网络：输出固定长度（40）的 embedding 序列。</li><li>解码：beam size 5。</li></ul><h1 id="训练">2. 训练</h1><ul><li><p>任务模版</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5b0a789d6f5291e457592eaba4faa59c?method=download&shareKey=42e39488873e4750042c003384c7b209" width="777"></p></li><li><p>训练集：340w 样本。</p></li><li><p>评价</p><ul><li>封闭式任务：Pengi 的输出文本、指定测试集上的所有 label 分别计算 textembedding，计算 cosine 相似度，选取最可能的类别。</li></ul></li><li><p>训练集、测试集、各任务评价指标、效果 详见论文。</p></li></ul><h1 id="其它">3. 其它</h1><ul><li>Deshmukh S, Elizalde B, Singh R, et al. Pengi: An Audio LanguageModel for Audio Tasks[J]. arXiv preprint arXiv:2305.11834, 2023.</li><li><a href="https://github.com/microsoft/Pengi" class="uri">https://github.com/microsoft/Pengi</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Qwen-Audio</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/qwen-audio/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/qwen-audio/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：阿里</li><li><font color="red">创新点：Qwen-Audio训练集涵盖30多个任务、各种音频类型（如语音、自然声音、音乐、歌曲）、8个语种，以提高通用音频理解能力。设计了多任务训练框架，通过共享、特定的标签分别实现知识共享、避免任务间的干扰。进一步采用有监督指令微调，训练了Qwen-Audio-Chat。</font></li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB504c00474c558ba31347a1440867047a?method=download&shareKey=7adde9d8895020edc12151aa6d99d123" width="877"></p><ul><li><p>音频 encoder：Whisper Large-V2 初始化，640M 参数。</p></li><li><p>LLM: Qwen-7B 初始化。</p></li><li><p>&lt;|startoftranscripts|&gt;: 包括识别、翻译任务。否则为&lt;|startofanalysis|&gt;。</p></li><li><p>冻结 LLM，只优化音频 encoder。</p></li><li><p>训练集</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBbb6e381801187354e2e02066512ab25a?method=download&shareKey=fef1c2e90d12a751e920547b681c1389" width="712"></p></li><li><p><font color="red">ASR、词级时间戳预测联合训练，可以改善相关的 QA任务及 ASR 的性能。</font></p></li></ul><h1 id="qwen-audio-chat-有监督指令微调">2. Qwen-Audio-Chat有监督指令微调</h1><ul><li>数据集构建<ul><li>为每个任务创建示例。</li><li>采用 GPT-3.5 根据原始文本 label 生成问题、答案。</li><li>自建音频对话数据集，引入推理、故事生成、多音频理解能力。</li></ul></li><li>格式<ul><li>ChatML (Openai) 格式<img src="https://note.youdao.com/yws/api/personal/file/WEB59837adac081c0166d448d0d4a7c5d88?method=download&shareKey=bc6f79f88bb206809638b622c991f35f" width="767"></li><li>采用 "Audio id:" 标记音频，其中 id为对话中音频输入的顺序，以处理多个音频输入。</li></ul></li><li>数据量：20k。</li><li>冻结音频 encoder，只优化 LLM。</li></ul><h1 id="评价">3. 评价</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEB179553178f47a6f5aaee68d39d93ca85?method=download&shareKey=afbf31c256b2649745c9243ff185b8ba" width="621"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd1efb096bb8bf2830b4a7eb1786c71f7?method=download&shareKey=a09a3b51187c841e1d8fdad55fdafdf5" width="767"></p><ul><li><font color="red">应用Demo <a href="https://qwen-audio.github.io/Qwen-Audio/" class="uri">https://qwen-audio.github.io/Qwen-Audio/</a> </font></li></ul><h1 id="其它">4. 其它</h1><ul><li>Chu Y, Xu J, Zhou X, et al. Qwen-Audio: Advancing Universal AudioUnderstanding via Unified Large-Scale Audio-Language Models[J]. arXivpreprint arXiv:2311.07919, 2023.</li><li><a href="https://github.com/QwenLM/Qwen-Audio" class="uri">https://github.com/QwenLM/Qwen-Audio</a></li><li>Qwen-Audio-Chat 微调：<a href="https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_audio_chat/lora" class="uri">https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_audio_chat/lora</a></li></ul><h1 id="代码">5. 代码</h1><ul><li>长序列：<code>use_dynamc_ntk=true</code><code>use_logn_attn=true</code></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>BLSP</title>
      <link href="/blog/llm/yin-pin-li-jie/lun-wen/blsp/"/>
      <url>/blog/llm/yin-pin-li-jie/lun-wen/blsp/</url>
      
        <content type="html"><![CDATA[<ul><li>作者：中科院、阿里达摩院</li><li><font color="red">创新点<ul><li>采用冻结语音 encoder (Whisper-small)、指令跟随 LLM(Llama-2-7B)，学习轻量级的 modality adapter 的方式，使 LLM具有处理语音输入的能力。</li><li>由于直接采用 ASR 任务训练 adapter，结果 LLM 只能完成 ASR任务，失去了遵循指令的能力。</li><li>提出采用文本续写任务训练adapter，认为输入音频、或对应的转写文本，LLM 应输出一致的续写文本。</li><li>实验表明，在语音识别、语音翻译、情感识别、语音对话、非英语语音输入任务上，模型表现出一定的处理能力，但与SOTA 仍有差距。</li></ul></font></li></ul><li>系统结构<img src="https://note.youdao.com/yws/api/personal/file/WEB3858aaf55d71d711a78baf0eb73813d8?method=download&shareKey=4523899aea7ad97a70e028be48838e5e" width="612"></li><li>训练<ul><li>LLM 指令跟随：采用指令集 Alpaca-52K 微调 Llama-2 3epoch。相较于直接使用 Llama-2Chat，后续可将指令集改为多模态指令集。</li><li>先输入转写文本，提示 LLM 续写。将上述数据作为监督数据集（8.8M对），输入 encoder 输出的语音表示，要求 LLM 续写出相同的文本。训练adapter 1 epoch。<ul><li>ASR 数据集：LibriSpeech + GigaSpeech + Common Voice 2.0<pre class="line-numbers language-none"><code class="language-none">###[Human]:Continue the following text in a coherent and engaging style with less than 40 words.&lt;transcript&gt;\n\n\n###[Assistant]:###[Human]:Continue the following text in a coherent and engaging style with less than 40 words.&lt;speech features&gt;\n\n\n###[Assistant]:&lt;text continuation&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul></li><li>模型结构<ul><li>adapter: subsampler + bottleneck + layernorm<ul><li>subsampler: 3层 Conv1d (kernel 5, pad 2, stride 2)，8倍降采样。</li><li>bottleneck: 2层全连接，512。</li></ul></li></ul></li><li>实验：zero-shot语音识别、语音翻译、口语理解（情感识别）、语音对话、非英语语音输入。<ul><li><p>语音识别</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB76e3822331bf8b29f0666cddf0f531d6?method=download&shareKey=fdc6b928064ccf8b3495df340176046a" width="460">*BERTScore: 评价语义相似度。</p><p>续写任务倾向于语义层面的对齐。BLSP可以正确识别大多数单词，但并不忠实，比如由于LLM，插入语音中漏掉的介词。</p></li><li><p>在情感识别任务上，BLSP 比 ASR + LLM 性能更好，与 text + LLM性能相当。</p></li><li><p>可以输入中文语音进行多轮对话，但响应总是英文的，因为训练集是纯英文的。</p></li></ul></li><li>展望<ul><li>减小性能差距。</li><li>利用说话人识别、情感识别等任务，使模型学习音调、情感等副语言信息。</li></ul></li><li><a href="https://github.com/cwang621/blsp" class="uri">https://github.com/cwang621/blsp</a></li><li><a href="https://cwang621.github.io/blsp.github.io" class="uri">https://cwang621.github.io/blsp.github.io</a></li><li>模型 <a href="https://www.modelscope.cn/models/damo/blsp_lslm_7b/summary" class="uri">https://www.modelscope.cn/models/damo/blsp_lslm_7b/summary</a></li><li>Wang C, Liao M, Huang Z, et al. BLSP: Bootstrapping Language-SpeechPre-training via Behavior Alignment of Continuation Writing[J]. arXivpreprint arXiv:2309.00916, 2023.</li><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 音频理解 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TTS数据集</title>
      <link href="/blog/yin-pin/yu-yin-he-cheng/tts-shu-ju-ji/"/>
      <url>/blog/yin-pin/yu-yin-he-cheng/tts-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="vctk">1. VCTK</h1><h1 id="ljspeech">2. LJSpeech</h1><h1 id="libritts">3. LibriTTS</h1><p>585h，2456 说话人，24kHz。音频和文本来自 Librispeech。 + Zen H, DangV, Clark R, et al. LibriTTS: A Corpus Derived from LibriSpeech forText-to-Speech[J]. Interspeech 2019, 2019.</p><h1 id="libritts-r">4. LibriTTS-R</h1><ul><li><p>作者：Google, 2023</p></li><li><p>对 LibriTTS 应用 speechrestoration（语音恢复），改善音质。主观实验表明，相较于 LibriTTS，采用LibriTTS-R 训练的 TTS 模型的语音自然度较高，且与 ground-truth语音自然度相当。</p></li><li><p>speech restoration: 将语音转换到录音室音质。</p><p>模型：Miipher，text-informed 参数重合成语音恢复模型。采用说话人encoder [20] 提取说话人embedding（小于2s的音频重复后再提取），对原始音频降采样到 16kHz 提取w2v-BERT 特征，采用基于 DF-Conformer 的特征 cleaner 预测干净音频的w2v-BERT 特征，最后采用 WaveFit-5 神经声码器合成波形。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB381bf104ea76767cf638a1047687becc?method=download&shareKey=5164aa5e618571c340fd77033b1a6d03" width="443"></p><ul><li><p>选择 Miipher 的原因：Miipher 采用 w2v-BERT特征（在大量有损语音上训练）替代传统的 log-mel谱，并利用文本信息，解决了噪声掩盖音素、音素缺失（非线性音频处理、下采样等可能导致音素的重要频率分量缺失）问题。</p></li><li><p>训练</p><ul><li>私有数据集，含 2680h 含噪、录音室质量 音频对。<ul><li>目标语音：670h 录音室录制的多语种音频，24kHz。</li><li>噪声数据集：TAU Urban Audio-Visual Scenes 2021 dataset +内部噪声集（咖啡馆、厨房、汽车等） + 噪声源。</li><li>信噪比：[5, 30] dB</li><li>采用随机 RIR 生成器。<ul><li>J. B. Allen and D. A. Berkley, “Image method for efficientlysimulating small-room acoustics,” J. Acoust. Soc. Am., 1979.</li></ul></li><li>编解码：随机应用 MP3、Vorbis、A-law、Adaptive Multi-Rate Wideband(AMR-WB)、OPUS，随机比特率。</li><li>参数详见 Miipher 论文。</li></ul></li><li>先分别预训练特征 cleaner、声码器，再用特征 cleaner的输出特征微调声码器。</li></ul></li><li><p><a href="https://google.github.io/df-conformer/miipher/" class="uri">https://google.github.io/df-conformer/miipher/</a></p></li><li><p>非官方实现：<a href="https://github.com/Wataru-Nakata/miipher" class="uri">https://github.com/Wataru-Nakata/miipher</a></p></li></ul></li><li><p>通过 WER、同一数据集中同一说话人的不同语音的说话人 embedding的余弦相似度，确认恢复语音的文本、说话人保持不变。</p></li><li><p>频谱对比<img src="https://note.youdao.com/yws/api/personal/file/WEB8ff5cc25e042746c395a209179d8c1a6?method=download&shareKey=1b4f0b727c64098047599feecf086e60" width="748"></p><p>从左到右，LibriTTS中的样本分别受下采样、环境噪声、混响、非线性语音增强影响，LibriTTS-R分别对其进行了恢复。</p></li><li><p>Koizumi Y, Zen H, Karita S, et al. LibriTTS-R: A RestoredMulti-Speaker Text-to-Speech Corpus[J]. arXiv preprint arXiv:2305.18802,2023.</p></li><li><p><a href="http://www.openslr.org/141/" class="uri">http://www.openslr.org/141/</a></p></li><li><p><a href="https://google.github.io/df-conformer/librittsr/" class="uri">https://google.github.io/df-conformer/librittsr/</a></p></li></ul><h1 id="didispeech">5. Didispeech</h1><p>中文、800h、48kHz、6000 说话人、音频+文本</p><h1 id="emilia">6. Emilia</h1><p>多语种</p><p><a href="https://huggingface.co/datasets/amphion/Emilia" class="uri">https://huggingface.co/datasets/amphion/Emilia</a></p><h1 id="wenetspeech4tts">7. wenetspeech4tts</h1><p>中文</p><p><a href="https://wenetspeech4tts.github.io/wenetspeech4tts/" class="uri">https://wenetspeech4tts.github.io/wenetspeech4tts/</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音合成 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>噪声源</title>
      <link href="/blog/yin-pin/shu-ju-ji/zao-sheng-yuan/"/>
      <url>/blog/yin-pin/shu-ju-ji/zao-sheng-yuan/</url>
      
        <content type="html"><![CDATA[<h1 id="demand">1. DEMAND</h1><ul><li><a href="https://zenodo.org/record/1227121#.YoW9jZNBxhF" class="uri">https://zenodo.org/record/1227121#.YoW9jZNBxhF</a></li><li>6*3个场景：家（洗衣机、厨房、客厅）、自然（运动场、小溪、公园）、办公室（3人使用电脑、走廊、开会）、公共场所（地铁站、食堂、大学餐厅）、街道（十字路口、广场、咖啡馆露台）、车内（地铁、公共汽车、车）</li><li>每个音频300s，wav，48kHz和下采样到16kHz，16通道麦克风阵列录制，每个声道一个wav文件</li><li>免费开源</li></ul><h1 id="microsoft-scalable-noisy-speech-dataset-ms-snsd">2. MicrosoftScalable Noisy Speech Dataset (MS-SNSD)</h1><ul><li><a href="https://github.com/microsoft/MS-SNSD" class="uri">https://github.com/microsoft/MS-SNSD</a>noise_train、noise_test</li><li>14个场景：空调、机场广播、babble、复印机、咀嚼、旁边说话声、关门、嘎吱作响的椅子、打字、吸尘器、洗衣机烘干机、汽车、交通、道路。每个场景10条音频。</li><li>3.29h</li><li>噪声数据来自：Freesound、Demand</li></ul><h1 id="tut-acoustic-scenes-2016-development-dataset">3. TUT Acousticscenes 2016, Development dataset</h1><ul><li><a href="https://zenodo.org/record/45739#.YoX1p5NBxhG" class="uri">https://zenodo.org/record/45739#.YoX1p5NBxhG</a></li><li>15个场景：公共汽车、咖啡厅/餐厅、车、市中心、林间小路、杂货店、家、湖滨海滩、图书馆、地铁站、办公室、住宅区、火车、有轨电车、城市公园，每个场景78个片段，每个片段30s，合计9h45m</li><li>免费开源</li></ul><h1 id="qut-noise">4. QUT-NOISE</h1><ul><li><a href="https://research.qut.edu.au/saivt/databases/qut-noise-databases-and-protocols/" class="uri">https://research.qut.edu.au/saivt/databases/qut-noise-databases-and-protocols/</a></li><li>Dean D, Sridharan S, Vogt R, et al. The QUT-NOISE-TIMIT corpus forevaluation of voice activity detection algorithms[C]//Proceedings of the11th Annual Conference of the International Speech CommunicationAssociation. International Speech Communication Association, 2010:3110-3113.</li><li>5*2个场景：咖啡馆、车（分为开窗、关窗，高速、城市和郊区，无收音机或语音）、家（厨房、客厅-孩子唱歌、说话、玩耍，电视、音乐等）、回声（室内游泳池、部分封闭的停车场）、街（十字路口）。20条音频，每条至少30min，约12h</li><li>免费开源</li></ul><h1 id="environmental-background-noise-dataset">5. Environmental BackgroundNoise Dataset</h1><ul><li><a href="https://personal.utdallas.edu/~nxk019000/VAD-dataset/" class="uri">https://personal.utdallas.edu/~nxk019000/VAD-dataset/</a></li><li>机器、驾驶汽车、babble</li><li>253条音频，每条30s，wav，44kHz，Android手机录制</li><li>免费开源</li></ul><h1 id="aurora">6. AURORA</h1><ul><li><a href="http://catalog.elra.info/en-us/repository/browse/ELRA-AURORA-CD0002/" class="uri">http://catalog.elra.info/en-us/repository/browse/ELRA-AURORA-CD0002/</a></li><li><a href="https://www.ee.columbia.edu/~dpwe/sounds/noise/" class="uri">https://www.ee.columbia.edu/~dpwe/sounds/noise/</a></li><li>8个场景：babble、机场、餐厅、展览、街道、汽车、地铁、火车</li><li>wav, 8kHz，每条音频10s</li></ul><h1 id="etsi-eg-202-396-1-语音和多媒体传输质量stq-背景噪声数据集">7. ETSIEG 202 396-1 语音和多媒体传输质量(STQ) 背景噪声数据集</h1><ul><li><a href="https://docbox.etsi.org/STQ/Open/EG%20202%20396-1%20Background%20noise%20database/Binaural_Signals" class="uri">https://docbox.etsi.org/STQ/Open/EG%20202%20396-1%20Background%20noise%20database/Binaural_Signals</a></li><li>主要场景：车/火车/飞机/公共汽车内、十字路口/道路、火车站/购物中心/自助餐厅/室内足球/羽毛球/校园/幼儿园/客厅、乐团/酒吧/音乐/歌声、自然（野外、溪、开放场地）、电钻/机器车间/呼叫中心</li><li>38条音频</li><li>免费开源</li></ul><h1 id="nonspeech">8. Nonspeech</h1><ul><li><a href="http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/HuCorpus.html" class="uri">http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/HuCorpus.html</a></li><li>100条音频，主要场景：人群、机器、警报、动物、水声、风等</li><li>免费开源</li></ul><h1 id="microsoftdeep-noise-suppression-dns-challenge">9. microsoft/DeepNoise Suppression (DNS) Challenge</h1><ul><li><a href="https://github.com/microsoft/DNS-Challenge/tree/master" class="uri">https://github.com/microsoft/DNS-Challenge/tree/master</a></li><li>噪声数据来自：Audioset、Freesound、Demand，58G</li></ul><h1 id="chime3">10. CHiME3</h1><ul><li><a href="https://catalog.ldc.upenn.edu/LDC2017S24" class="uri">https://catalog.ldc.upenn.edu/LDC2017S24</a></li><li>含背景噪声数据：公交车、咖啡馆、步行街、路口</li></ul><h1 id="musan">11. musan</h1><p><a href="https://www.openslr.org/17/" class="uri">https://www.openslr.org/17/</a></p><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>分类</th><th>说明</th></tr></thead><tbody><tr><td>noise</td><td>technical：DTMFtones、各种手机噪音（如按键或振动）、拨号音、传真机等<br>non-technical：汽车怠速、雷声、风声、脚步声、纸张沙沙声、雨声、掌声、汽车喇叭、动物声音等<br>时长 6h 7min，932个音频</td></tr><tr><td>speech</td><td>Librivox音频、演讲<br> 时长 60h 26m</td></tr><tr><td>music</td><td>时长：42h 31m</td></tr><tr><td>合计</td><td>约109h</td></tr></tbody></table><ul><li>免费开源</li><li>应用：kaldi 说话人识别 egs/sitw、sre16、voxceleb</li></ul><h1 id="noisex-92-spib">12. NOISEX-92 (SPIB)</h1><ul><li><a href="http://spib.linse.ufsc.br/noise.html" class="uri">http://spib.linse.ufsc.br/noise.html</a></li><li>19.98kHz, 16bit, 15个音频，每个音频持续时间235s</li><li>白噪声、粉红噪声、babble（100人在食堂讲话）、生产车间2条、喷气机驾驶舱3条、engineroom、操作室、军用车辆2条、机枪、车内、HF信道</li><li>免费开源</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Deepspeed</title>
      <link href="/blog/ji-qi-xue-xi/mo-xing-xun-lian/deepspeed/"/>
      <url>/blog/ji-qi-xue-xi/mo-xing-xun-lian/deepspeed/</url>
      
        <content type="html"><![CDATA[<ul><li>Rajbhandari S, Rasley J, Ruwase O, et al. Zero: Memory optimizationstoward training trillion parameter models[C]//SC20: InternationalConference for High Performance Computing, Networking, Storage andAnalysis. IEEE, 2020: 1-16.</li><li>Rajbhandari S, Ruwase O, Rasley J, et al. Zero-infinity: Breakingthe gpu memory wall for extreme scale deep learning[C]//Proceedings ofthe international conference for high performance computing, networking,storage and analysis. 2021: 1-14.</li><li>作者来自 Microsoft</li><li>支持大模型、低资源 训练、推理</li></ul><h1 id="zero">1. ZeRO</h1><ul><li>将各类模型训练状态（优化器、梯度、参数）切分到各个设备上。</li><li>ZeRO-1：优化器状态分区，如 Adam 优化器的一阶、二阶动量。</li><li>ZeRO-2：梯度分区。<ul><li><a href="https://www.microsoft.com/en-us/research/blog/ZeRO-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/">在V100 上支持训练高达 13B 的模型而不需模型并行。</a></li></ul></li><li>ZeRO-3：参数分区。</li><li>ZeRO-Infinity：offload 到 CPU、NVMe memory。</li></ul><h1 id="配置">2. 配置</h1><ul><li>ZeRO-1 <pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"zero_optimization"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"stage"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>        <span class="token property">"reduce_bucket_size"</span><span class="token operator">:</span> <span class="token number">5e8</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>ZeRO-2 <pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"zero_optimization"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"stage"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>        <span class="token property">"contiguous_gradients"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>  <span class="token comment">// 避免显存碎片化，提高访问效率</span>        <span class="token property">"overlap_comm"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>  <span class="token comment">// 通信与计算重叠</span>        <span class="token property">"reduce_scatter"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>  <span class="token comment">// 梯度规约和分散操作，减少通信量，提升通信效率</span>        <span class="token property">"reduce_bucket_size"</span><span class="token operator">:</span> <span class="token number">5e8</span><span class="token punctuation">,</span>        <span class="token property">"allgather_bucket_size"</span><span class="token operator">:</span> <span class="token number">5e8</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>ZeRO-3 <pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"zero_optimization"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"stage"</span><span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span>        <span class="token property">"contiguous_gradients"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>        <span class="token property">"stage3_max_live_parameters"</span><span class="token operator">:</span> <span class="token number">1e9</span><span class="token punctuation">,</span>        <span class="token property">"stage3_max_reuse_distance"</span><span class="token operator">:</span> <span class="token number">1e9</span><span class="token punctuation">,</span>        <span class="token property">"stage3_prefetch_bucket_size"</span><span class="token operator">:</span> <span class="token number">1e7</span><span class="token punctuation">,</span>        <span class="token property">"stage3_param_persistence_threshold"</span><span class="token operator">:</span> <span class="token number">1e5</span><span class="token punctuation">,</span>        <span class="token property">"reduce_bucket_size"</span><span class="token operator">:</span> <span class="token number">1e7</span><span class="token punctuation">,</span>        <span class="token property">"sub_group_size"</span><span class="token operator">:</span> <span class="token number">1e9</span><span class="token punctuation">,</span>        <span class="token property">"offload_optimizer"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"device"</span><span class="token operator">:</span> <span class="token string">"cpu"</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token property">"offload_param"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"device"</span><span class="token operator">:</span> <span class="token string">"cpu"</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="其它">3. 其它</h1><ul><li><a href="https://www.deepspeed.ai/tutorials/mixture-of-experts/">MoE专家并行</a></li></ul><h1 id="相关资源">4. 相关资源</h1><ul><li><a href="https://github.com/microsoft/DeepSpeed" class="uri">https://github.com/microsoft/DeepSpeed</a></li><li><a href="https://www.deepspeed.ai/" class="uri">https://www.deepspeed.ai/</a></li><li><a href="https://deepspeed.readthedocs.io/en/latest/index.html" class="uri">https://deepspeed.readthedocs.io/en/latest/index.html</a></li><li><a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/deepspeed" class="uri">https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/deepspeed</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型训练 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HuBERT</title>
      <link href="/blog/yin-pin/yin-pin-biao-shi/lun-wen/hubert/"/>
      <url>/blog/yin-pin/yin-pin-biao-shi/lun-wen/hubert/</url>
      
        <content type="html"><![CDATA[<p>HuBERT: Hidden-Unit BERT，自监督语音表示学习，预测 masked帧的声音单元，学习声学模型、语言模型的联合信息。</p><p>创新点：</p><ul><li>输入原始波形。</li><li>离线 k-means 聚类生成预训练帧级对齐 label、聚类集成（ClusterEnsembles）、聚类的迭代优化。</li></ul><h1 id="模型结构">1. 模型结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBadb1556608d2e51b58df51ea434e174d?method=download&shareKey=86093c761ff02e4eafde0417da5672e7" width="389"></p><ul><li><p>输入原始波形。</p></li><li><p>wav2vec 2.0 结构</p><p>16kHz 音频，CNN encoder 320倍降采样，即 20ms 帧率。</p></li><li><p>mask</p><p>随机选取 8% 的 CNN encoder 输出帧作为起点，长度为 10，替换为<font color="green">mask embedding</font>。</p></li><li><p>参数量</p><p>BASE 90M, LARGE 300M, X-LARGE 1B。<img src="https://note.youdao.com/yws/api/personal/file/WEB62034141a58deb24a0b36124dc89488b?method=download&shareKey=eddba0f9941fc84750712684dba6e17b" width="385"></p></li></ul><h1 id="预训练">2. 预训练</h1><ul><li><p>label</p><p>在 39 维 MFCC 上采用 k-means 聚为100类，作为第1次迭代的label。由于隐含层特征维度更高，受内存限制，后续随机采样10%的训练数据，将隐含层表示聚为500类。</p></li><li><p>损失函数</p><p><span class="math display">\[p_f^{(k)}(c \mid \tilde{X},t)=\frac{\exp \left(\operatorname{sim}\left(A^{(k)} o_t, e_c\right) /\tau\right)}{\sum_{c^{\prime}=1}^C \exp\left(\operatorname{sim}\left(A^{(k)} o_t, e_{c^{\prime}}\right) /\tau\right)}\]</span></p><p>其中，<span class="math inline">\(A^{(k)}\)</span> 为 第 <span class="math inline">\(k\)</span> 个聚类模型（聚类集成）的 projection矩阵，<span class="math inline">\(e_c\)</span> 为 codeword c的embedding，<span class="math inline">\(sim(,)\)</span>计算两个向量的余弦相似度，<span class="math inline">\(\tau=0.1\)</span>（使 softmax函数结果更尖锐）。</p></li><li><p>k-means</p><p><font color="green">采用 scikit-learn 实现的 MiniBatchKMeans算法，batch size 10k 帧。采用 k-means++ [57] with 20 random starts进行初始化。</font></p></li><li><p>聚类集成</p><p>利用多个 codebook size不同的聚类模型，产生不同粒度的训练目标，类似多任务学习。</p></li><li><p>积量化（product quantization，PQ）[39]</p><p>将特征空间分为多个子空间，每个子空间分别量化，则目标空间的理论大小为所有codebook size 的乘积。对于高维特征、子空间尺度差异很大的异构特征，PQ可以有效地实现基于欧氏距离的量化，如 k-means。</p><ul><li>采用 117 维 MFCC 特征，含上下文（3帧），0阶、1阶差分、2阶差分分别被量化为 100 entries。</li></ul></li><li><p>HuBERT BASE</p><p>Librispeech，32GPU，每张卡 batch size 最多 87.5s，2次迭代，第1次迭代250k 步，第2次迭代 400k 步，每 100k 步约9.5h。第2次迭代采用第1次迭代模型第 6 个 transformer层的输出进行聚类生成的 label 。</p></li><li><p>HuBERT LARGE、X-LARGE</p><p>Libri-light，1次迭代，400k 步，128、256 GPU，每张卡 batch size56.25s、22.5s。用 BASE 第2次迭代模型的 第9个 transformer层的输出进行聚类。</p></li><li><p>优化器</p><p>Adam <span class="math inline">\(\beta=(0.9,0.98)\)</span> , 前 8% 步warm up，BASE/LARGE/X-LARGE 的峰值学习率分别为5e-4/1.5e-3/3e-3，学习率线性变化。</p></li><li><p>SOTA 模型对比</p><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th></th><th>DiscreteBERT</th><th>wav2vec 2.0</th><th>HuBERT</th></tr></thead><tbody><tr><td>输入</td><td>量化单元，有损</td><td>原始波形</td><td>原始波形</td></tr><tr><td>隐层表示</td><td></td><td>量化 waveform encoder 输出</td><td>量化 transformer 层输出。消融实验表明性能更好，可能由于 CNN encoder模型容量有限</td></tr><tr><td>声学单元</td><td>vq-wav2vec，单一、固定 label</td><td></td><td>k-means 聚类，聚类集成，迭代优化</td></tr><tr><td>损失函数</td><td>预测 masked 单元</td><td>对比损失，需要仔细设计负样本采样；<br> auxiliary diversityloss；<br> 需要适当的Gumbel-softmax temperature annealing schedule</td><td>预测 masked 单元</td></tr></tbody></table></li></ul><h1 id="fine-tuning">3. fine-tuning</h1><ul><li><p>除了 CNN encoder，fine-tuning 其它参数，projection层替换为随机初始化的 softmax 层。</p><ul><li>freeze-step：多少步 transformer 参数固定，仅训练新的 softmax矩阵。</li></ul></li><li><p>ASR CTC损失，预测26个英文字母、空格、撇号、blank。</p></li><li><p>采用 <font color="green">Fairseq wav2letter++ [59] beam search解码器，结合LM </font></p><p><span class="math display">\[\log p_{C T C}(Y \mid X)+w_1 \log P_{LM}(Y)+w_2 |Y|\]</span></p><p>其中|Y| 为预测文本长度。</p></li></ul><h1 id="实验">4. 实验</h1><ul><li><p>数据集</p><ul><li>预训练：Librispeech (960h)、Libri-light (60kh)</li><li>fine-tuning：Libri-light 10min, 1h, 10h, Librispeech train-clean-100(100h), train-* (960h)<ul><li>上述 Libri-light 的3个子集，均一半来自 Librispeechtrain-clean-*，剩下的来自 train-other-500。</li></ul></li></ul></li><li><p>性能<img src="https://note.youdao.com/yws/api/personal/file/WEB77917921e6a0110c2b3e847fa89f3340?method=download&shareKey=ed8f6daceb889a07970c7b68f5cc10ae" width="794"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB71db9dc1a4c1dfe21877e48e90f50e43?method=download&shareKey=f5c82c973ca79a1b337bac7233b125e2" width="762"></p><ul><li>对比了半监督算法（迭代伪标签IPL [12]、slimIPL [54]、noisystudent[61]）、自监督算法（DeCoAR 2.0 [50], DiscreteBERT [51], wav2vec2.0 [6]）</li><li>与 SOTA 的 wav2vec 2.0 性能一致或有提升。</li></ul></li><li><p>聚类 label <span class="math inline">\(z_t\)</span> 的评价指标采用 ASR 模型强制对齐得到帧级音素label <span class="math inline">\(y_t\)</span>，</p><p>联合分布 <span class="math display">\[p_{y z}(i,j)=\frac{\sum_{t=1}^T\left[y_t=i \wedge z_t=j\right]}{T}\]</span></p><p>边缘概率 <span class="math display">\[p_z(j)=\sum_i p_{y z}(i,j)\]</span></p><p><span class="math display">\[p_y(j)=\sum_j p_{y z}(i, j)\]</span></p><p>每个音素 <span class="math inline">\(i\)</span> 对应的最可能的聚类label <span class="math inline">\(j\)</span> <span class="math display">\[z^*(i)=\arg \max _j p_{y z}(i, j)\]</span></p><p>同理， <span class="math display">\[y^*(j)=\arg \max _i p_{y z}(i,j)\]</span></p><p>条件概率 <span class="math display">\[p_{y \mid z}(i \mid j)=p_{yz}(i, j) / p_z(j)\]</span></p><ul><li><p>音素纯度 <span class="math display">\[\mathbb{E}_{p_z(j)}\left[p_{y \mid z}\left(y^*(j)\mid j\right)\right]\]</span></p><p>表示如果将 k-means label 替换为它对应的最可能的音素，帧级音素准确率。若每个 k-means label 唯一对应1个音素，则音素纯度为1。</p></li><li><p>聚类纯度</p><p><span class="math display">\[\mathbb{E}_{p_y(i)}\left[p_{z \midy}\left(z^*(i) \mid i\right)\right]\]</span></p><p>聚类纯度越高，表明同一音素的帧更可能对应同一聚类label。当聚类类别数增加时，该值会下降。</p></li><li><p>phone-normalized 互信息（PNMI）</p><p><span class="math display">\[\begin{aligned}\frac{I(y ; z)}{H(y)} &amp; =\frac{\sum_i \sum_j p_{y z}(i, j) \log\frac{p_{y z}(i, j)}{p_y(i) p_z(j)}}{\sum_i p_y(i) \log p_y(i)} \\&amp; =1-\frac{H(y \mid z)}{H(y)}\end{aligned}\]</span></p><p>已知 k-means label <span class="math inline">\(z\)</span>，对音素label <span class="math inline">\(y\)</span> 的熵的减少量。</p></li></ul></li><li><p>消融实验</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>结果</th><th>结论</th></tr></thead><tbody><tr><td>k-means 稳定性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf4420dc406c6d88d2ada6590a849725b?method=download&shareKey=400d7143fa4a13c031033d7c9fd91198"></td><td>同一参数、多次实验标准差很小；<br>聚类模型训练集数据量提升PNMI会改善，但增益有限，因此采样部分训练集进行k-means 聚类是可行的；<br> 相较于 MFCC，聚类迭代优化 PNMI 显著提升</td></tr><tr><td>聚类所用特征的层数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB7325b2e4a53333be5d279ce50f0516b6?method=download&shareKey=e20622a825887ddf006978c23ed87980"><br> 第0层表示 transformer 层的输入 <br> 基线：MFCC (cluster purity,phone purity, PNMI) = (0.099, 0.335, 0.255) C = 100，(0.031, 0.356,0.287) C=500</td><td>BASE-it1 最后几个 transformer 层用于聚类的效果急剧下降，可能和第1次迭代预训练标签质量较差有关。</td></tr><tr><td>损失函数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB6f928f272cac99502c7161d5785de58f?method=download&shareKey=f4e6e8f390d7cfb1d4c2479dbb1e8d22"><br> chenone[64]：基于字符，强制对齐生成标签</td><td>若仅考虑未被 mask 的位置，该模型类似聚类模型/声学模型。<br>仅考虑masked 位置，驱使模型学习声学模型、语言模型的联合信息，效果最好。</td></tr><tr><td>聚类集成</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB20cc38279c55aac79d177009e320e25e?method=download&shareKey=d8d37bc6a4e4b85aab28715b10cf46b3"><br>积量化：采用 117 维 MFCC 特征，含上下文（3帧），0阶、1阶差分、2阶差分分别被量化为 100 entries</td><td>对比表5、表6，聚类集成比单个聚类模型性能好</td></tr><tr><td>超参数</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB5fd73ef4bfe7b4a97b3a369b7e12b2d9?method=download&shareKey=9568d7ce1eb06efb3a6b492b4e39976b"><img src="https://note.youdao.com/yws/api/personal/file/WEB02250c74874949959a7c9f96d27dd4b8?method=download&shareKey=692177e2e4ee1cee4326877ce335eb48"></td><td>选取 8% 的帧作为 mask 起点性能较好；<br> 增大 batch size可以显著提升性能；<br> 训练更长时间模型性能更好</td></tr></tbody></table></li></ul><h1 id="论文">5. 论文</h1><ul><li>Hsu W N, Bolte B, Tsai Y H H, et al. Hubert: Self-supervised speechrepresentation learning by masked prediction of hidden units[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021,29: 3451-3460.</li><li><a href="https://github.com/pytorch/fairseq/tree/master/examples/hubert">代码、预训练模型、fine-tuned模型</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 音频表示 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT</title>
      <link href="/blog/nlp/mo-xing/bert/"/>
      <url>/blog/nlp/mo-xing/bert/</url>
      
        <content type="html"><![CDATA[<p>BERT: Bidirectional Encoder Representations fromTransformers。无监督预训练的语言表示模型。</p><p><font color="red">创新点：</font></p><ul><li><p>双向性：采用 Transformer 的 Encoder 结构、masked LM预训练策略，能够同时考虑左右上下文的信息，更好地建模上下文信息和语义关系，学习深度双向表示，而传统的表示学习只学习单向LM，或者独立训练 left-to-right（LTR）、right-to-leftLM，再简单地拼接两个方向的表示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6d2dd6042429f8d7362ba9a48585da8a?method=download&shareKey=03683aa46038f4e89675a79b985b5e00" width="703"></p></li><li><p>预训练策略：通过 maskedLM、预测是否为下一个句子（NSP）两个预训练目标，来学习 token级的上下文语义表示 和 句子间的关系。对于涉及文本对的下游任务，拼接文本对+ self-attention 有效地包含了文本对的 bidirectional crossattention。可以简单地加输出层fine-tuning，得到广泛的NLP任务的SOTA模型，而不需要特定于任务的架构修改。</p></li></ul><h1 id="模型结构">1. 模型结构</h1><h2 id="输入表示">1.1. 输入表示</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2076aca79516cbb11b75e1f084cce145?method=download&shareKey=f107d1cd5cb7e44a8f03992d8985d099" width="699"></p><p>其中，token embedding 采用 WordPiece embedding，共 3万 tokens。</p><h2 id="模型结构-1">1.2. 模型结构</h2><p>Transformer 结构</p><p><span class="math inline">\(\mathrm{BERT}_{\mathrm{BASE}}\)</span>:L=12, H=768, A=12, 总参数量 110M，与 GPT 模型大小相同。</p><p><span class="math inline">\(\mathrm{BERT}_{\mathrm{LARGE}}\)</span>:L=24, H=1024, A=16, 总参数量 340M。</p><p>其中，L 为 Transformer block 数，H 为 hidden size，A 为self-attention 的头数，feed-forward size 为 4H。</p><h2 id="无监督预训练目标">1.3. 无监督预训练目标</h2><ul><li>masked LM (MLM，完形填空)<ul><li>每个序列随机 mask 15% 的 token。80% 替换为 [MASK] token，10%替换为随机 token，10% 保留原 token。被 mask 的 token对应的输出表示接1层分类层，预测被 mask 的 token 的词表ID。</li><li>分词后，对于partial word pieces，mask 时不特别考虑。</li></ul></li><li>预测是否为下一个句子 (NSP)<ul><li>50% 为实际的下一个句子，50% 从语料库中随机选取。[CLS] token对应的最终输出表示，接1层分类层。</li><li>本文的“句子”可以是连续文本的任意跨度，而不是语言学意义上的1句，从而可以学习长序列。</li><li>对问答、自然语言推理等涉及句子间关系的任务都非常有益。</li><li>最终模型 NSP 准确率为 97%-98%。</li><li><font color="teal">若不 fine-tuning，[CLS] token对应的最终输出表示不是有意义的句子表示。</font></li></ul></li></ul><h2 id="用于特定任务">1.4. 用于特定任务</h2><p>将预训练表示用于下游任务的两种策略：</p><ul><li>fine-tuning：在预训练模型上添加较少的随机初始化的参数，并在下游任务上对所有参数联合微调。如GPT、BERT。</li><li>基于特征：从预训练模型中提取固定的特征表示，采用特定于任务的模型架构。如ELMo。<ul><li>拼接 BERT 最后 4 层的表示。</li></ul></li></ul><p>在 1 个 Cloud TPU 上训练最多 1h，在 1 个 GPU上几个小时，就可以复现本文结果。</p><h1 id="实验">2. 实验</h1><h2 id="参数">2.1. 参数</h2><ul><li>绝对位置embedding，最大长度为512。</li><li>激活函数：gelu</li><li>优化器：各层 dropout 概率 0.1，L2 weight decay 0.01，学习率 1e-4，前1w 步 warmup，学习率线性衰减，Adam，<span class="math inline">\(\beta_1=0.9\)</span>，<span class="math inline">\(\beta_2=0.999\)</span>。</li><li>预训练语料库：约 3.3 billion 单词。</li><li>预训练：batch size 256，1 百万步，约为 40 epochs。</li><li>由于长序列 attention 计算量较大，为了加速预训练，90% 的 step采用序列长度 128，剩下的 10% 采用序列长度 512 来学习位置embedding。<font color="green">batch 的序列长度为 batch内序列的最大长度，是动态变化的？</font></li><li>在16个TPU芯片上训练<span class="math inline">\(\mathrm{BERT}_{\mathrm{BASE}}\)</span>，64个TPU芯片上训练<span class="math inline">\(\mathrm{BERT}_{\mathrm{LARGE}}\)</span>，预训练需要4天。</li></ul><h2 id="对比消融实验">2.2. 对比、消融实验</h2><h3 id="预训练">2.2.1. 预训练</h3><ul><li><p>训练目标<img src="https://note.youdao.com/yws/api/personal/file/WEBcf8bec39f002e85aa5fb5027951b5e5f?method=download&shareKey=920d74916d4d5ec4f76f379662396ba7" width="354"></p><p>在所有任务上，LTR LM 都不如 masked LM。</p><p>NSP训练目标对问答、自然语言推理等涉及句子间关系的任务都非常有益。</p></li><li><p>mask 策略<img src="https://note.youdao.com/yws/api/personal/file/WEB1fd4195f93f978a1079f18c1b5e74f08?method=download&shareKey=fb92e712f10869f86ed9672d46c077e0" width="343"></p><p>若仅使用 [MASK]、或仅使用随机替换，性能较差。</p></li><li><p>收敛速度<img src="https://note.youdao.com/yws/api/personal/file/WEB39173a9a2ea7b31c1008345bb24c5f0a?method=download&shareKey=a22a8addcd76420edd8b8b83b94b59b7" width="340"></p><p>由于 masked LM 只对每个 batch 中 15% 的 token 进行预测，相较于 LTRLM，收敛速度更慢，但性能更好。</p></li><li><p>训练步数 相较于 500k 步的预训练，1M 步提升了约 1%的准确度。</p></li><li><p>模型大小模型越大，预训练表示更有表现力，性能越好（前提是该模型已经被充分地预训练），特别是在训练集较小的下游任务上。但是，<span class="math inline">\(\mathrm{BERT}_{\mathrm{LARGE}}\)</span>在小数据集上 fine-tuning 时有时不稳定。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB72c21baeb6fdf2488dd9d5ae9193eaa9?method=download&shareKey=c61e41cf5801f4ab251b5518ebc2ccb0" width="340"></p></li></ul><h3 id="fine-tuning">2.2.2. fine-tuning</h3><ul><li><p>虽然最优超参数值是特定于任务的，但以下可能值范围对于所有任务都能work well。由于fine-tuning非常快，最优超参数可以在开发集上穷举搜索。</p><ul><li>batch size：16、32</li><li>学习率：5e-5, 3e-5, 2e-5</li><li>epoch 数：2, 3, 4</li></ul></li><li><p>11 个 NLP 任务（GLUE 数据集包含的 8个NLP任务（排除了1个有问题的数据集）、SQuAD v1.1、SQuAD2.0、SWAG）上均取得了 SOTA 结果，包括 token 级任务（如NER、QA）和句子级任务（如自然语言推理）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe2aef02cb0d986bf1a9c23c56e23d3e0?method=download&shareKey=2e29b4a63754a221ae493376b3ff1897" width="704"></p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB072fe8e332c1500ac0cc84064971fa90?method=download&shareKey=319c32231b3458bb01e6ce25118cbe1e"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf6dbf658b24ec8736f0f1195f0fe988c?method=download&shareKey=72b4de7d008bf77f03794e87fbbe0775"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB63401f0b0c341fcc1011ece08c537f7f?method=download&shareKey=8723c249d59edf055ee22bcd9a63f47c"></td></tr></tbody></table><ul><li>GLUE 数据集介绍详见论文附录B。</li><li>The Stanford Question Answering Dataset (SQuAD v1.1):包含10万个众包问题/答案对，给定1个问题和1个wikipidea中包含答案的段落，任务是预测答案在段落中的跨度。<ul><li><font color="green">fine-tuning 时引入开始向量 <span class="math inline">\(S \in \mathbb{R}^H\)</span> 和结束向量 <span class="math inline">\(E\)</span></font>，第 <span class="math inline">\(i\)</span> 个token为答案区间起点的概率为<span class="math inline">\(P_i=\frac{e^{S \cdot T_i}}{\sum_j e^{S \cdotT_j}}\)</span>。</li><li>训练目标：答案区间正确的开始位置、结束位置的 log-likelihood的和。</li><li>预测：候选答案区间的得分为 <span class="math inline">\(S \cdot T_i+E\cdot T_j\)</span>，选取 <span class="math inline">\(j \geq i\)</span>且得分最大的区间。</li><li>fine-tuning：先用 TriviaQA 数据集，再用 SQuAD v1.1 数据集。即使不用TriviaQA 数据集，仍然在很大程度上优于所有的现有系统。</li></ul></li><li>SQuAD 2.0：段落中可能不包含答案。<ul><li>用开始、结束位置均对应 [CLS] token 来建模。对比 没有答案的得分 <span class="math inline">\(s_{null} = S \cdot C + E \cdot C\)</span> 和非零跨度的最佳得分 <span class="math inline">\(\hat{s_{i, j}}=\max _{j\geq i} S \cdot T_i+E \cdot T_j\)</span>，若 <span class="math inline">\(\hat{s_{i, j}}&gt;s_{\mathrm{null}}+\tau\)</span>则预测为包含答案，其中 <span class="math inline">\(\tau\)</span>在开发集上选择以最大化 F1 值。</li></ul></li><li>SWAG (Situations With Adversarial Generations)：给定 1 个句子，从 4个选项中选择最合理的延续。<ul><li>将给定句子与候选分别拼接为 4 个输入序列，<font color="green">引入 1个向量</font>，其与 [CLS] 的最终输出表示的点积作为每个候选的得分，并用softmax 层归一化。</li></ul></li></ul></li></ul><h3 id="基于特征的方法">2.2.3. 基于特征的方法</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd1026d3f97d6217753114ba637726d93?method=download&shareKey=9ac5c0784b9a1b4ccb76822110f43bc9" width="344"></p><p>NER 任务：将第一个 sub-token 的表示作为输入。</p><p>拼接 BERT 最后 4 层的表示，性能最好。</p><h3 id="bert-elmo-gpt-表示学习模型对比">2.2.4. BERT, ELMo, GPT表示学习模型对比</h3><ul><li>BERT、GPT 微调。<ul><li>双向性和两个预训练目标是 BERT 性能更优的主要原因（见论文Tabel5的消融实验）。</li><li>其它区别<ul><li>训练语料：GPT BooksCorpus(800M words), BERT BooksCorpus(800M words)+ English Wikipedia(2500Mwords，只使用文本段落，忽略列表、表格、标题)。</li><li>GPT 只在微调时引入[SEP]、[CLS] token，BERT 在预训练时学习[SEP]、[CLS] 和 segment embedding。</li></ul></li></ul></li><li>ELMo 基于特征。</li></ul><h1 id="论文">3. 论文</h1><ul><li>Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deepbidirectional Transformers for language understanding[J]. arXiv preprintarXiv:1810.04805, 2018.</li><li><a href="https://github.com/google-research/bert">代码、预训练模型</a></li></ul><h1 id="个人思考">4. 个人思考</h1><ul><li>表示学习的理解<ul><li>神经网络LM：神经网络倒数第二层的表示，经过最后一个分类层，输出词表中每个单词的概率分布。由于上文/下文/上下文不同时，当前词的概率分布不同，因此，模型倒数第二层的表示蕴含了输入文本的信息。包括：句子中各个词之间的语义关系，句子中其它词与当前词的语义相关性，不同词的语义重要性等等。体现了模型的语义理解和表达能力。</li><li>预训练模型：例如，在 LTR LM 预训练中，每个 token对应的输出表示都蕴含了 包含当前 token的上文信息，可以用于下游任务。</li><li>masked LM 预训练<ul><li>[MASK] token 对应的输出表示蕴含了上下文信息。由于[MASK]在句子中的位置随机，所以句子中各个位置的输出表示都蕴含了上下文信息。<font color="green">但是，由于 [MASK] token 无语义信息，仅使用 [MASK] token替换当前词时，当前位置的输出表示 与当前输入的关联性较差，因此，当下游任务采用基于特征的方法时，相较于采用BERT 提出的 mask 策略，性能较差。</font></li><li>保留当前词：当上下文不足以推断当前位置信息时，该策略会驱使模型也考虑当前位置的输入。而同时采用其它两种替换策略，避免了模型原样输出。同时，缓解预训练与下游任务的不匹配，因为实际预测时不会出现[MASK]。</li><li>随机替换：该策略可以促使模型学习上下文和当前词的关系。实际应用中，输入可能包含错误单词，可以提高模型的鲁棒性。另外，由于模型无法区分句子中的原始单词和随机替换的单词，也不知道需要预测的单词的位置，所以驱使模型学习每个位置的token表示。<font color="green">（避免模型在正常词的位置原样输出，[MASK]对应的位置输出上下文表示）</font></li></ul></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 模型 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SpeechGPT</title>
      <link href="/blog/llm/yu-yin-dui-hua/lun-wen/speechgpt/"/>
      <url>/blog/llm/yu-yin-dui-hua/lun-wen/speechgpt/</url>
      
        <content type="html"><![CDATA[<ul><li>Zhang D, Li S, Zhang X, et al. Speechgpt: Empowering large languagemodels with intrinsic cross-modal conversational abilities[J]. arXivpreprint arXiv:2305.11000, 2023.</li><li>作者来自复旦</li><li><a href="https://github.com/0nutation/SpeechGPT" class="uri">https://github.com/0nutation/SpeechGPT</a></li><li><a href="https://0nutation.github.io/SpeechGPT.github.io/" class="uri">https://0nutation.github.io/SpeechGPT.github.io/</a></li><li><font color="red">创新点：提出了第一个能够感知和生成多模态内容（文本、语音）的多模态LLM，具有指令遵循和语音对话能力。采用 HuBERT + LLaMA-13B +HiFi-GAN。采用3阶段训练：音频预训练、跨模态指令微调（ASR、TTS、纯文本指令跟随）、chain-of-modality指令微调（语音指令-文本指令-文本回复-语音回复，LoRA）。</font></li></ul><h1 id="系统结构">1. 系统结构</h1><ul><li>采用离散语音表示 <a href="https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt">HuBERT</a>，添加到LLM 的词表中。<ul><li>移除相邻帧的重复单元。<font color="green">存疑：合成语音的自然度？</font></li><li>语音生成：训练了多说话人 HiFi-GAN。包含 up-sample 模块，说话人embedding 与上采样之后的每一帧语音表示拼接。</li></ul></li><li>LLaMA-13B，最大序列长度 2048。</li></ul><h1 id="指令数据集">2. 指令数据集</h1><ul><li>SpeechInstruct：语音-文本 跨模态指令数据集，包含<ul><li><p>跨模态指令：采用 GPT-4 生成 ASR、TTS 任务指令各 100条来模拟实际用户指令。采用 ASR 数据集（Gigaspeech、CommonVoice、LibriSpeech）构建 ASR、TTS 任务样本。模板：</p><p>其中 <span class="math inline">\(\{D\}\)</span>为任务描述。拼接成多轮对话。</p></li><li><p>chain-of-modality 指令：采用 Transformer encoder-decoder 架构在LibriSpeech 上训练 text-to-unit 生成器。从文本指令数据集 <a href="https://huggingface.co/datasets/fnlp/moss-002-sft-data">moss-002-sft-data</a>中选取回复长度少于 35个词的样本，构建（语音指令，文本指令，文本回复，语音回复）四元组，支持四类任务</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7f4076a2451b94e26a51b701ddc12a74?method=download&shareKey=0495ae922e9fc860a85abb73524ab3f2" width="510"></p></li></ul></li></ul><h1 id="阶段训练">3. 3阶段训练</h1><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th></th><th>数据集</th></tr></thead><tbody><tr><td>音频预训练（next token 预测，用于理解、生成音频）</td><td>LibriLight 6wh 无监督英语有声书</td></tr><tr><td>跨模态指令微调</td><td>SpeechInstruct 跨模态指令数据集（ASR、TTS任务，用于模态对齐）、moss-002-sft（用于指令跟随）</td></tr><tr><td>chain-of-modality 指令微调（LoRA）</td><td>SpeechInstruct chain-of-modality指令数据集（用于跨模态指令跟随）</td></tr></tbody></table><h1 id="其它">4. 其它</h1><ul><li>评价：仅提供 case，指令跟随、语音对话均无客观/主观评价。</li><li>局限性<ul><li>无法生成不同情感的语音。</li><li>需要先生成文本回复再生成语音回复。</li><li>由于上下文长度限制，无法支持多轮对话。</li></ul></li><li>ASR + LLM + TTS 级联结构的缺点<ul><li>丢失了语音信号中的情感、韵律等信息。</li><li>LLM 的知识无法转移到 ASR 模块。</li><li>TTS 无法利用 LLM 中的语义信息。</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
          <category> 语音对话 </category>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ECAPA-TDNN</title>
      <link href="/blog/yin-pin/shuo-hua-ren-shi-bie/ecapa-tdnn/"/>
      <url>/blog/yin-pin/shuo-hua-ren-shi-bie/ecapa-tdnn/</url>
      
        <content type="html"><![CDATA[<h1 id="结构特点">1. 结构特点</h1><p>集成了计算机视觉领域最新的增强模块：Res2Net、SEblock，聚合多层的特征，并采用 channel-dependent帧注意力改进了统计池化模块。</p><h1 id="系统结构">2. 系统结构</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe03b7ab1ccd2402b22ce13eb3be6ce5a?method=download&shareKey=c864e0ebfeafd8d51f27e9d819df55e5" width="443"><img src="https://note.youdao.com/yws/api/personal/file/WEBb244ce5a3bcad0e1a25fa95a504b6f89?method=download&shareKey=3f37b0c013ccdeb9c16a91877158c65c" width="440"></p><h1 id="特征提取">3. 特征提取</h1><ul><li>数据增强：为每句话生成6个额外的样本<ul><li>kaldi 加噪（MUSAN babble、noise）、加混响（RIR数据集[21]）</li><li>SoX 变速（tempo up, tempo down）、FFmpeg 压缩（交替 opus 或 aac压缩）</li><li>speechbrain 实现<ul><li>concat_augment：可选择多种数据增强方式 串行（样本量不变） 或1个batch内并行（batch_size *= X）</li><li>时域增强<ul><li>变速：采用基于sinc的插值进行重采样，比如重采样到原始音频长度的[90%,100%,110%]。<font color="green">注意：由于特征提取时默认所有信号为原始采样率，所以重采样到90%采样率的实际效果，为语速*1.11，音高*1.11</font></li><li>采用滤波器随机mask部分频带。<font color="red">作用：帮助模型学习到依赖所有频带，而不是其中的少部分</font></li><li>随机mask部分音频片段，可选是否填充白噪声<code>4A * torch.rand() - 2A</code>，其中<code>A</code>为原始音频的平均幅值，默认全部置0。作用：帮助模型学习到依赖整个信号</li><li>上述3种增强方式 可选是否添加，串行</li></ul></li><li>环境扰动<ul><li>加混响：采用room 冲击响应<ul><li><a href="http://www.openslr.org/resources/28/rirs_noises.zip" class="uri">http://www.openslr.org/resources/28/rirs_noises.zip</a></li></ul></li><li>加babble</li><li>加噪</li><li>上述3种增强方式 可选是否添加，串行</li></ul></li><li>由此导致的音频长度改变 需要通过末尾截断 或 补0来与原音频保持一致</li><li>实现详见<code>speechbrain.processing.speech_augmentation</code></li></ul></li></ul></li><li>输入特征：80维MFCCs，<font color="green">随机裁剪为2s的片段</font>，倒谱均值减法，不加VAD。<ul><li>speechbrain 实现：60维 Fbank</li></ul></li><li>SpecAugment</li></ul><h1 id="res2net">4. Res2Net</h1><p><a href="https://nanyang2015.github.io/blog/ji-qi-xue-xi/cnn/res2net/">Res2Net</a>：表示多尺度的特征，增大感受野。</p><p>论文与 speechbrain 实现的差异：论文中 Res2 Dilated Conv1D 前后的denselayer，前一个 dense layer 降低通道维度，后一个 dense layer恢复通道维度，从而控制参数量。</p><h1 id="squeeze-excitation-block">5. Squeeze-Excitation Block</h1><p><a href="https://nanyang2015.github.io/blog/ji-qi-xue-xi/cnn/senet/">SEBlock</a>：显式地建模通道间的相关性，根据输入的全局特征，自适应地对各个通道的feature map 重新加权。</p><h1 id="多层特征聚合">6. 多层特征聚合</h1><p>出发点：浅层的特征也有助于更鲁棒的embedding。</p><ul><li>拼接所有SE-Res2Blocks的输出feature map，经过1层dense层。</li><li>SE-Res2Block的残差连接实现为前面所有SE-Res2Blocks的输出的和。求和，而不是拼接，用于约束模型参数量。（speechbrain未实现）</li></ul><h1 id="channel-dependent-attentive-statistics-pooling">7. channel-dependentattentive statistics pooling</h1><ul><li><font color="red">出发点：将时间注意力机制进一步扩展到通道维度，使得网络可以关注不会同时激活的说话人特征，如元音和辅音属性（不同channel 的时间注意力系数不一样）。</font></li><li>实现<ul><li><p><font color="red">将 <span class="math inline">\(\boldsymbol{h}_t\)</span> 与其整句的非加权的均值、标准差拼接（<span class="math inline">\(C*=3\)</span>），使得attention能自适应句子的全局属性，如噪声、录音环境。</font></p></li><li><p>channel attention：channel 维的 bottleneck结构的两层全连接，所有时刻参数共享。</p><ul><li>speechbrain 实现 <pre class="line-numbers language-none"><code class="language-none">self.tdnn &#x3D; TDNNBlock(channels * 3, attention_channels, 1, 1)  # 降维self.tanh &#x3D; nn.Tanh()self.conv &#x3D; Conv1d(in_channels&#x3D;attention_channels, out_channels&#x3D;channels, kernel_size&#x3D;1)  # 恢复原始形状attn &#x3D; self.conv(self.tanh(self.tdnn(attn)))attn &#x3D; F.softmax(attn, dim&#x3D;2)  # 沿时间轴做softmax<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><font color="red">attentive statistics pooling：attention到不同帧，类似VAD以检测无关的非语音帧。</font></p><p>加权均值： <span class="math display">\[\tilde{\mu}_c=\sum_t^T\alpha_{t, c} h_{t, c}\]</span></p><p>加权标准差： <span class="math display">\[\tilde{\sigma}_c=\sqrt{\sum_t^T \alpha_{t, c}(h_{t, c}-\tilde{\mu}_c)^2}=\sqrt{\sum_t^T \alpha_{t, c} h_{t,c}^2-\tilde{\mu}_c^2}\]</span></p><p>pooling层的输出为 <span class="math inline">\(\tilde{\mu}\)</span>、<span class="math inline">\(\tilde{\sigma}\)</span> 拼接</p></li></ul></li></ul><h1 id="bottleneck-结构的两层全连接">8. bottleneck 结构的两层全连接</h1><p>bottleneck 层的输出可以作为低维的说话人embedding。</p><h1 id="训练">9. 训练</h1><ul><li>采用参考文献[23]中的 triangular2 policy，循环学习率，[1e-8,1e-3]，一次循环为 130k 迭代。</li><li><font color="green">AAM-softmax，margin 0.2，softmax prescaling of30 for 4 cycles。</font></li></ul><h1 id="实验结果">10. 实验结果</h1><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe9098186e1d400650ef1821ea33629ef?method=download&shareKey=f434762b2145eb1db885eb270b0c89d4" width="727"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB08c933cf955a2b189352633460c292fb?method=download&shareKey=0a0a54795b59fa9de701da3df3d15f9e" width="350"></p><h1 id="论文">11. 论文</h1><ul><li>Desplanques B, Thienpondt J, Demuynck K. ECAPA-TDNN: Emphasizedchannel attention, propagation and aggregation in TDNN based speakerverification[J]. arXiv preprint arXiv:2005.07143, 2020.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 说话人识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SENet</title>
      <link href="/blog/ji-qi-xue-xi/cnn/senet/"/>
      <url>/blog/ji-qi-xue-xi/cnn/senet/</url>
      
        <content type="html"><![CDATA[<ul><li><p><font color="red">出发点：显式地建模通道间的相关性，根据输入的全局特征，自适应地对各个通道的featuremap 重新加权。</font></p></li><li><p>模块结构</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4ed16068c8341262f00526b614216eb7?method=download&shareKey=2d73fbb8a7eb2bde12495222c2cfce0a" width="589"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB56580c1f90254168b06438d27a116ece?method=download&shareKey=685a5692827f2751871a4000ab160045" width="462"></p><ul><li><p>Squeeze</p><p>global average pooling：分别计算每个通道的激活均值。</p><p><span class="math display">\[ z_c=\mathbf{F}_{sq}\left(\mathbf{u}_c\right)=\frac{1}{H \times W} \sum_{i=1}^H\sum_{j=1}^W u_c(i, j) \]</span></p></li><li><p>Excitation</p><p>自适应权重：</p><p><span class="math display">\[\mathbf{s}=\mathbf{F}_{e x}(\mathbf{z},\mathbf{W})=\sigma\left(\mathbf{W}_2 \delta\left(\mathbf{W}_1\mathbf{z}\right)\right)\]</span></p><p>其中，<span class="math inline">\(\sigma\)</span> 为 sigmoid函数，<span class="math inline">\(\delta\)</span> 为ReLU，通过 <span class="math inline">\(\mathbf{W}_1 \in \mathbb{R}^{\frac{C}{r} \timesC}\)</span>、<span class="math inline">\(\mathbf{W}_2 \in \mathbb{R}^{C\times \frac{C}{r}}\)</span>bottleneck结构的两层全连接限定模型复杂度。</p><p>加权：channel-wise 乘法</p><p><span class="math display">\[\widetilde{\mathbf{x}}_c=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_c, s_c\right)=s_c \cdot \mathbf{u}_c\]</span></p><ul><li>设计的考虑点<ul><li>能学习到通道间非线性的相关性；</li><li>允许多个通道同时被激励，而不是one hot的；</li><li>每个通道的权重根据输入自适应地计算。</li></ul></li></ul></li></ul></li><li><p>性能</p><ul><li><p><font color="red">很容易与其它结构整合。在许多任务上都取得了性能提升。</font></p></li><li><p>CPU推理时间</p><p>对于 <span class="math inline">\(224 \times 224\)</span>的图像，ResNet-50 为164ms，SE-ResNet-50 为 167ms。</p><p>* GPU库中 global pooling 未做优化</p></li><li><p>增加的参数量</p><p><span class="math display">\[\frac{2}{r} \sum_{s=1}^S N_s \cdotC_s^2\]</span></p><p>其中，<span class="math inline">\(S\)</span> 为 stage 数，<span class="math inline">\(N_s\)</span>为第s个stage的层数，这些层通道数相同，为<span class="math inline">\(C_s\)</span>。</p><p>通常，最后一个 stage 的 SE blocks参数量相对较多，在参数量是关键考虑因素的场景下，可以移除该部分。可以实现参数量仅相对增加4%，而相较于不移除，ImageNet 上的 top-1 error 仅下降 &lt;0.1%。</p></li></ul></li><li><p>实验</p><ul><li><p>模型的性能并不是随着 <span class="math inline">\(r\)</span>的增加而单调提升，可能与 <span class="math inline">\(r\)</span>过大时过拟合有关。设置 <span class="math inline">\(r=16\)</span>在模型准确度和复杂度之间取得了较好的权衡。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0b7c919f145654b8478606ddf3765ee0?method=download&shareKey=eb52f523ac477cb3a72e15b6f9dd5547" width="368"></p></li><li><p>可视化</p><p>从 ImageNet中选取了语义、外观差异较大的4个类，每个类选取50个样本，计算每个阶段最后一个SEblock 的平均激活，并将所有类的平均激活作为参考。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8d17a71ee60fac532851e3f7d0e2cf10?method=download&shareKey=fab4d939765032da527da4d78ab6b369" width="764"></p><ul><li>SE_2_3、SE_3_4：较低的层中，各channel的重要性分布，对于不同的类几乎相同。即较低层的特征通常更一般，在分类任务中具有类别不可知性。</li><li>SE_4_6、SE_5_1：更深的层中，各channel的重要性分布，对于不同的类差异较大。而通过SE block 自适应地重新加权，有助于特征提取和专门化。</li><li>SE_5_2、SE_5_3：在网络末端，各channel的重要性趋于相同。因此，在最后阶段删除SEblock，可以显著地减少总体参数量，但性能损失很小。</li></ul></li></ul></li><li><p>论文</p><p>J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In IEEEConf. Comput. Vis. Pattern Recog., 2018.</p></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> CNN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Res2Net</title>
      <link href="/blog/ji-qi-xue-xi/cnn/res2net/"/>
      <url>/blog/ji-qi-xue-xi/cnn/res2net/</url>
      
        <content type="html"><![CDATA[<ul><li><p>出发点：表示多尺度的特征、增大感受野。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB61ffb780869eb695d691e4d54876ac5b?method=download&shareKey=3f447136c62dbd906f1b59cde85798da" width="385"></p><p><span class="math display">\[\mathbf{y}_i= \begin{cases}\mathbf{x}_i &amp; i = 1 \\\mathbf{K}_i\left(\mathbf{x}_i\right) &amp; i = 2 \\\mathbf{K}_i\left(\mathbf{x}_i+\mathbf{y}_{i-1}\right) &amp; 2&lt;i\leqslant s\end{cases}\]</span></p></li><li><p>名称由来：残差块里又有残差连接。</p></li><li><p>多尺度的理解：<span class="math inline">\(\mathbf{x}_2\)</span>经过1个 <span class="math inline">\(3 \times 3\)</span> 的卷积得到 <span class="math inline">\(\mathbf{y}_2\)</span>，而 <span class="math inline">\(\mathbf{y}_2\)</span> 会作为 <span class="math inline">\(\mathbf{K}_3\)</span> 的输入，再次经过1个 <span class="math inline">\(3 \times 3\)</span> 的卷积。2次 <span class="math inline">\(3 \times 3\)</span> 的卷积相当于1个 <span class="math inline">\(5 \times 5\)</span> 的卷积，因此，<span class="math inline">\(\mathbf{K}_3\)</span> 融合了 <span class="math inline">\(3 \times 3\)</span> 的感受野的特征 和 <span class="math inline">\(5 \times 5\)</span>的感受野的特征。以此类推。得到的 <span class="math inline">\(\mathbf{y}\)</span>可以表征多尺度的特征。</p></li><li><p>性能</p><ul><li><p>很容易与其它结构整合。</p></li><li><p>由于具有较强的多尺度特征表征能力，在许多任务上都取得了性能提升，如图片分类、目标检测、语义分割、显著目标检测等，对于不同尺寸的对象性能都有提升。<img src="https://note.youdao.com/yws/api/personal/file/WEB8f041f566f23b3eaa720ab325b3570c6?method=download&shareKey=8b34ae8793fd313603365c485b8a7c82" width="320"><img src="https://note.youdao.com/yws/api/personal/file/WEBad279f36483cb4953f11ee2987747fa1?method=download&shareKey=21ea4d5cec3f46fcf6856e76b1dc810a" width="377"></p></li><li><p><font color="green">虽然 <span class="math inline">\(\mathbf{y}_i\)</span>需要顺序计算，但Res2Net模块引入的额外运行时间通常可以忽略不计。</font></p></li><li><p>相较于模型深度 depth、1层的 channel 数width、cardinality[68]，增加 scale 可以获得更快的性能提升。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0b0ad93f6733adeef588208ca17e008c?method=download&shareKey=3b201a76aede1628dec4ee4d9199853a" width="380"></p><p>*scale为5、6时性能提升有限，可能由于CIFAR数据集中的图片太小（<span class="math inline">\(32 \times 32\)</span>）。</p><p>*当图片中的对象尺寸已经可以被Res2Net模块覆盖时，继续增加 scale，性能提升有限；若固定模型复杂度，增加 scale会导致每个感受野的通道数减少，可能会降低模型处理特定尺度特征的能力。</p></li></ul></li><li><p>论文 Gao S H, Cheng M M, Zhao K, et al. Res2net: A newmulti-scale backbone architecture[J]. IEEE transactions on patternanalysis and machine intelligence, 2019, 43(2): 652-662.</p><ul><li>第2章相关工作<ul><li>计算机视觉领域的Backbone Networks：AlexNet、VGGNet、Network inNetwork、GoogLeNet、Inception、ResNet、DenseNet、DLA等。</li><li>视觉任务的多尺度表示：spatial pyramid pooling、featurepyramid、atrous convolutional。</li></ul></li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> CNN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>不当停顿</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/te-zheng/bu-dang-ting-dun/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/te-zheng/bu-dang-ting-dun/</url>
      
        <content type="html"><![CDATA[<ul><li>感知停顿：令听者感到话语中断（、破坏语义/韵律单位）的语音现象</li><li>不当停顿与二语水平：不当停顿与学习者的单词辨认、意群划分、内容理解能力有关</li><li>停顿时长阈值<ul><li>朗读、复述：100ms</li><li>自发语音：200-300ms</li><li>句间：400ms （取论文中边界位置不当停顿的平均时长）</li></ul></li><li>常出现的位置<ul><li>短语内：如自我修正</li><li>较简单和意义紧密相联的句法单位之间<ul><li>短语间<ul><li>主语为人称代词的主谓结构之间，如：Take it back to the shop where you(259ms) bought it.</li><li>句末短状语之前，如：Do you want to go to the cinema (460ms)tonight?</li></ul></li><li>从句间<ul><li>常见于宾语从句和主句之间，如：I don't know (400ms) what I can dowith it.</li><li>并列从句之间：Do you want to go to the cinema tonight，(649ms) orhave you got to stay late at work again?</li></ul></li></ul></li><li><font color="red">注意：并不是所有标点符号的位置都应该停顿</font>，如：<ul><li>直接引语和句末报告短语之间，如："Will they come tomorrow?" (557ms)Betty asked.</li><li>句首附加成分（如well、yes、no、oh）之后，如：Well, (663ms) let'shave a look at the newspaper.</li></ul></li></ul></li><li><font color="red"> 无实际静默的主观停顿：主要包括3个方面 </font><ul><li>发音问题<ul><li>无协同发音，如：likes swimming，未连读，读了两个/s/</li><li>元音拖音</li><li>词尾增读/ə/</li></ul></li><li>错误的重音模式：应弱读的单词（特别是介词）被重读，容易感觉其后有停顿</li><li>不恰当的音高重设，使得调群被错误地分割。如：Tell me that story aboutthe clever monkey, (70ms) mummy.</li></ul></li><li>程欣, 陈桦. 二语朗读中不当停顿的感知研究[J]. 外语与外语教学, 2020,1(01): 81.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 特征 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ETS 《Automated Speaking Assessment》</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ets-automated-speaking-assessment/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ets-automated-speaking-assessment/</url>
      
        <content type="html"><![CDATA[<h1 id="automated-speaking-assessment">1. Automated speakingassessment</h1><ul><li>Automated speaking assessment: Using language technologies to scorespontaneous speech[M]. Routledge, 2019.</li></ul><p><em>Innovations in LANGUAGE LEARNING and ASSESSMENT at ETS</em>系列共3卷：</p><ul><li>Volume 1: Second Language Educational Experiences for AdultLearners</li><li>Volume 2: English Language Proficiency Assessments for YoungLearners</li><li>Volume 3: Automated speaking assessment: Using language technologiesto score spontaneous speech</li></ul><p>本书是ETS2019年出版的，侧重于自发语音评分。由于该书的目标群体包括非专业人士，所以技术细节相对较少。</p><h1 id="educational-testing-service-ets">2. Educational Testing Service(ETS)</h1><blockquote><p>美国教育考试服务中心，成立于1947年，是全球最大的民营非营利性教育考试和评估机构。ETS每年对全球180多个国家逾5000多万次考试进行开发、管理和评分，包括TOEFL(Test of English as a Foreign Language), TOEIC (Test of English forInternational Communication), GRE (Graduate RecordExamination)等。其研究范围非常广泛，涵盖全球英语教学、学习和评估（认知模型和学习设计原则），评估的设计与开发（信度与效度，涉及统计学、心理测量学和评估技术等），基于人工智能的个性化学习路径（持续评估进度、即时反馈、建议）等。其产品包括SpeechRater（自发语音评分和反馈）、e-raterscoring engine（写作技巧评估和反馈）。</p></blockquote><ul><li><a href="https://www.ets.org/about/what.html" class="uri">https://www.ets.org/about/what.html</a></li><li><a href="https://en.wikipedia.org/wiki/Educational_Testing_Service" class="uri">https://en.wikipedia.org/wiki/Educational_Testing_Service</a></li><li><a href="https://baike.baidu.com/item/ets/12424958" class="uri">https://baike.baidu.com/item/ets/12424958</a></li></ul><h1 id="自动语音评分系统">3. 自动语音评分系统</h1><p>对于研究人员和从业者来说，重要的是不要孤立地考虑自动评分系统，而是将其作为整个评估周期的一部分，包括测试规范、任务设计、特征提取、分数生成、分数报告和分数使用。</p><ul><li>测试设计：首先，确定语言测试的设计目的是什么，期望给予什么反馈。基于此，可以决定引出什么领域（比如学术场景）下的什么样的语言行为。之后，可以更精细地分析所考察的各个维度，同时考虑如何以最适合自动评分的方式实现。</li><li>Construct：评估旨在考察的一组知识、技能和能力，由语言使用场景的需求确定。如果自动评分系统难以准确、全面地测量，测试设计者应该考虑结合人工评分和自动评分是否更合适。</li><li>口头表达的持续时间：使用人工或自动评分，当回答很短时，口语能力的某些方面可能难以可靠地评估，比如流利度。一种解决方案是跨多个回答聚合信息，或者仅使用可以从较短的响应中可靠地提取的测量值，例如内容准确性。</li><li>音质：确保使用高质量的音频非常重要，会影响自动评分系统的性能。并且，应该制定一个程序来标记任何可能音质较差的数据，以便它们能够得到额外的关注。</li><li>有效性：自动语音评分的有效性，还应该考虑测试的性质、如何使用分数等。<ul><li>低风险：比如用于了解学生的学习进度。</li><li>高风险：比如用于升学、招聘等。</li></ul></li><li><blockquote><p>自动评分系统缺乏背景知识，难以对创意、逻辑、想法的质量进行评价。</p><p>自动评分系统用于高风险评估的唯一评分者必须满足3个条件：透明的内部机制、有效性证据的广泛基础（比如自动评分与受试者在某些交流环境下的语言行为强相关，表明了其语言水平）、检测可能的异常性能的质量控制系统。</p></blockquote><ul><li>Zhang, M. , 2013. Contrasting automated and human scoring of essays.ETS R &amp; D Connections 21, 1–11.</li></ul></li></ul><h1 id="asr">4. ASR</h1><ul><li>non-nativeASR的难点：发音错误、不流畅（破坏了语法结构；填充词通常很短，容易与其它单词（如a）混淆）、语法或用词错误。<ul><li>非母语说话人最普遍的3种不流畅现象：filled pauses（比如uh, um, andhm）、不完整的单词（比如Tha-）、重复（比如he he he）。</li></ul></li><li>TOEFLiBT口语考试的非母语自发语音，最优的WER约为20%-25%。依据对人类转写员一致性的研究，TOEFLiBT 数据的WER下限可能在 15%左右。同时也意味着非母语ASR训练集中噪声水平比较高。</li><li>除识别结果外，ASR还可输出置信度分（混淆网络）、AM分和LM分、时间边界，用于后续评分特征的计算。</li><li>建议<ul><li>对于音素级的声学模型，重要的是训练集尽可能覆盖所有可能的音素上下文，而不是目标领域的内容。</li><li>对于大多数应用，找到一个声学条件相似的开源数据集，结合少量领域内数据，可以取得不错的声学模型效果。</li><li>标准化数据收集流程：包括麦克风、采样率、音频文件格式等。如果不确定目标应用程序的设置，最好使用高质量的设备、高采样率、无损压缩或不压缩。此外，重要的是录音环境（如环境噪声、室内混响）、说话人（如年龄、性别、语言水平）、语音（自发语音或朗读）等的特征与目标应用程序匹配。记录每条音频任何相关的元数据，便于后处理。</li><li>可以采用诸如ROVER投票之类的方法，将对同一音频的多个转写员的转写结果合并成一个、更高质量的转写。<ul><li>Fiscus J G. A post-processing system to yield reduced word errorrates: Recognizer output voting error reduction (ROVER)[C]//1997 IEEEWorkshop on Automatic Speech Recognition and Understanding Proceedings.IEEE, 1997: 347-354. (引用量：1615)</li></ul></li><li>转写指南应涵盖非语音声音（如噪声、笑声等）、非字母表示的单词（如数字、日期、货币等，一般应采用如three而不是3）的处理。</li><li>语言模型训练集应尽可能地接近应用场景。所需的数据量很大程度上取决于应用场景下数据的可变性。</li></ul></li></ul><h1 id="filtering-model">5. filtering model</h1><blockquote><ul><li><p>混合评分流程</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc8532827c64dd0e36b6c72d7404fe031?method=download&shareKey=4a3b2eb463fbdfc5a480154fc1ddc6ac" width="647px"></p><p>*若extended filter判为可评分，再输入baseline filter</p></li><li><p>不可评分（non-scorable）的回答类型</p><ul><li>technical difficulty(TD)：严重音质问题，如音量小、截幅<ul><li>静音SI：听不到任何环境音</li><li>噪声掩盖作答NC</li><li>失真DR：如快速回放、慢速回放、过度放大等</li><li>采样缺失MS：采样点丢失</li><li>其它OT</li></ul></li><li>0分回答<ul><li>无语音XS：录音正常但说话人无语音，如呼吸、咳嗽等</li><li>非英语作答XE</li><li>可懂度低XU</li><li>离题XT</li><li>预制的回答：如抄袭</li><li>其它XO</li><li>由于受试者不配合的行为在其数据中极为罕见，所以本文不检测非英语作答、离题、预制的回答等</li></ul></li><li>ASR结果不可信：如ASR结果不合预期的过短或过长，置信度分低</li><li>自动评分不可信：如ASR结果为空</li></ul></li><li><p>baseline filter</p><ul><li>针对TD、0分回答、自动评分不可信</li><li>特征<ul><li>基础特征（22个）：如单词数、语音片段时长、停顿频率、停顿时长</li><li>声学特征（16个）：如能量、音调的均值、标准差，频谱（如MFCC）的变化</li><li>ASR特征（7个）：如AM分、LM分、置信度分的均值、标准差</li></ul></li><li>采用特征选择算法，从上述45个特征中选取了6个特征<ul><li>单词数</li><li>除去不流畅、停顿的片段总时长</li><li>音频能量均值</li><li>音频能量最大值</li><li>音频能量方差</li><li>基于MFCC的音质分nrprob</li></ul></li><li>采用决策树C4.5模型</li></ul></li><li><p>extended filter</p><ul><li>还处理ASR结果不可信的情况。通过分析大分差数据，基于规则构建。</li><li>ConfidenceScoreFilter：词级平均置信度分&lt;0.4</li><li>ShortResponseFilter：仅两个单词或以下，语音过短评价不可靠</li><li>LongResponseFilter：朗读或复述题，词数&gt;=2*预期值。因为SpeechRater会因多说的内容扣分，而人工评分允许重复等</li></ul></li><li><p>评价</p><ul><li><p>数据集</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6f071533e49919ece88056f1298dff6f?method=download&shareKey=69dba328a14d9c968e0190239dad3762" width="857px"></p><p>*Table5：未标记ASR结果不可信的数据，所以未统计</p></li><li><p>评价指标</p><ul><li>precision：检测为不可评分的数据中真正不可评分数据的占比</li><li>recall：不可评分数据被检测到的占比</li><li>f-score</li></ul></li><li><p>评价结果</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6f70b78a1d1a03cc65d9c2156e29df31?method=download&shareKey=4351b82dde2600091db8ee0b67fe1a6e" width="590px"><img src="https://note.youdao.com/yws/api/personal/file/WEBb653d451ce44038ed7f615a9716b895f?method=download&shareKey=039ecb1c0a98b1a3eece0002d00e32a0" width="709px"></p><ul><li>由于人工未标记ASR结果不可信的数据，所以extendedfilter虚警率高符合预期。</li><li>采用extendedfilter可以检测出更多人机偏差较大的数据。HMSD：人机评分绝对差（table8-9排除了人工标记不可评的数据）</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBab4164d29d519b516b8dde7368d4cd2c?method=download&shareKey=90676260ad564a2daa060efebc972192" width="578px"></p><ul><li>imputation：检测为不可评的数据取该受试者可评数据的机器分的均值。</li><li>hybrid：检测为不可评的数据 人工评分。</li></ul></li></ul></li><li><p>hybrid评分的缺点：人工评分的人力和时间成本。</p></li><li><p>参考文献</p><ul><li>Bejar, I.I. , 2011. A validity-based approach to quality control andassurance of automated scoring. Assess. Edu. 18 (3), 319–341 .</li></ul></li></ul></blockquote><ul><li>Yoon S Y, Zechner K. Combining human and automated scores for theimproved assessment of non-native speech[J]. Speech Communication, 2017,93: 43-52.</li><li>不可评分的回答类型，除上述外，还有：<ul><li>笼统的回答：仅包含填充词的回答或笼统的回答，例如“I don't know”、“Itis too difficult to answer”、“well” 等。</li><li>重复题干：重复提问或朗读听力材料。</li></ul></li><li>除无语音外，其它类型的回答可能由于流利度较好而得到较高的分数。</li><li>non-scorable检测<ul><li>通过计算ASR结果与参考答案的文本相似度检测离题、重复题干等。采用wordembedding可以避免要求词的精确匹配。缺点：需要事先为每个题目收集大量的参考答案。</li><li>SpeechRater:实际应用场景下游戏作答的比例非常低，因此主要检测无语音和不适合自动评分的数据。<ul><li>采用音质、能量、语音长度等特征，决策树模型，检测不可评和0分数据。</li><li>基于规则的filters：过滤作答过短、录音的平均能量过大或过小、ASR结果不可信的数据，避免评分不可靠。</li></ul></li></ul></li><li>【音质】Jeon J H, Yoon S Y. Acoustic feature-based non-scorableresponse detection for an automated speaking proficiencyassessment[C]//Thirteenth Annual Conference of the International SpeechCommunication Association. 2012.</li><li>【非英语】Yoon S Y, Higgins D. Non-English response detection methodfor automated proficiency scoring system[C]//Proceedings of the SixthWorkshop on Innovative Use of NLP for Building Educational Applications.2011: 161-169.</li><li>【抄袭】Wang X, Evanini K, Bruno J, et al. Automatic plagiarismdetection for spoken responses in an assessment of english languageproficiency[C]//2016 IEEE Spoken Language Technology Workshop (SLT).IEEE, 2016: 121-128.</li><li>【离题】Malinin A, Van Dalen R, Knill K, et al. Off-topic responsedetection for spontaneous spoken english assessment[C]//Proceedings ofthe 54th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers). 2016: 1075-1084.</li><li>【garbage model】Cheng, J. , Shen, J. , 2011. Off-topic detection inautomated speech assessment applications. In: Proceedings ofInterSpeech, pp. 1597–1600 .</li></ul><h1 id="评分特征">6. 评分特征</h1><ul><li>评分标准包含3个维度：发音和流利度、语用（词汇和语法）、内容和语篇。</li><li>SpeechRater计算了大约 170个特征，本书不包含约60个左右与受限或可预测语音相关的特征。排除了特征间相关系数&gt;0.9的特征。</li><li>特征处理<ul><li>为了支持特征与评分之间的非线性关系、归一化特征分布，将原始特征进行非线性变换，比如取平方根、取对数等<ul><li><font color="red"> 以下部分特征备注了对应的变换，部分特征未备注</font></li></ul></li><li>符号翻转（乘以-1）：使其与人工分正相关?</li><li>99%winsorization处理：根据特征分布，阈值取分布上端、下端的各5%，超过离群点阈值的特征值匹配到对应的阈值</li><li>归一化（0均值、1标准差）</li></ul></li><li>数据集：从2012-2015 TOEFL iBT考试中收集的约24万个回答。</li></ul><h2 id="发音">6.1. 发音</h2><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th></th><th>特征</th><th>Pearson相关系数</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>音段特征</td><td>AcousticModelScore</td><td><font color="red"> 0.43 </font></td><td>整句AM分/音素数</td><td></td></tr><tr><td></td><td>Pronunciation1</td><td>-0.37</td><td>所有单词AM分的和</td><td></td></tr><tr><td></td><td>Pronunciation2</td><td>0.27</td><td>Pronunciation1/单词数</td><td></td></tr><tr><td></td><td>Pronunciation3</td><td>0.39</td><td>Pronunciation1/音素数</td><td></td></tr><tr><td></td><td>VowelDuration</td><td><font color="red"> -0.4 </font></td><td>句中各个元音的平均时长分别与在native数据集上统计的各音素的平均时长的绝对差</td><td></td></tr><tr><td>重音相关</td><td>StressedSyllPercent</td><td>0.38</td><td>重读音节占比</td><td></td></tr><tr><td></td><td>StressDistanceSyllMean</td><td>-0.37</td><td>重读音节距离的均值（in syllables）</td><td></td></tr><tr><td></td><td>StressDistanceSyllSD</td><td>-0.33</td><td>重读音节距离的平均差（in syllables）</td><td>log</td></tr><tr><td></td><td>StressDistanceMean</td><td><font color="red"> -0.47 </font></td><td>重读音节距离的均值（s）</td><td>log</td></tr><tr><td></td><td>StressDistanceSD</td><td><font color="red"> -0.41 </font></td><td>重读音节距离的平均差（s）</td><td></td></tr><tr><td>语调相关</td><td>ToneDistanceSyllMean</td><td>0.08</td><td>带调音节距离的均值（in syllables）</td><td>sqrt</td></tr><tr><td></td><td>ToneDistanceSyllSD</td><td>0.10</td><td>带调音节距离的平均差（in syllables）</td><td>sqrt</td></tr><tr><td>韵律相关</td><td>VowelPercent</td><td>-0.30</td><td>元音时长/总单词时长的百分比</td><td>\</td></tr><tr><td></td><td>VowelDurationSD</td><td>-0.26</td><td>元音音程标准差</td><td></td></tr><tr><td></td><td>ConsonantDurationSD</td><td>-0.20</td><td>辅音音程标准差</td><td></td></tr><tr><td></td><td>SyllableDurationSD</td><td>-0.31</td><td>音节时长标准差</td><td></td></tr><tr><td></td><td>ConsonantSDNorm</td><td>-0.22</td><td>辅音音程标准差/辅音音程平均时长</td><td></td></tr><tr><td></td><td>SyllableSDNorm</td><td>-0.24</td><td>音节时长标准差/音节平均时长</td><td></td></tr><tr><td></td><td>VowelPVI</td><td>-0.39</td><td>元音音程的PVI</td><td></td></tr><tr><td></td><td>ConsonantPVI</td><td>-0.36</td><td>辅音音程的PVI</td><td></td></tr><tr><td></td><td>SyllablePVI</td><td>-0.36</td><td>音节的PVI</td><td>log</td></tr><tr><td></td><td>VowelPVINorm</td><td>-0.25</td><td>归一化的元音音程PVI</td><td></td></tr><tr><td></td><td>ConsonantPVINorm</td><td>-0.32</td><td>归一化的辅音音程PVI</td><td>\</td></tr><tr><td></td><td>SyllablePVINorm</td><td>-0.29</td><td>归一化的音节PVI</td><td></td></tr></tbody></table><ul><li>特征与<font color="red">总分人工分（非维度分）</font>间的相关系数，<font color="green">P值&lt;0.01，有显著的统计学差异</font>。</li><li>两个声学模型：ASR中的AM用non-native语音训练，且仅用于ASR；第2个AM用native（主要）、高阶的non-native语音训练，采用上述识别结果计算AM分。<ul><li>Chen L, Zechner K, Xi X. Improved pronunciation features forconstruct-driven assessment of non-native spontaneousspeech[C]//Proceedings of human language technologies: The 2009 annualconference of the North American chapter of the Association forComputational Linguistics. 2009: 442-449.</li></ul></li><li>重读、语调检测决策树：采用30个左右能量、音调、时长、word-intensity、单词内音节位置、词典重音、与上一重读或带调音节的距离（syllables）相关的特征，利用人工标注的数据集，训练决策树检测子句或句子中的重读音节或带调的音节（升调、降调、无调）。<ul><li>Zechner K, Xi X, Chen L. Evaluating prosodic features for automatedscoring of non-native read speech[C]//2011 IEEE workshop on automaticspeech recognition &amp; understanding. IEEE, 2011: 461-466.</li></ul></li><li>相较于音节相关的重读特征，时间相关的特征与人工分的相关性更高，可能由于还表征了语速。</li><li>语调：更高水平的说话人可能表现出更长的带调短语（基于音节，而不是时间），且长度变化更大（由特征相关系数的正负推断）。</li><li>韵律特征：Chen L, Zechner K. Applying rhythm features toautomatically assess non-native speech[C]//Twelfth annual conference ofthe international speech communication association. 2011.</li><li>展望：提取语调轮廓相关的特征。</li></ul><h2 id="流利度">6.2. 流利度</h2><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th></th><th>特征</th><th>Pearson相关系数</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>停顿相关</td><td>FilledPauseRate</td><td>-0.23</td><td>filled pause (uh, um)数/秒</td><td>sqrt</td></tr><tr><td></td><td>Silences</td><td>-0.26</td><td>静音次数</td><td></td></tr><tr><td></td><td>SilenceMean</td><td>-0.32</td><td>静音时长均值(s)</td><td>log</td></tr><tr><td></td><td>SilenceAbsoluteDeviation</td><td>-0.32</td><td>静音时长平均差</td><td></td></tr><tr><td></td><td>SilenceRate1</td><td><font color="red"> -0.50 </font></td><td>静音次数/总词数</td><td></td></tr><tr><td></td><td>SilenceRate2</td><td><font color="red"> -0.45 </font></td><td>静音次数/总时长(s)</td><td></td></tr><tr><td></td><td>LongSilenceDeviation</td><td>-0.26</td><td>长静音的平均差(s)</td><td></td></tr><tr><td></td><td>SilenceDistribution1</td><td><font color="red"> -0.45 </font></td><td>子句内长静音/子句内静音</td><td></td></tr><tr><td></td><td>SilenceDistribution2</td><td>-0.28</td><td>子句内静音的时长均值</td><td></td></tr><tr><td>语速相关</td><td>SpeakingRate</td><td><font color="red"> 0.54 </font></td><td>词数/秒（排除音频首尾的静音）</td><td></td></tr><tr><td></td><td>ArticulationRate</td><td>0.38</td><td>词数/秒（总时长-静音、filled pauses时长）</td><td></td></tr><tr><td></td><td>LengthOfRunWords</td><td><font color="red"> 0.45 </font></td><td>chunk（连续的词序列，中间没有filledpauses、超过0.195s的静音）内平均词数</td><td>sqrt</td></tr><tr><td></td><td>RunLengthWordsSD</td><td><font color="red"> 0.41 </font></td><td>chunk平均差</td><td></td></tr><tr><td>修正相关</td><td>InterruptionPointRate1</td><td>-0.23</td><td>Repair IP数/子句总数</td><td>sqrt</td></tr><tr><td></td><td>InterruptionPointRate2</td><td>-0.26</td><td>Repair IP数/总词数</td><td>sqrt</td></tr><tr><td></td><td>RepetitionRate</td><td>-0.28</td><td>重复数/总词数</td><td></td></tr></tbody></table><ul><li>静音：两个单词间的静音时长 &gt; 0.145s；子句内长静音：时长 &gt;0.195s；长静音：时长 &gt; 0.495s。</li><li>IP（interruptionpoint）：一段语音被丢弃或修正，开始新的句子或短语的位置。</li><li>定位语音中的修正现象，包括：restart（丢弃句子，重新开始一句话）、重复（单词或短语）、修正（一个或多个单词被替换为“正确”的单词）。<ul><li>Chen L, Yoon S Y. Detecting structural events for assessingnon-native speech[C]//Proceedings of the Sixth Workshop on InnovativeUse of NLP for Building Educational Applications. 2011: 38-45.</li><li>Chen L, Yoon S Y. Application of structural events detected on ASRoutputs for automated speaking assessment[C]//Thirteenth AnnualConference of the International Speech Communication Association.2012.</li></ul></li><li>修正现象与总分相关度较低。</li><li>表达关心或道歉时，较慢的语速可能更表明其交际能力。</li></ul><h2 id="词汇语法">6.3. 词汇、语法</h2><table style="width:100%;"><colgroup><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"></colgroup><thead><tr><th>维度</th><th>子类别</th><th>特征</th><th>Pearson相关系数（绝对值）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>词汇</td><td>词汇丰富度</td><td>Types</td><td><font color="red"> 0.49 </font></td><td>unique单词数</td><td>\</td></tr><tr><td></td><td></td><td>TypeTokenRatio</td><td>0.10</td><td>unique单词数/总词数</td><td></td></tr><tr><td></td><td>平均单词难度</td><td>AverageVocabularyFrequency</td><td>0.36</td><td></td><td></td></tr><tr><td></td><td></td><td>LogVocabularyFrequency</td><td>0.16</td><td>各unique单词的log词频的和/unique单词数</td><td>\</td></tr><tr><td></td><td>词频表</td><td>VocabularyRank1</td><td>0.17</td><td>回答中在最高频的100词中的单词数/回答总词数</td><td></td></tr><tr><td></td><td></td><td>VocabularyRank3</td><td>0.11</td><td>Rank3：词频排序301-700</td><td></td></tr><tr><td></td><td></td><td>VocabularyRank6</td><td>0.14</td><td>Rank6：词频排序大于3000</td><td>\</td></tr><tr><td>语法</td><td>基于词性</td><td>SyntacticSimilarityScore1</td><td>0.30</td><td>回答与人工分为1的语料库中的回答的POS序列相似度</td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore2</td><td>0.34</td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore3</td><td>0.39</td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityScore4</td><td><font color="red"> 0.42 </font></td><td></td><td></td></tr><tr><td></td><td></td><td>SyntacticSimilarityMax</td><td>0.31</td><td>相似度最高的人工分</td><td></td></tr><tr><td></td><td>基于子句</td><td>ClausesCount2</td><td>0.33</td><td></td><td></td></tr><tr><td></td><td></td><td>DependentClause</td><td>0.26</td><td>附属限定从句数量</td><td></td></tr><tr><td></td><td></td><td>DependentInfinitives</td><td>0.21</td><td>附属非限定从句数量</td><td>log(x+0.5)</td></tr><tr><td></td><td>基于短语</td><td>NounPhrases</td><td>0.38</td><td>名词短语数量</td><td></td></tr><tr><td></td><td></td><td>PrepositionalPhrases</td><td>0.33</td><td>介词短语数量</td><td>\</td></tr><tr><td></td><td></td><td>VerbPhrases</td><td><font color="red"> 0.40 </font></td><td>动词短语数量</td><td>\</td></tr><tr><td></td><td></td><td>ComplexNominals</td><td>0.22</td><td>基于embedding的复杂名词短语</td><td></td></tr><tr><td></td><td></td><td>CoordinatePhrases</td><td>0.10</td><td>并列短语（并列连词连接的短语）数量</td><td>sqrt</td></tr><tr><td></td><td></td><td>NounPhrasesNorm</td><td>0.11</td><td>名词短语数量/总词数</td><td></td></tr></tbody></table><ul><li>特征提取流程：ASR -&gt; 句子边界检测 -&gt; 词性标记、句法分析 -&gt;特征计算。</li><li>平均单词难度：从TOEFL学术语言语料库中统计了大量学术用语的频次，Rank为在按词频逆序排列的词汇表中的次序。<ul><li>Biber D, Conrad S, Reppen R, et al. Speaking and writing in theuniversity: A multidimensional comparison[J]. TESOL quarterly, 2002,36(1): 9-48.</li></ul></li><li>词频表（Lexical frequencyprofile）：根据在TOEFL学术语言语料库中的频次分为了6个等级：Rank1（最高频的1-100）、Rank2（101-300）、Rank3（301-700）、Rank4（701-1500）、Rank5（1501-3000）、Rank6（大于3000）。</li><li>基于词性的特征：Bhat S, Yoon S Y. Automatic assessment of syntacticcomplexity for spontaneous speech scoring[J]. Speech Communication,2015, 67: 42-57.</li><li>SpeechRater使用的词性标记：PennTreebank词性集（36个）+6个为口语设计的标记。<ul><li>Marcinkiewicz M A. Building a large annotated corpus of English: ThePenn Treebank[J]. Using Large Corpora, 1994, 273.（引用量9805）</li></ul></li><li>词性序列特征可以在一定程度上评价语法准确性。另外，相较于基于句法分析的特征，基于词性的特征对不可避免的ASR错误更加鲁棒。但解释性较差。</li><li>基于子句、短语的特征：Chen M, Zechner K. Computing and evaluatingsyntactic complexity features for automated scoring of spontaneousnon-native speech[C]//Proceedings of the 49th annual meeting of theAssociation for Computational Linguistics: Human Language Technologies.2011: 722-731.<ul><li><a href="https://langeek.co/en/grammar/course/713/dependent-clauses/intermediate">附属从句、限定从句、非限定从句</a></li></ul></li><li>未归一化的特征与回答的长度强相关。</li><li>暂无评价词汇、语法准确性的特征：一方面，非母语自发语音ASRWER接近20%，（另外由于语言模型的作用，）内部分析表明识别结果仅包含原始语法错误的约30%；另一方面，从较短的口语回答中提取的特征波动较大，比如，1个句子边界检测错误就可能严重影响特征的准确性。<ul><li>作文自动评分中有评价词汇、语法的自动检错技术。</li></ul></li><li>其它特征<ul><li>词汇丰富度<ul><li>D measure：由于TTR受文本长度影响，Dmeasure是TTR的一种复杂数学变换，不受长度影响。<ul><li>Malvern D, Richards B, Chipere N, et al. Lexical diversity andlanguage development[M]. New York: Palgrave Macmillan, 2004.</li></ul></li><li>trigrams频次得分</li></ul></li><li>句法复杂性<ul><li>句子平均长度</li><li>T-unit（主句，包含其附属的从句或非子句单元）内动词短语的数量</li></ul></li><li>单词、语法准确性<ul><li>没有错误的T-unit占比</li><li>动词时态、第三人称单数、复数标记、介词、冠词的准确性</li></ul></li></ul></li><li>所需技术：子句或句子边界检测、词性标注、句法解析器。</li></ul><h2 id="内容">6.4. 内容</h2><ul><li>基于参考答案：与提前给定的1个参考答案计算文本相似度。<ul><li>WordNet-based text-to-text similarity metrics：Xiong W, Evanini K,Zechner K, et al. Automated content scoring of spoken responsescontaining multiple parts with factual information[C]//Speech andLanguage Technology in Education. 2013.</li><li>1个参考答案无法覆盖所有可能的正确回答，但是在实际的大规模测试中收集多个参考答案成本较高。</li></ul></li><li>基于回答：计算与已经评分的回答的文本相似度。<ul><li>内容向量分析、潜在语义分析、Pointwise Mutual Information（互信息）<ul><li>Xie S, Evanini K, Zechner K. Exploring content features forautomated speech scoring[C]//Proceedings of the 2012 conference of theNorth American chapter of the Association for Computational Linguistics:Human language technologies. 2012: 103-111.</li><li>优化<ul><li>word embedding</li><li>Doc2Vec：将回答转换为100维的向量<ul><li>Le Q, Mikolov T. Distributed representations of sentences anddocuments[C]//International conference on machine learning. PMLR, 2014:1188-1196.（引用量10384）</li></ul></li></ul></li></ul></li><li>ROUGE：用于评价文本摘要和机器翻译质量的标准指标。采用少量（少于10）满分回答。<ul><li>Loukina A, Zechner K, Chen L. Automatic evaluation of spokensummaries: the case of language assessment[C]//Proceedings of the ninthworkshop on innovative use of NLP for building educational applications.2014: 68-78.</li></ul></li><li>对于每个题目，需要事先人工标注足量的回答。</li></ul></li><li>基于提示：比如引发自由表述的听力或阅读材料。相较于前两者，相关度较低，但成本低。</li></ul><h2 id="语篇">6.5. 语篇</h2><ul><li><p>基于surface（单词和连接词链）的特征</p><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>特征</th><th>与语篇人工分的Pearson相关系数（基于人工转写）</th><th>（基于ASR输出）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>Pronouns</td><td>0.204</td><td>0.186</td><td>代名词数量</td><td></td></tr><tr><td>PronounsNorm2</td><td>-0.128</td><td>-0.106</td><td>代名词词数/unique名词数量</td><td></td></tr><tr><td>Conjunctions</td><td>0.174</td><td>0.209</td><td>连词数量</td><td>\</td></tr><tr><td>ConnectiveTypes</td><td>0.381</td><td>0.352</td><td>unique语篇连接词数量</td><td></td></tr><tr><td>Connectives</td><td>0.337</td><td>0.33</td><td>语篇连接词数量</td><td>\</td></tr><tr><td>ConnectiveChains1</td><td>0.068</td><td>0.155</td><td>回答与各参考答案的语篇连接词链的BLEU分的最大值</td><td></td></tr><tr><td>ConnectiveChains2</td><td>0.282</td><td>0.268</td><td>回答与各参考答案的语篇连接词链的编辑距离的最小值</td><td></td></tr></tbody></table><ul><li>语篇连接词（来自Penn DiscourseTreebank中的语篇连接词列表），分为4类：<ul><li>所有从属连词：从属连词引入句法上依赖于主句的子句，常见关系：时间（比如when、assoon as）、因果（比如because）、让步（比如although、eventhough）、目的（比如so that、in orderthat）、条件（比如if、unless）。</li><li>所有并列连词：比如and、but、or。</li><li>部分Adverbial连接词：表达两个事件或状态之间的语篇关系，比如however、therefore、then、asa result、in addition、in fact等。</li><li>相邻句子间的隐式连接词。</li></ul></li><li>连接词链特征：采用高分数据，提取连接词链（仅保留代词、连词、语篇连接词，其他词都被删除）。计算测试数据与每个参考回答的连接词链相似度。<ul><li>相似度计算：BLEU分数、编辑距离、WER（归一化的编辑距离）</li></ul></li><li>特征较简单，旨在对ASR错误和口语中的语法错误更鲁棒。</li><li>Wang X, Evanini K, Zechner K, et al. Modeling Discourse Coherencefor the Automated Scoring of Spontaneous Spoken Responses[C]//SLaTE.2017: 132-137.</li></ul></li><li><p>基于RST的特征</p><ul><li><p>修辞结构理论（Rhetorical Structure Theory，RST）</p><ul><li>首先确定基本语篇单元（elementary discourseunits，EDU）（不重叠的文本片段），通过修辞关系连接相邻单元构成层次树结构。</li><li>78种修辞关系：53种单核关系（两个相邻单元，其一更重要） +23种多核关系（两个或多个单元，权重相同）。<ul><li>Carlson L, Marcu D. Discourse tagging reference manual[J]. ISITechnical Report ISI-TR-545, 2001, 54(2001): 56.</li></ul></li><li>RST Discourse Treebank：一个标准测试集，包含从Penn Treebank中精选的385篇华尔街日报文章，并基于RST框架进行了语篇结构标注。</li><li><a href="https://www.isi.edu/~marcu/discourse/">标注工具</a></li><li>可基于上述数据集构建自动RST解析器，检测语法错误，并提供反馈。</li></ul></li><li><p>数据集：ETS对1440条TOEFL iBT测试中的自发口语进行了标注。</p><ul><li><p>相较于书面文本，非母语的自发语音经常包含语法错误、填充、修正、重复、falsestart、未完成的话语等。因此在RST DiscourseTreebank标记指南的基础上，添加如下关系：</p><ul><li>不流畅：其中不流畅的片段是satellite，对应的流畅片段是核。</li><li>Awkward：语篇结构不恰当的部分。<ul><li>aukuard-Reason：预期的关系很明确，但表达不连贯。</li><li>aukward-Other：该片段与周围的话语没有明确关系。</li></ul></li><li>未完成的句子：考试时间结束但考生未说完。其中，未完成的片段是satellite，语篇树的根结点是核。</li><li>Discourse Particle：比如<em>youknow</em>、<em>right</em>等，它们是相邻片段的satellite。</li><li>Wang X, Bruno J, Molloy H, et al. Discourse annotation of non-nativespontaneous spoken responses using the rhetorical structure theoryframework[C]//Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Papers). 2017:263-268.</li></ul></li><li><p>对语篇连贯性人工评分，分为1（不连贯）、2（有点连贯）、3（高度连贯）3档，并要求对于评分为2的数据，尽量忽略不流畅或语法错误，标记出不连贯的具体位置。</p></li><li><p>示例</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB262ab5bd2c3af12b5701a09012d4363e?method=download&shareKey=ff921fe7831c7a7f826d9b431c4d0079" width="578px"></p></li></ul></li><li><p>特征</p><table style="width:100%;"><colgroup><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"><col style="width: 16%"></colgroup><thead><tr><th>特征</th><th>Pearson相关系数-总分（人工转写、人工标注RST树）</th><th>语篇分</th><th>总分（ASR、自动解析的RST树）</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>EDUs</td><td><font color="red"> 0.612 </font></td><td>0.366</td><td>0.424</td><td>EDU数量</td><td></td></tr><tr><td>Relations</td><td><font color="red"> 0.624 </font></td><td>0.391</td><td>0.401</td><td>关系数量</td><td></td></tr><tr><td>AwkwardRelations</td><td><font color="red"> -0.425 </font></td><td>-0.533</td><td>-0.096</td><td>不恰当的语篇结构关系数量</td><td></td></tr><tr><td>RhetoricalRelations</td><td><font color="red"> 0.719 </font></td><td>0.536</td><td>0.418</td><td>排除不流畅、awkward的关系数量</td><td></td></tr><tr><td>RhetoricalRelationTypes</td><td><font color="red"> 0.675 </font></td><td>0.547</td><td>0.314</td><td>unique rhetorical relation 数量</td><td></td></tr><tr><td>RhetoricalRelationPercent</td><td><font color="red"> 0.586 </font></td><td>0.609</td><td>0.225</td><td>rhetorical relation 占比</td><td></td></tr><tr><td>TreeDepth</td><td><font color="red"> 0.402 </font></td><td>0.249</td><td>0.329</td><td>RST树的深度</td><td></td></tr><tr><td>EDUDepthRatio</td><td><font color="red"> 0.536 </font></td><td>0.308</td><td>0.316</td><td>EDU数量/RST树深度</td><td></td></tr></tbody></table></li><li><p>Wang X, Gyawali B, Bruno J V, et al. Using Rhetorical StructureTheory to assess discourse coherence for non-native spontaneousspeech[C]//Proceedings of the Workshop on Discourse Relation Parsing andTreebanking 2019. 2019: 153-162.</p></li><li><p>展望</p><ul><li>提高对ASR错误的鲁棒性，比如根据置信度分排除部分单词或短语。</li><li>开发内容特征：细粒度地评价指定观点是否存在。</li><li>开发与主题发展结构相关的特征：比如比喻的使用、论证策略、对事实示例的恰当使用等。</li></ul></li></ul></li></ul><h1 id="评分模型">7. 评分模型</h1><ul><li><p>人工设计的特征可以保证与特定的评分维度一致，使得评分模型易于解释。但是可能和人类评分的方式不完全一致，比如人类关注沟通的准确有效性，这是比句法复杂性特征复杂得多的关系。此外，ETS尝试了采用深度学习构建端到端评分系统，到目前为止，相较于SpeechRater基线的改进不大（从Pearsonr=0.585到r=0.602），但其缺乏可解释性，且可能学习到与评分维度不相关的特征。</p><ul><li>Chen L, Tao J, Ghaffarzadegan S, et al. End-to-end neural networkbased automated speech scoring[C]//2018 IEEE international conference onacoustics, speech and signal processing (ICASSP). IEEE, 2018:6234-6238.</li></ul></li><li><p>纯数据驱动的特征选择可能导致评分模型学习到与口语水平无关变量（如语音长度）的影响。SpeechRater结合理论基础和数据驱动进行特征选择。为了确保最终模型是合适的，至少在高风险测试中，自动选择的特征集由专家审查，并在必要时进行调整。</p><ul><li><p>自动特征选择：LASSO</p><p>Loukina A, Zechner K, Chen L, et al. Feature selection for automatedspeech scoring[C]//Proceedings of the Tenth Workshop on Innovative Useof NLP for Building Educational Applications. 2015: 12-19.</p></li></ul></li><li><p>评分模型</p><ul><li>决策树：更接近人类评分员的打分过程，比如在不同分数段考虑不同的评分特征、支持评分与特征间的非线性关系。</li><li>随机森林回归：可以视为多个评分员的平均分。</li><li>SVM：可以识别不同分数水平之间的边界，实现数据的最佳分离。</li><li>但是，相较于线性回归，上述模型通常并无改进或改进很小。</li><li>深度神经网络的缺点：可解释性差，难以获得可用的、有意义的反馈信息。</li></ul></li><li><p>为了支持特征与评分之间的非线性关系，可以事先将原始特征进行非线性变换，比如取平方根、取对数等。</p></li><li><p>特征在线性回归模型中的权重可能和其与人工分的相关系数正负不一致，可能是由于特征之间的多重共线性不明确（unclearmulticollinearity）和/或权重较小导致的。</p></li><li><p>使用更大的训练集进行模型训练尚未被证明是有利的；大约1-2万个回答通常足以实现TOEFL iBT 数据上的最佳模型性能。</p></li><li><p>评分模型</p><blockquote><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe8bdfbf008095926fb10bbdb11158ebd?method=download&shareKey=d7a2b29e988f59a3c86bcc049cf6fd33" width="615px"><img src="https://note.youdao.com/yws/api/personal/file/WEB826535314fa7107a57c42094059601ce?method=download&shareKey=2698bc0877fb1ab709328489d9685bff" width="584px"><img src="https://note.youdao.com/yws/api/personal/file/WEBb46b7820f1a80f6954d20d0c4f73bd30?method=download&shareKey=53537b60c4c3b2902dfce5aec4198d47" width="614px"></p><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th>数据集</th><th>说明</th><th>训练集总数据量</th><th>平均每个模型的训练集数据量</th><th>平均每个模型的测试集数据量</th></tr></thead><tbody><tr><td>main</td><td>随机选取、单人评分</td><td>464,664</td><td>77,444</td><td>36505</td></tr><tr><td>main*</td><td>与exemplar等数据量</td><td>12,398</td><td>2,066</td><td></td></tr><tr><td>exemplar</td><td>双人评一致，且由多名专家复审保证评分准确</td><td>12,390</td><td>2,065</td><td>689</td></tr></tbody></table><ul><li>评分特征：77个，覆盖发音和流利度、语用（词汇和语法），不评价内容维度。</li><li>当训练集足够大时，用不同训练集训练的系统在同一测试集上性能几乎相同。大数据量对于抵消训练集中存在的hardcase的影响较为重要。虽然相较于随机构建的训练集，采用人人一致性较高的训练集，达到最佳性能所需的数据量更少，但是收集足够数量的人人一致性较高数据的成本可能更高。</li><li>相较于随机构建的测试集，在人人一致性较高的测试集上人机一致性较高(r =0.80 vs. r =0.66)。相较于在标签存在争议的数据上预测出错，在标签一致确认的数据上犯错，是更严重的有效性问题。因此，人人一致的测试集有助于评估评分系统的性能。</li><li>相较于线性回归，其它线性、非线性机器学习模型至多只有较微小的评分性能提升。</li><li>在同一数据集上训练的不同评分模型，或者在不同数据集上训练的同一模型，预测结果的相关性很高（r= 0.97 / 0.98）。模型组合策略可能不会特别有效。</li></ul></blockquote><ul><li>Loukina A, Zechner K, Bruno J, et al. Using exemplar responses fortraining and evaluating automated speech scoring systems[C]//Proceedingsof the thirteenth workshop on innovative use of NLP for buildingeducational applications. 2018: 1-12.</li></ul></li><li><p>由于ETS将自动语音评分用于考试，所以其很重要的一个考量是自动评分的透明性和有效性。</p></li><li><p>评价指标：人机评分的一致性。包括各种统计指标：kappa、quadraticallyweighted kappa、Pearson相关系数、一致性比例等。</p></li></ul><h1 id="自动反馈">8. 自动反馈</h1><ul><li>提供自动反馈的商业语言学习软件<ul><li>SRI International's EduSpeak：发音错误检测</li><li>NativeAccent：由CarnegieSpeech开发，检测发音、韵律错误，通过音频（正确发音的音频）、文本（比如关于声道活动的文本描述）、图片（比如声道图）等形式提供反馈</li><li><a href="http://englishdiscoveries.net/english-discoveries/">EnglishDiscoveries</a>：由ETS的子公司开发，根据单词的声学分数进行染色</li></ul></li><li>SpeechRater自动反馈系统<ul><li><p>特征选择标准：可靠性、特征的透明性和可理解性、帮助用户提升口语水平的可操作性、同一说话人在不同类型的测试任务上相对一致（相较于提供特定任务的反馈，可以提供特定说话人在所有任务上的综合反馈）、与人工分相关度较高、特征集合覆盖了各个维度。</p></li><li><p>SpeechRater报告中选用的特征</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>报告中使用的名称（SpeechRater特征名）</th><th>维度</th><th>分数报告中提供的描述</th></tr></thead><tbody><tr><td>语速（SpeakingRate）</td><td>流利度</td><td>语速表示每分钟说的词数。水平较高的说话人往往语速较快（注意：如果语速过快，别人可能很难听懂）。</td></tr><tr><td>连续语音（LengthOfRunWords）</td><td>流利度</td><td>连续语音表示不包含停顿、填充词（如um等）的语音片段的平均词数。水平较高的说话人往往可以在不停顿、不使用填充词的情况下说出更多的词。</td></tr><tr><td>停顿频率（SilenceRate1）</td><td>流利度</td><td>停顿频率表示您说话时停顿的频率。水平较高的说话人停顿频率较低。注意：停顿的位置也很重要，比如在句子末尾停顿比在表述一个想法的中间停顿要好。</td></tr><tr><td>重复（RepetitionRate）</td><td>流利度</td><td>重复频率表示您重复一个词或短语的频率，比如“I need to go to, to go tothe library”。水平较高的说话人往往重复频率较小。</td></tr><tr><td>韵律（StressedSyllPercent）</td><td>发音</td><td>韵律表示音节是否被正确地重读。水平较高的说话人往往在适当的音节上明显地重读。</td></tr><tr><td>元音时长（VowelDuration）</td><td>发音</td><td>元音时长表示您的元音发音时长与母语人士的对比。水平较高的说话人的元音时长往往更接近母语人士。</td></tr><tr><td>词汇深度（AverageVocabularyFrequency）</td><td>词汇</td><td>词汇深度表示您的词汇范围。水平较高的说话人倾向于使用各种词。分数越高表示您使用的词越不常用和/或越精确。</td></tr></tbody></table><p>*报告中使用的名称是为了便于用户理解</p></li><li><p>专家选择，心理测量验证</p><ul><li>在同一考试不同任务上的一致性：Cronbach's alpha</li><li>与人工分的相关度：Pearson相关系数</li><li>冗余度：特征间的Pearson相关系数</li></ul></li><li><p>反馈报告设计</p><ul><li><p>由分数报告设计团队和测试开发团队等设计，并进行了两轮可用性研究，用于确定用户能够理解和解释所提供信息的程度，并收集反馈。</p></li><li><p>特征值：特征值的范围难以直接解释。两种方式：采用低-高、最小-最多等定性描述词；采用百分位数定量标记。可用性研究最终决定展示为特征值在一组参考值中的相对位置。注意：需要将不同的特征放在同一参考框架上，便于对比不同的特征。</p></li><li><p>特征值是否应该明确地对应于分数：是。参考组：对每一档分数范围，统计中间50%（四分位范围，inter-quartileranges，IQR，即第25%-第75%）的受试者的特征。由于不同分数段的特征间有重叠，用户可能难以理解，加入了“常见问题”进行解释。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3be7c98727fe465d376a71b4b73c599b?method=download&shareKey=fdcc53770f3bcb23a27f4c8448ee1fd6" width="440px"></p><ul><li>箭头：受试者的特征值在参考组特征值中的百分位数（应该对特征值进行了排序，顺序或逆序）。</li><li>重叠：由于人工分综合考虑了口语水平的各个维度，因此不同水平考生的单一特征值间有重叠。</li></ul></li><li><p>是否包含教学建议：是。由于无法根据每个学生的优劣势提供个性化的建议、特征可能有多种解释（比如停顿通常是由于流利度较差，但也可能是想法之间的简单停顿）、导致分数报告过长，因此，未根据分数报告中的特征提供相应的学习技巧，而是简单地提供了指向ETS备考资源的链接。</p></li><li><p>需要明确解释反馈报告的局限性。</p></li><li><p>示例 <a href="https://note.youdao.com/yws/api/personal/file/WEB825b08e4d81b0b48f83b9514eceb1c73?method=download&amp;shareKey=7dd4ae0dfb62e07c9cf6345931f13c93">图1</a><a href="https://note.youdao.com/yws/api/personal/file/WEB95191f582c107ac31b19e36f3a734d47?method=download&amp;shareKey=a0f19b9335df9a314ccf130e2b9f00c4">图2</a><a href="https://note.youdao.com/yws/api/personal/file/WEBffb192f45435b3becc39ffb68edf8633?method=download&amp;shareKey=f5ac7fb4075458222ff4db6c5dffed22">图3</a></p></li></ul></li></ul></li></ul><h1 id="口语对话系统sds">9. 口语对话系统SDS</h1><ul><li>相较于独白式的自发口语，口语对话可以更全面地评价口语能力，比如轮流策略的使用、不同<font color="green">registers </font> 的恰当使用、语义理解等，也更接近真实的语用场景。</li><li>系统结构：ASR -&gt; 口语理解（spoken language understanding, SLU）-&gt; 对话管理器（dialog manager,DM，决定SDS下一步怎么做，比如问用户另一个问题、给用户呈现信息等） -&gt;语言生成（language generatin, LG） -&gt; 语音合成（TTS）。</li><li><a href="http://www.halef.org/">HALEF框架</a>：一个开源、模块化、基于云的口语对话系统框架，兼容多个W3C和开放行业标准。<ul><li><p>HALEF 语音对话系统示意图</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdf45da51c2aac5e2f524ad65ad485b5c?method=download&shareKey=7de7e638a91d12d335a87dab49cf444a" width="576px"></p></li><li><p>Ramanarayanan V, Suendermann-Oeft D, Lange P, et al. Assemblingthe jigsaw: How multiple open standards are synergistically combined inthe HALEF multimodal dialog system[M]//Multimodal Interaction with W3CStandards. Springer, Cham, 2017: 295-310.</p></li><li><p>其它语音或多模态对话系统</p><ul><li>学术实现：Olympus、Alex、Virtual Human Toolkit、<a href="http://www.opendial-toolkit.net">OpenDial</a></li><li>工业实现：<a href="https://evolution.voxeo.com/prophecy/licensing.jsp">Voxeo</a>、<a href="https://developer.amazon.com/en-US/alexa">Alexa</a></li><li>缺点：其中较多实现使用特殊的架构、接口、编程语言，而很少关注语音、多模态信号处理的现有标准。</li></ul></li></ul></li><li>在TOEFL MOOC中提供了3个口语对话任务<ul><li>The CoffeeSpot：与咖啡师（由口语对话系统播放）互动，从菜单中点一份食物和一份饮料，学习者被提示回答有关定制订单的不同问题（比如饮料的大小、订单是“在这里”还是“外带”）。考察：阅读简单文本（菜单）、理解和遵循书面英语说明、通过英语会话实现预定目标的能力。</li><li>The GroupProject：与虚拟同学互动，讨论课堂项目。学习者需要邀请他们的虚拟同学与他们见面、重温他们准备好的幻灯片，并回答虚拟朋友提出的其他问题。除了“TheCoffee Spot”中的能力外，还考察以适当的方式提出请求的能力。</li><li>The JobInterview：与虚拟的大学职业中心顾问进行模拟面试。被提示回答背景、资格等一系列实际工作面试中可能遇到的典型问题。</li><li>难度递增</li></ul></li><li>应用开发流程<ul><li><p>任务设计。比如，为了引发较长的语音以评价其流利度等，可以要求学习者提供建议（Doyou mind if I asked you for some advice?），并提供详细的理由 (Why do youthink so? Can you tell more?)。</p></li><li><p>应用发布。采用HALEF中的<a href="https://halef.readthedocs.io/en/latest/openvxml.html">OpenVXML</a>，它是一种基于流程图的软件设计工具，封装了语音对话系统所需的各种资源，比如语法模型或语言模型、用于播放的录音；它还使设计人员能够指定控制SDS功能的各种参数，比如语音活动超时阈值、打断设置、语法格式等。</p></li><li><p>作为聊天应用程序众包部署（Amazon Mechanical Turk众包平台）。使用聊天机器人版本的HALEF系统（称为HALEFbot）测试分支对话树，从真实用户处引出文本回应，便于快速且低成本地发现比如用户的意外响应、对话分支中的逻辑错误等，并快速完善。</p></li><li><p>对话优化。</p></li><li><p>语音化。采用之前收集的数据训练定制的语言模型。使用TTS将文本提示转换为音频。同理，也可以采用众包技术进行优化，比如更新语言模型。</p></li><li><p>录制会话。相较于TTS，人工录制的提示可以让语音体验更加身临其境和真实。</p></li><li><p>反馈开发。示例：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa50a972bbfdd32f921cbdfdf83fd7969?method=download&shareKey=118f0c2c235eb0f37f1cec080aa72df3" width="576px"></p></li><li><p>部署。监控系统的鲁棒性、是否存在崩溃或显著影响学习体验的问题。系统运行一段时间后，转写学生的响应以进一步优化ASR效果。分析是否存在相关响应未被分支对话适当处理，并对对话设计进行优化。</p></li></ul>*每一个步骤都是在上一步骤完成后或者上一系统达到预期效果后再进行优化升级的。</li><li>自动评分<ul><li>Ramanarayanan V, Lange P L, Evanini K, et al. Human and AutomatedScoring of Fluency, Pronunciation and Intonation During Human-MachineSpoken Dialog Interactions[C]//INTERSPEECH. 2017: 1711-1715.</li></ul></li><li>挑战<ul><li>心理测量研究需要确定应该从对话任务中收集的观察的数量和种类。</li></ul></li></ul><h1 id="信度与效度">10. 信度与效度</h1><p>信度（reliability）与效度（validity）是教育测试中两个最核心的概念。信度通常指测试结果在不同测试场景下的可复制性以及测试分数在多大程度上反映了“真实分数”，效度通常指对分数的解释和基于分数做出的推论的意义。</p><h2 id="信度">10.1. 信度</h2><p>两种类型的信度：评分准确性（scoringaccuracy）、测试准确性（assessmentaccuracy）。此外，还应考虑测试测量感兴趣的属性的有效性。</p><ul><li><p>本章涉及的额外的特征</p><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>特征</th><th>子维度</th><th>描述</th><th>变换</th></tr></thead><tbody><tr><td>AverageASRConfidenceScore</td><td>发音</td><td>各单词的ASR置信度分的和/单词数量</td><td>\</td></tr><tr><td>SilenceRate3</td><td>流利</td><td>停顿次数/句子长度（排除首尾停顿）</td><td>\</td></tr><tr><td>GrammaticalAccuracyScore4</td><td>语法</td><td>语法表达与人工分满分（4）数据的相似度分</td><td>\</td></tr><tr><td>GrammaticalAccuracyMax</td><td>语法</td><td>语法表达相似度分最高的人工分</td><td>\</td></tr></tbody></table></li><li><p>评分准确性</p><ul><li>true score：随机选择评分员，期望的人工分</li><li>评分员常见现象<ul><li>漂移（随着时间的推移，倾向于更严厉或更宽松的评分）</li><li>光环效应（受到同一考生先前回答分配的分数的影响）</li><li>集中趋势（避免量表的极端）</li><li>疲劳</li></ul></li><li>根据经典测试理论，X=T+e，其中X表示观察到的人工分，T表示未观察到的真实分数（truescore），e表示未观察到的测量误差，e中所有元素的期望值均为0，T和e不相关。</li><li>测量方式<ul><li>最佳线性预测（BLP）</li><li>相较于预测为常量-期望E，均方误差减小的比例（proportional reductionof mean squared error，PRMSE），值越大表示预测越有效。</li><li>stepwise regression<ul><li>回归模型可以仅保留partial <span class="math inline">\(R^2\)</span>(逐步加入特征，模型<span class="math inline">\(R^2\)</span>的增量)&gt;=0.01的特征。</li></ul></li></ul></li></ul></li><li><p>测试准确性</p><ul><li><p>true score：随机选择一项测试，期望的评分</p></li><li><p>测量方式：generalized Cronbach's coefficientalpha，测量内部一致性</p></li><li><p>实验结果</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd7750038b423f64b28adc76345ec636c?method=download&shareKey=f68ed1895db218852f1f614a76b276aa" width="414px"></p><p>*值越大表明其在不同的测试项目中更一致；较小的值可能与不同的测试项目关注不同的能力有关。</p></li></ul></li><li><p>结论</p><ul><li>相较于采用人工分，增加SpeechRater特征的PRMSE较小，因此，在人工分基础上，结合自动评分的有效性增量非常有限。并且，仅使用SpeechRater特征，PRMSE相对较小。</li><li>在人工分基础上，仅有少量特征对预测truescore有明显的帮助：WordTypes、GrammaticalAccuracyMax、AverageASRConfidenceScore、SilenceRate3（该项研究仅使用28个特征，详见书）。特征间的相关性较高。未来工作：采用更少冗余的特征集。</li></ul></li></ul><h2 id="效度">10.2. 效度</h2><p>construct相关性和代表性、与人工评分的相关性、与独立的外部测量的相关性、跨任务和测试形式的分数泛化性、分数能多大程度上反映考生的真实水平、是否能基于自动评分进行适当的决策。</p><h2 id="其它">10.3. 其它</h2><ul><li>测试的真实性：语言学习任务在多大程度上反映了现实生活中可能遇到的语用场景。</li><li>对不同测试群体的公平性。</li></ul><h1 id="挑战">11. 挑战</h1><ul><li>非母语自发语音ASR TOEFLiBT口语考试的非母语自发语音，最优的WER约为20%-25%。依据对人类转写员一致性的研究，TOEFLiBT 数据的WER下限可能在 15%左右。同时也意味着非母语ASR训练集中噪声水平较高。另一方面，说话人之间存在差异，对于某些说话人，ASR的性能可能接近最佳，而对于其他说话人，例如30% 的WER，会导致许多特征计算模块的输入出现严重失真。</li><li>评分特征<ul><li>开发新特征覆盖衡量语音水平的各个维度，特别是语用和结构维度。</li><li>ASR和用于特征提取的NLP组件的错误。</li><li>口头回答通常较短，减少了特征计算的证据基础。改进：同一说话人的多个回答拼接。</li></ul></li><li>过滤模型：如果测试的利害关系相当高，则可以预期应试者会发明新的、创造性的方法来欺骗自动评分系统，因此，对部署的自动评分系统进行持续监控，对于确保分数有效性至关重要。</li><li>持续监控与升级</li><li>测试设计：比如回答的长短、期望考察的维度、任务类型。</li></ul><h1 id="其它-1">12. 其它</h1><ul><li>使用自动语音评分技术提供非母语语音反馈的商业语言学习应用<ul><li><a href="https://elsaspeak.com/home">ELSA</a></li><li><a href="www.liulishuo.com/en/">Liulishuo</a></li><li><a href="www.duolingo.com">Duolingo</a></li><li><a href="www.speakingpal.com">SpeakingPal</a></li><li><a href="www.carnegiespeech.com">Carnegie Speech</a></li><li><a href="www.rosettastone.com">Rosetta Stone</a></li></ul></li><li><a href="https://www.ets.org/pdfs/toefl/toefl-ibt-speaking-rubrics.pdf">TOEFLiBT口语评分标准</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>易混淆发音</title>
      <link href="/blog/yin-pin/yu-yin-xue/yi-hun-yao-fa-yin/"/>
      <url>/blog/yin-pin/yu-yin-xue/yi-hun-yao-fa-yin/</url>
      
        <content type="html"><![CDATA[<ul><li><p>n-ŋ</p><ul><li>sin sing</li><li>thin thing</li><li>ran rang</li></ul></li><li><p>eɪ-iː</p><ul><li>fate feat</li><li>way we</li><li>say see</li></ul><p>区分eɪ和iː <a href="https://www.ximalaya.com/sound/69244789" class="uri">https://www.ximalaya.com/sound/69244789</a></p><p>区分eɪ和ɪ <a href="https://www.ximalaya.com/sound/69243511" class="uri">https://www.ximalaya.com/sound/69243511</a></p></li><li><p>v-w</p><ul><li>visit/view we</li><li>very well</li><li>valid wag</li><li>vast want</li></ul></li><li><p>h-f</p><ul><li>have five</li></ul></li><li><p>ʒ-ʃ</p><ul><li>vision version</li><li>pleasure fisher</li></ul></li><li><p>æ-aɪ</p><ul><li>have five</li></ul></li><li><p>l-r</p></li><li><p>l-n</p><ul><li>light night</li><li>let net</li><li>low no</li><li>like narrow</li><li>laugh nasty</li></ul></li><li><p>s-θ</p><ul><li>sin thin</li><li>sink think</li><li>sank thank</li><li>mouse mouth</li><li>pass path</li></ul></li><li><p>ð-z</p><ul><li>that zero</li></ul></li><li><p>æ-e</p><ul><li>bag beg</li><li>mat met</li><li>sad said</li><li>bad bed</li><li>land lend London</li><li>sand send</li><li>pack peck</li><li>pan pen</li></ul></li><li><p>ɪ-iː</p><ul><li>ship sheep</li><li>sin seen</li><li>fill feel</li><li>hill heel</li><li>sick seek</li><li>pitch peach</li><li>slip sleep</li><li>grin green</li></ul><p>区分：<a href="https://www.bilibili.com/video/av498505663/" class="uri">https://www.bilibili.com/video/av498505663/</a></p></li><li><p>p-pʊ</p><ul><li>please</li></ul></li><li><p>ʃ-ʃɪ</p><ul><li>shrill</li></ul></li><li><p>k-kə</p><ul><li>clean</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>英语常见发音现象</title>
      <link href="/blog/yin-pin/yu-yin-xue/ying-yu-fa-yin-chang-jian-fa-yin-xian-xiang/"/>
      <url>/blog/yin-pin/yu-yin-xue/ying-yu-fa-yin-chang-jian-fa-yin-xian-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="送气---不送气浊化">1. 送气 -&gt; 不送气（浊化）</h1><p>重读音节中，/s/+清辅音/t/ /p/ /k//tr/+元音，清辅音发音为对应的浊辅音/d/ /b/ /g//dr/，如student、speak、sky、strict <br> <a href="https://www.yingyutu.com/aspirated">讲解及示例</a></p><h1 id="闪音">2. 闪音</h1><p>美式英语两个元音之间的/t/发为[ɾ]，注意不是/d/，如city、not at all<br> <a href="https://www.yingyutu.com/flap">讲解及示例</a></p><h1 id="无声除阻">3. 无声除阻</h1><p>爆破音是指发音器官在口腔中形成阻碍，然后气流冲破阻碍而发出的音，也被称为塞音。发这些音时有三个阶段：成阻（发音部位紧闭）、持阻（让气流积聚，形成压力）、除阻（除去气流的阻碍）。当一个塞音紧跟一个需成阻音时，这个塞音会变弱许多，不完全发出来。这是因为前者的除阻后跟后者的成阻，在快速语流中，这样不方便。所以前一个塞音并不释放气流，只做动作，时间上仍然保留原时间。</p><ul><li>爆破音（/p, b, t, d, k, ɡ/）+爆破音，如doctor, good boy</li><li>爆破音+破擦音（/tʃ, dʒ, tr, dr/），如picture, good job</li><li>爆破音+摩擦音（/f, v, s, z, θ, ð, ʃ, ʒ, r, h/），如eighth/eɪ(t)θ/、keep silent /kiː(p)/</li></ul><p><a href="https://www.yingyutu.com/unreleased">讲解及示例</a></p><h1 id="喉塞音">4. 喉塞音</h1><p>喉塞音是一种由声门关闭引起的气流瞬时中断而成的塞音。英式英语和美式英语中，/t/发为/ʔ/，如button</p><blockquote><p>The /t/ is pronounced as a glottal stop /ʔ/ (the sound in the middleof the word 'uh-oh') when it is between a vowel, /n/, or /r/ (includingall r-controlled vowels) and followed by an /n/(including a syllabic/n/), /m/, or non-syllabic /l/.</p></blockquote><p><a href="https://www.yingyutu.com/glottal">讲解及示例</a></p><h1 id="连读">5. 连读</h1><p>linking/r/：英音中，单词末尾的/r/一般不发音，但当其后的单词以元音开头时，/r/发音且与元音连读。如forever /fəˈrevə/，here are/hɪərə/</p><h1 id="同化">6. 同化</h1><p>连续语流，若发音较快、较随便，有些音受前后音的影响，变得与之相近或相同。同化后，发音步骤简化，更省力。</p><ul><li>融合同化<ul><li>/t/+/j/→/tʃ/，如Nice to meet you.</li><li>/d/+/j/→/dʒ/，如Did you do it?</li><li>/s/+/j/→/ʃ/, 如God bless you.</li><li>/z/+/j/→/ʒ/, 如As you wish.</li><li>/g/+/j/→/dʒ/，如I beg your pardon.</li></ul></li><li>前一个音受后一个音影响，发音部位变化<ul><li>/t/+/p, b, m/，/t/→/p/，如that boy /ðæp bɔɪ/</li><li>/t/+/k, g/，/t/→/k/，如that girl /ðæk ɡɜːl/</li><li>/d/+/p, b, m/，/d/→/b/，如good-bye</li><li>/d/+/k, g/，/d/→/g/，如good guy</li><li>/n/+/p, b, m/，/n/→/m/，如ten miles</li><li>/n/+/k, g/，/n/→/ŋ/，如in general</li><li>/nt/+/p, b, m/，/nt/→/mp/，如don't mind</li><li>/nt/+/k, g/，/nt/→/ŋk/，如don't go</li><li>/nd/+/p, b, m/，/nd/→/mb/，如second place</li><li>/nd/+/k, g/，/nd/→/ŋg/，如second class</li><li>/s/+/ʃ, j/，/s/→/ʃ/，如this ship, this year</li><li>/z/+/ʃ, j/，/z/→/ʒ/，如these years</li></ul></li><li>词末浊辅音+词首清辅音，词末浊辅音清化<ul><li>/z/→/s/，如has to, lose face</li><li>/v/→/f/，如of course, have to</li><li>/ð/→/θ/，如with pleasure</li></ul></li></ul><h1 id="加音">7. 加音</h1><p>前一单词以元音结尾，后一单词以元音开头时</p><ul><li>/ə/, /ɪə/, /ɑː/, /ɔː/ +元音，加/r/，如China and India, the idea of,pa and ma, law and order</li><li>/ɪ/, /iː/, /eɪ/, /aɪ/, /ɔɪ/ +元音，加/j/，如I am</li><li>/uː/, /ʊ/, /əʊ/, /aʊ/ +元音，加/w/，如who is</li></ul><h1 id="省音">8. 省音</h1><ul><li>单词内正确省音，如：<ul><li>similar /sɪmələ(r)/，different /'dɪf(ə)r(ə)nt/</li><li>成音节：单词最后一个音节为辅音+/ə/+/l/或/n/，如trouble/ˈtrʌb(ə)l/、season /ˈsiːz(ə)n/</li><li>/t(ə)ri/：单词末尾为tary、tery或tory时，如secretary/ˈsekrət(ə)ri/</li></ul></li><li>/t/、/d/前后为辅音（其后辅音非/h/）时省音（/d/后辅音为/w/, /d/,/r/或/s/时，不省音），如next day /neks deɪ/、you and me</li><li>h−dropping 常见于he, him, her, his, have, who，如get him, tellher</li><li>/v/后跟辅音，如a lot of time /ə lɒt ə taɪm/, give me that</li><li>词尾辅音+词首同一辅音，只读一次，如got to、want to、some more</li></ul><h1 id="缩读">9. 缩读</h1><p>如 I am=I'm, is, are, not, have, has, had, will, shall</p><h1 id="弱读">10. 弱读</h1><p>元音常弱化为/ə/，如</p><table><thead><tr><th>单词</th><th>强读式</th><th>弱读式</th><th>示例</th></tr></thead><tbody><tr><td>as</td><td>/æz/</td><td>/əz/</td><td></td></tr><tr><td>and</td><td>/ænd/</td><td>/ənd/ /ən/ /n/</td><td>you and/n/ me</td></tr></tbody></table><h1 id="重读">11. 重读</h1><ul><li>以下词类一般重读：名词、实义动词、形容词、副词等；</li><li>以下词类一般不重读：代词、冠词、介词、连词、情态动词、助动词等</li></ul><h1 id="升降调">12. 升降调</h1><ul><li><p>升调：一般和反义疑问句尾、选择疑问句==第一个选项后==</p><p>如：Is he tall ↗ or short?</p></li><li><p>降调：选择疑问句、特殊疑问句、祈使句、感叹句、陈述句 句尾</p><p>如：Is he tall or short? ↘</p><p>Don't open the window. ↘</p></li><li><p>长句语调：并列的词，and前升，and后降</p><p>如：I've decided to stay away from fried food ↗ and soft drinks.↘</p></li></ul><h1 id="参考资料">13. 参考资料</h1><ul><li>同化、加音、弱读 王式仁. 英语标准发音教程[M]. 高等教育出版社,2012</li><li>浊化、失去爆破、缩读 刘金龙, 高莉敏.《零基础从头开始学音标》[M].华东理工大学, 2018</li><li>省音 <a href="%3Chttps://mandradey.wixsite.com/connectedspeech">《Features ofconnected speech》</a></li><li><a href="https://tristone13th.netlify.app/2021/02/09/英语语音技巧.html" class="uri">https://tristone13th.netlify.app/2021/02/09/英语语音技巧.html</a></li><li>重读、升降调 <a href="https://mp.weixin.qq.com/s?__biz=MzIyNDI3MTIxNg==&amp;mid=2651145729&amp;idx=1&amp;sn=daf3381b9b3f9af2c8f95e33f2978527&amp;chksm=f3e0681ec497e1087ac8809d96df38293203ef0ed91552da594e67a1aed85b4e7771ed37cb35&amp;token=341174366&amp;lang=zh_CN#rd">讯飞易听说广东省梅州市高考英语听说考试解读及备考策略分享</a></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeNet代码</title>
      <link href="/blog/yin-pin/gong-ju-bao/wenet/dai-ma/"/>
      <url>/blog/yin-pin/gong-ju-bao/wenet/dai-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="数据预处理">1. 数据预处理</h1><ul><li>compute_cmvn_stats：生成global_cmvn<ul><li>mean_stat：训练集上各维特征的总和</li><li>var_stat：训练集上各维特征的平方和</li><li>frame_num：训练集总帧数</li></ul></li></ul><h1 id="iterabledataset---processor">2. IterableDataset - Processor</h1><ul><li>根据输入list生成IterableDataset<ul><li>数据类型<ul><li>raw：json列表，存储key、wav(、start、end)、txt</li><li>shard：压缩包列表。随机：压缩包间随机，压缩包内部顺序遍历</li></ul></li><li>训练集按rank、worker_id划分数据；CV集各卡均用全集</li></ul></li><li>tokenize<ul><li><font color="green">non_lang_syms：用[……]、&lt;……&gt;、{……}分割文本（如'12[12]34&lt;34&gt;56{56}{78}'分割为['12','[12]', '34', '&lt;34&gt;', '56', '{56}', '', '{78}','']），转大写。若token在non_lang_syms中，直接使用；否则BPE/按空格分割/按字符分割</font></li><li>BPE <pre class="line-numbers language-none"><code class="language-none">import sentencepiecesp &#x3D; sentencepiece.SentencePieceProcessor()sp.load(bpe_model)sp.encode_as_pieces(word)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>symbol_table：token转id</li></ul></li><li>filter：过滤过短或过长的样本<ul><li>max_length(10240)、min_length(10)：帧数，帧移10ms</li><li>token_max_length(200)、token_min_length(1)</li><li>min_output_input_ratio(0.0005)、max_output_input_ratio(1)</li></ul></li><li>resample</li><li>speed_perturb：各样本随机变速[0.9, 1.0, 1.1]。默认不使用</li><li><font color="green">提取fbank/mfcc</font>：<font color="red">音频要求16bit</font><pre class="line-numbers language-none"><code class="language-none">torchaudio.compliance.kaldi as kaldifeat &#x3D; kaldi.fbank(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, energy_floor&#x3D;0.0, sample_frequency&#x3D;sample_rate)feat &#x3D; kaldi.mfcc(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, num_ceps&#x3D;num_ceps, high_freq&#x3D;high_freq, low_freq&#x3D;low_freq, sample_frequency&#x3D;sample_rate)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li>spec_aug：频谱mask，num_t_mask、num_f_mask、max_t、max_f，max_w未使用。默认使用</li><li>spec_sub：频谱替换，num_t_sub、max_t，随机替换为该时刻前的特征。默认不使用</li><li>shuffle：<font color="red">局部shuffle，shuffle_size默认10000</font>。默认使用</li><li>sort：局部按帧数排序，便于帧数相近的样本放到同一batch，sort_size=500。默认使用</li><li>batch<ul><li>静态batch：除最后一个batch，batch size固定</li><li>动态batch：按max_frames_in_batch（含padding）拼batch</li></ul></li><li>padding：batch内样本按帧数降序排列，特征补0，label补-1</li><li>torchaudio <pre class="line-numbers language-none"><code class="language-none">torchaudio.backend.sox_io_backend.info(wav_file).sample_ratetorchaudio.load(wav_file)torchaudio.backend.sox_io_backend.load(filepath&#x3D;wav_file, num_frames&#x3D;end_frame - start_frame, frame_offset&#x3D;start_frame)torchaudio.transforms.Resample(orig_freq&#x3D;sample_rate, new_freq&#x3D;resample_rate)(waveform)wav, _ &#x3D; torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, [[&#39;speed&#39;, str(speed)], [&#39;rate&#39;, str(sample_rate)]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="模型结构">3. 模型结构</h1><h2 id="encoder">3.1. encoder</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    输入("输入：x: [B, T, D], <br> mask_pad: [B, 1, T]，有效帧对应1，拼batch时补的帧对应0")    输入 --> CMVN    CMVN --> 下采样    subgraph 下采样        不下采样        4倍下采样        6倍下采样        8倍下采样        subgraph 不下采样            direction TB            Linear1("Linear(input_size, output_size)")            Linear1 --> LayerNorm1("LayerNorm") --> Dropout1("Dropout")        end        subgraph 4倍下采样            direction TB            Conv2d1("Conv2d(1, output_size, 3, 2)")            Conv2d2("Conv2d(output_size, output_size, 3, 2)")            Linear2("Linear(output_size * (((input_size - 1) // 2 - 1) // 2), output_size)")            mask_pad1("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2]")            Conv2d1 --> ReLU1(ReLU) --> Conv2d2 --> ReLU2(ReLU) --> Linear2        end        subgraph 6倍下采样            direction TB            Conv2d3("Conv2d(1, output_size, 3, 2)")            Conv2d4("Conv2d(output_size, output_size, 5, 3)")            Linear3("Linear( , output_size)")            mask_pad2("mask_pad = mask_pad[:, :, :-2:2][:, :, :-4:3]")            Conv2d3 --> ReLU3(ReLU) --> Conv2d4 --> ReLU4(ReLU) --> Linear3        end        subgraph 8倍下采样            direction TB            Conv2d5("Conv2d(1, output_size, 3, 2)")            Conv2d6("Conv2d(output_size, output_size, 3, 2)")            Conv2d7("Conv2d(output_size, output_size, 3, 2)")            Linear4("Linear( , output_size)")            mask_pad3("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2][:, :, :-2:2]")            Conv2d5 --> ReLU5(ReLU) --> Conv2d6 --> ReLU6(ReLU) --> Conv2d7 --> ReLU7(ReLU) --> Linear4        end    end    下采样 --> 位置embedding    subgraph 位置embedding        pos_emd("pos_emb(pos, 2i) = sin(pos/(10000^(2i/dim))) <br> pos_emb(pos, 2i+1) = cos(pos/(10000^(2i/dim))) <br> 拼batch计算时，offset小于0的位置采用pos_emb[0]"):::font_green        位置embedding1("x = x * math.sqrt(dim) + pos_emb"):::font_green        相对位置embedding("x *= math.sqrt(dim)"):::font_green        无位置embedding("x不变，pos_emb全0")        Dropout2("dropout(x), dropout(pos_emb)")        tmp1(" ") --> pos_emd        pos_emd -- 位置embedding --> 位置embedding1 --> Dropout2        pos_emd -- 相对位置embedding --> 相对位置embedding --> Dropout2        tmp1 -- 无位置embedding --> 无位置embedding --> Dropout2    end    位置embedding --> 计算chunk_mask    计算chunk_mask --> transformer*num_blocks -->tmp2    计算chunk_mask --> conformer*num_blocks -->tmp2    tmp2(" ") -- normalize_before -->LayerNorm2("LayerNorm")    subgraph 计算chunk_mask        use_dynamic_chunk{"use_dynamic_chunk"} -- Yes --> decoding_chunk_size{"decoding_chunk_size"}        decoding_chunk_size -- 小于0 --> tmp18("不分chunk，历史、未来全部可见")        decoding_chunk_size -- 大于0 --> 采用固定的chunk_size --> num_decoding_left_chunks{"num_decoding_left_chunks"}        decoding_chunk_size -- 等于0 --> tmp17("随机动态chunk_size，帧数取值范围为[1, 25]或全部可见（概率50%），支持use_dynamic_left_chunk") --> num_decoding_left_chunks        use_dynamic_chunk -- No --> static_chunk_size{"static_chunk_size"}        static_chunk_size -- 小于等于0 --> tmp18        static_chunk_size -- 大于0 --> 采用固定的chunk_size        num_decoding_left_chunks -- 小于0 --> tmp19("历史全部可见，当前chunk可见，未来chunk不可见")        num_decoding_left_chunks -- 大于等于0 --> 仅可见指定的历史chunk数    end    subgraph transformer*num_blocks        subgraph transformer_attention            tmp3(" ") --> normalize_before1{"normalize_before"}            normalize_before1 -- Yes --> LayerNorm3("LayerNorm") --> MultiHeadedAttention1("MultiHeadedAttention"):::font_red            normalize_before1 -- No --> MultiHeadedAttention1            MultiHeadedAttention1 --> concat_after1{"concat_after"}            concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp4(" ")            concat_after1 -- No --> Dropout3("Dropout") --> tmp4            tmp3 -- + --> tmp4            tmp4 --> normalize_before2{"normalize_before"}            normalize_before2 -- Yes --> tmp5(" ")            normalize_before2 -- No --> LayerNorm4("LayerNorm") --> tmp5        end        subgraph transformer_FeedForward            tmp5 --> normalize_before3{"normalize_before"}            normalize_before3 -- Yes --> LayerNorm5("LayerNorm") --> PositionwiseFeedForward1("PositionwiseFeedForward: <br> w2(dropout(activation(w1*x+b1)))+b2 <br> w1:[input, hidden] w2:[hidden, input]"):::font_red            normalize_before3 -- No --> PositionwiseFeedForward1            PositionwiseFeedForward1 --> Dropout4("Dropout")            tmp5 -- + --> Dropout4            Dropout4 --> normalize_before4{"normalize_before"}            normalize_before4 -- Yes --> tmp6(" ")            normalize_before4 -- No --> LayerNorm6("LayerNorm") --> tmp6        end    end    subgraph conformer*num_blocks        subgraph conformer_macaron            tmp7(" ") --> macaron{"macaron"}            macaron -- Yes --> normalize_before5{"normalize_before"}            macaron -- No --> tmp8(" ")            normalize_before5 -- Yes --> LayerNorm7("LayerNorm") --> PositionwiseFeedForward2("PositionwiseFeedForward"):::font_red            normalize_before5 -- No --> PositionwiseFeedForward2            PositionwiseFeedForward2 --> Dropout5("Dropout") --> scale("*0.5")            tmp7 -- + --> scale            scale --> normalize_before6{"normalize_before"}            normalize_before6 -- Yes --> tmp8            normalize_before6 -- No --> LayerNorm8("LayerNorm") --> tmp8        end        subgraph conformer_attention            tmp8 --> normalize_before7{"normalize_before"}            normalize_before7 -- Yes --> LayerNorm9("LayerNorm") --> tmp9(" ")            normalize_before7 -- No --> tmp9            subgraph Attention                direction TB                计算qkv("q、k、v均采用Linear计算 <br> 更新att_cache: 历史[k, v]，(1, head, chunk_size * num_decoding_left_chunks + T, d_k * 2)"):::font_red                计算attention系数("计算attention系数：采用了chunk_mask <br> Dropout"):::font_red                attention("scaled dot product self-attention")                Linear5("Linear(output_size, output_size)")                Rel("pos_bias_u、pos_bias_v: [head, d_k]，xavier_uniform_初始化，可训练 <br> softmax{[(q + pos_bias_u)k + (q + pos_bias_v)(W*pos_emb)]/math.sqrt(d_k)}v <br> rel_shift: 未采用"):::font_red                tmp9 --> 计算qkv -- MultiHeadedAttention --> 计算attention系数 --> attention --> Linear5                计算qkv -- RelPositionMultiHeadedAttention --> Rel --> Linear5            end            Linear5 --> concat_after3{"concat_after"}            concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp10(" ")            concat_after3 -- No --> Dropout6("Dropout") --> tmp10            tmp8 -- + --> tmp10            tmp10 --> normalize_before8{"normalize_before"}            normalize_before8 -- Yes --> tmp11(" ")            normalize_before8 -- No --> LayerNorm10("LayerNorm") --> tmp11        end        subgraph conformer_conv            tmp11 --> conv{"conv"}            conv -- Yes --> normalize_before9{"normalize_before"}            normalize_before9 -- Yes --> LayerNorm11("LayerNorm") --> tmp12(" ")            normalize_before9 -- No --> tmp12            tmp12 --> mask_pad4("mask_pad的帧均置0"):::font_red --> causal{"causal"}            causal -- Yes -->            padding("第一次计算时左侧填充(kernel_size-1)帧0，<br> 后续从cache中取缓存的历史有效输入"):::font_red --> pointwise_conv1            causal -- No --> pointwise_conv1            pointwise_conv1("pointwise_conv: 1D卷积，channel扩充为2倍"):::font_red --> GLU("GLU，channel减半"):::font_red --> depthwise_conv            click GLU "https://pytorch.org/docs/stable/generated/torch.nn.functional.glu.html"            depthwise_conv("nn.Conv1d(channels, channels, kernel_size, stride=1, padding=padding, groups=channels, bias=bias) <br> 若非causal，左右填充(kernel_size - 1) // 2帧0"):::font_red            depthwise_conv --> BatchNorm1d/LayerNorm --> activation("activation，如SiLU/swish")            click activation "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"            activation --> pointwise_conv2("pointwise_conv: 1D卷积"):::font_red --> mask_pad5("mask_pad的帧均置0"):::font_red -->Dropout7("Dropout")            tmp11 -- + --> Dropout7            Dropout7 --> normalize_before10{"normalize_before"}            normalize_before10 -- Yes --> tmp13(" ")            normalize_before10 -- No --> LayerNorm12("LayerNorm") --> tmp13            conv -- No --> tmp13        end        subgraph conformer_FeedForward            tmp13 --> normalize_before11{"normalize_before"}            normalize_before11 -- Yes --> LayerNorm13("LayerNorm") --> PositionwiseFeedForward3("PositionwiseFeedForward"):::font_red            normalize_before11 -- No --> PositionwiseFeedForward3            PositionwiseFeedForward3 --> Dropout8("Dropout") --> macaron2{"macaron"}            macaron2 -- Yes --> scale2(*0.5) --> tmp14(" ")            macaron2 -- No --> tmp14(" ")            tmp13 -- + --> tmp14            tmp14 --> normalize_before12{"normalize_before"}            normalize_before12 -- Yes --> tmp15(" ")            normalize_before12 -- No --> LayerNorm14("LayerNorm") --> tmp15            tmp15--> conv2{"conv"}            conv2 -- Yes --> LayerNorm15("LayerNorm") --> tmp16(" ")            conv2 -- No --> tmp16(" ")        end    end</div><ul><li>load_cmvn：计算均值、方差倒数。支持json格式、kaldi文本格式输入</li><li>下采样：2D卷积下采样至1/4或1/6或1/8，感受野分别为7、11、15帧（延迟分别为6、10、14)<br><font color="green">存在的问题：x_mask。如下采样至1/4时，取<code>x_mask[:, :, :-2:2][:, :, :-2:2]</code>，第1个点中心时刻为第3帧，感受野为0-6帧，第一个点的mask应取第6帧</font></li><li>forward_chunk：输入(chunk_size - 1) * subsample_rate +subsample.right_context +1帧有效数据，输出chunk_size帧，并更新att_cache、cnn_cache</li><li>forward_chunk_by_chunk：输入有效帧，模拟逐chunk流式计算</li></ul><h2 id="decoder">3.2. decoder</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    subgraph TransformerDecoder        tmp(" ") -- train --> 输入1("输入label、label长度") --> tgt_mask("mask padding、未来输出") --> embedding --> +位置embedding --> transformer*num_blocks        tmp -- decode --> 输入2("自回归：输入上一时刻解码结果") --> embedding        transformer*num_blocks --> normalize_before1{"normalize_before"}        normalize_before1 -- Yes --> LayerNorm1("LayerNorm") --> tmp1(" ")        normalize_before1 -- No --> tmp1        tmp1 --> output{"use_output_layer"}        output -- Yes --> Linear1("Linear(attention_dim, vocab_size)") --> train_decode{"train/decode"}        train_decode -- train --> tmp2(" ")        train_decode -- decode --> log_softmax --> tmp2        output -- No --> tmp2        subgraph transformer*num_blocks            direction TB            subgraph self_attention                direction TB                tmp4(" ") --> normalize_before2{"normalize_before"}                normalize_before2 -- Yes --> LayerNorm2("LayerNorm") --> tmp5(" ")                normalize_before2 -- No --> tmp5                tmp5 --> MultiHeadedAttention1("MultiHeadedAttention: q: 上一时刻输出，k/v: 历史输出，mask未来输出"):::font_red                MultiHeadedAttention1 --> concat_after1{"concat_after"}                concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp6(" ")                concat_after1 -- No --> Dropout3("Dropout") --> tmp6                tmp4 -- + --> tmp6                tmp6 --> normalize_before3{"normalize_before"}                normalize_before3 -- Yes --> tmp7(" ")                normalize_before3 -- No --> LayerNorm3("LayerNorm") --> tmp7            end            subgraph inter_attention                direction TB                tmp7 --> normalize_before4{"normalize_before"}                normalize_before4 -- Yes --> LayerNorm4("LayerNorm") --> tmp8(" ")                normalize_before4 -- No --> tmp8                tmp8 --> MultiHeadedAttention2("MultiHeadedAttention: q: decoder隐层表示，k/v: encoder输出，mask未来输出"):::font_red                MultiHeadedAttention2 --> concat_after3{"concat_after"}                concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp9(" ")                concat_after3 -- No --> Dropout4("Dropout") --> tmp9                tmp7 -- + --> tmp9                tmp9 --> normalize_before5{"normalize_before"}                normalize_before5 -- Yes --> tmp10(" ")                normalize_before5 -- No --> LayerNorm5("LayerNorm") --> tmp10            end            subgraph FeedForward                tmp10 --> normalize_before6{"normalize_before"}                normalize_before6 -- Yes --> LayerNorm6("LayerNorm") --> PositionwiseFeedForward:::font_red                normalize_before6 -- No --> PositionwiseFeedForward                PositionwiseFeedForward --> Dropout5("Dropout")                tmp10 -- + --> Dropout5                Dropout5 --> normalize_before7{"normalize_before"}                normalize_before7 -- Yes --> tmp11(" ")                normalize_before7 -- No --> LayerNorm7("LayerNorm") --> tmp11            end            decode("decode：缓存每一时刻decoder各block的输出；输入tgt递增存储每一时刻的输出")        end    end    subgraph BiTransformerDecoder        tmp3(" ") -- train/rescore --> 输出left_decoder与right_decoder结果:::font_red        tmp3 -- decode --> 采用left_decoder:::font_red    end</div><h2 id="transducer">3.3. Transducer</h2><div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    input("输入上一时刻预测值") --> Embedding1("Embedding(vocab_size, embed_size)") --> Dropout1("Dropout")    Dropout1 -- RNNPredictor --> RNNPredictor    Dropout1 -- EmbeddingPredictor --> EmbeddingPredictor    Dropout1 -- ConvPredictor --> ConvPredictor    subgraph RNNPredictor        RNN("RNN/LSTM/GRU，记录历史h、c，初始为全0"):::font_red --> linear1("Linear(hidden_size, output_size)") --> tmp1    end    subgraph EmbeddingPredictor        direction TB        input1("拼接定长的历史输入 <br> input(B,L,1,C,E)，B: batch size, L: seq_len, C: context_size, E: embed_size"):::font_red --> attention:::font_red        subgraph attention            input1 --> dot            W("W(H,C,E)，H: num_heads") --> dot            dot("broadcast dot product, (B,L,H,C,E)"):::font_red --> sum1("sum, (B,L,H,C)") --> unsqueeze("unsqueeze, (B,L,H,1,C)") --> matmul            input1 --> matmul("matmul，(B,L,H,1,E)") --> sum2("squeeze, sum，(B,L,E)") --> scale("scale，/(H*C)")            click dot "https://arxiv.org/pdf/2109.07513.pdf"        end        scale --> proj("Linear(embed_size, embed_size)") --> LayerNorm --> swish --> tmp1    end    subgraph ConvPredictor        input2("拼接定长的历史输入，初始输入blank") --> conv --> LayerNorm2("LayerNorm")--> activation --> tmp1    end    subgraph join        input3("输入：encoder_output") --> prejoin_linear1{"prejoin_linear"}        tmp1("predictor output") --> prejoin_linear2{"prejoin_linear"}        prejoin_linear1 -- Yes --> prejoin_linear3("Linear(enc_output_size, join_dim)") --> tmp2(" ")        prejoin_linear1 -- No --> tmp2        prejoin_linear2 -- Yes --> prejoin_linear4("Linear(pred_output_size, join_dim)") --> tmp3(" ")        prejoin_linear2 -- No --> tmp3        tmp2 --> tmp4("+"):::font_red        tmp3 --> tmp4        tmp4 --> postjoin_linear1{"postjoin_linear"}        postjoin_linear1 -- Yes --> postjoin_linear2("Linear(join_dim, join_dim)") --> tmp5(" ")        postjoin_linear1 -- No --> tmp5        tmp5 --> activation2("activation") --> linear2("Linear(join_dim, vocab_size)")    end</div><h2 id="ctc-attentiontransducer">3.4. CTC-Attention/Transducer</h2><ul><li>CTC：输入encoder_output --&gt; Linear("Linear(, vocab_size)")</li><li>attention：KLDivLoss + label smoothing</li></ul><p><span class="math display">\[ L_{\mathrm{ATT}}(x, y)=(1-\alpha)L_{\mathrm{ATT-L2R}}(x, y)+\alpha {L}_{\mathrm{ATT-R2L}}(x, y)\]</span></p><p><span class="math display">\[ L_{\mathrm{CTC-ATT}}(x, y)=\lambdaL_{\mathrm{CTC}}(x, y)+(1-\lambda) {L}_{\mathrm{ATT}}(x, y)\]</span></p><p><span class="math display">\[ L_{\mathrm{CTC-ATT-RNNT}}(x, y)=\gammaL_{\mathrm{RNNT}}(x, y)+\beta L_{\mathrm{ATT}}(x, y)+\lambdaL_{\mathrm{CTC}}(x, y) \]</span></p><h2 id="解码">3.5. 解码</h2><p>目前支持四种解码算法： + CTC greedy beamsearch，帧级别输出，解码过程不合并前缀，最终n-best上进行ctc序列处理。 +CTC prefix beam search：帧级别解码，合并相同的ctc序列前缀。 + Attentiondecoder beam search：基于cross-attention的label级别解码。 + CTC +attention rescoring：第一遍采用CTC prefix beamsearch，该结果可作为流式结果实时返回。将CTCdecoder的n-best结果，通过attention decoder进行teacher forcingrescoring（指定各时刻输出，仅提取分数），根据得分重新排序。</p><pre><code>score = 反向attention decoder分 * w_r + 正向attention decoder分 * (1-w_r) + CTC分 * w_ctc</code></pre><h3 id="transducer解码">3.5.1. Transducer解码</h3><ul><li><p>greedy search <div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    输入一帧encoder_out --> nblk{"上一输出是否为blank"}    nblk -- Yes --> predictor.forward_step --> decode    nblk -- No --> 采用之前计算的pred_out --> decode    decode --> blank{"是否为blank"} -- No --> 输出 --> per_frame_noblk{"单帧encoder_out解码出的非blank token>=per_frame_noblk"}:::font_red    per_frame_noblk -- No --> predictor.forward_step    per_frame_noblk -- Yes --> 输入一帧encoder_out    blank -- Yes --> 输入一帧encoder_out</div></p></li><li><p>prefix beam search <div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    RNNT("RNNT: 仅支持一帧encoder_out输出一个token"):::font_red --> shallow_fusion("后验概率加权求和")    CTC --> shallow_fusion --> topk("取topk，得到N * N个候选") --> prefix("解码结果（不含blank）相同的合并后验概率") --> 取topk</div></p></li><li><p>RNNT + attention rescoring <div class="mermaid">  %%{init: {'themeVariables': {'fontSize': '24px'}}}%%flowchart TD    classDef font_red color:red;    classDef font_green color:green;    decode1("CTC/RNNT prefix beam search") --> torchaudio.functional.rnnt_loss:::font_green    decode1 --> attention_rescore    decode1 -- w_c --> score加权求和    torchaudio.functional.rnnt_loss -- w_t --> score加权求和    attention_rescore -- w_a --> score加权求和</div></p></li></ul><h2 id="模型训练">3.6. 模型训练</h2><ul><li><p>LRScheduler</p><p><span class="math display">\[l_{r}=\begin{cases}    l_{r} * \frac{\text { step }}{\text { warmup_step }} &amp; , \text {step } \leqslant \text { warmup_step } \\    l_{r} * \sqrt{\frac{\text { warmup_step }}{\text { step }}} &amp; ,\text { step }&gt;\text { warmup_step}\end{cases}\]</span></p></li><li><p>fp16梯度同步 <pre class="line-numbers language-none"><code class="language-none">model.cuda()model &#x3D; torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters&#x3D;True)from torch.distributed.algorithms.ddp_comm_hooks import (default as comm_hooks,)model.register_comm_hook(state&#x3D;None, hook&#x3D;comm_hooks.fp16_compress_hook)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p></li><li><p><font color="green"><a href="https://pytorch.org/docs/stable/notes/amp_examples.html">混合精度训练</a></font></p></li><li><p>梯度累积 <pre class="line-numbers language-none"><code class="language-none">from contextlib import nullcontextif is_distributed and batch_idx % accum_grad !&#x3D; 0:    context &#x3D; model.no_syncelse:    context &#x3D; nullcontextwith context():    loss_dict &#x3D; model(feats, feats_lengths, target, target_lengths)    loss &#x3D; loss_dict[&#39;loss&#39;] &#x2F; accum_grad    loss.backward()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p></li><li><p>梯度裁剪 <pre class="line-numbers language-none"><code class="language-none">from torch.nn.utils import clip_grad_norm_if use_amp:    scaler.unscale_(optimizer)    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)    # We don&#39;t check grad here since that if the gradient    # has inf&#x2F;nan values, scaler.step will skip optimizer.step().    scaler.step(optimizer)    scaler.update()else:    grad_norm &#x3D; clip_grad_norm_(model.parameters(), clip)    if torch.isfinite(grad_norm):        optimizer.step()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p></li><li><p>average model：取cv_loss最小或最后的n个epoch的模型，取参数均值</p></li></ul><h2 id="参考资源">3.7. 参考资源</h2><ul><li><font color="green">相对positionembedding：https://arxiv.org/abs/1901.02860 </font></li><li><font color="green">Embeddingpredictor：https://arxiv.org/pdf/2109.07513.pdf </font></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>多维度、多粒度语音评分</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/</url>
      
        <content type="html"><![CDATA[<h1 id="cambridge-phd-thesis">1. 2022 Cambridge PhD Thesis</h1><ul><li>发表信息：剑桥大学博士论文</li><li><font color="red">创新点：由于大量、可靠的多维度人工评分较难获取，采用总分标注数据，通过限定输入特征进行端到端多维度评分。相较于特征提取-评分两阶段模型，端到端模型能更好地拟合人工分，且对不同的数据集、不同的任务，泛化性能更好</font></li><li><font color="red">存在的问题：需要强制对齐信息；仅自制数据集上的评分效果；由于没有维度分人工标注，采用总分统计维度分评分效果</font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><h3 id="发音评分">1.1.1. 发音评分</h3><ul><li>采用音素距离特征，与发音人属性（音色、性别等）不相关</li><li>传统方案：采用单高斯模型建模各音素的发音，计算各音素模型间的对称KL散度，拼接为<span class="math inline">\(\frac{1}{2} p\left ( p-1 \right)\)</span>维的向量，并取<span class="math inline">\(\log\left ( d+1\right)\)</span>。对于短语音，包含缺失音素的KL散度设为-1。输入一层全连接进行评分。<ul><li>缺点：每个说话者需要大量数据训练高斯模型；可能丢弃发音相关的信息；未考虑音素的发音过程、同一音素在不同上下文中发音可能不同、音素对于评分的重要性取决于上下文；存在错误的识别、强制对齐</li></ul></li><li>端到端方案：将各音素片段的帧序列编码为固定长度的embedding，对应同一音素的音素片段采用attention（学习忽略对齐错误的音素片段、关注发音错误的片段）加权平均得到音素embedding，计算音素embedding间的欧式距离。输入一层全连接进行评分。<ul><li>各模块初始化<ul><li>双胞胎双向RNN：+sigmoid，判断两音素是否一致，预训练</li><li>评分全连接层：采用基线模型初始化</li></ul></li><li>端到端fine-tuning</li><li>为了模型收敛，损失函数需要加attention权重熵的惩罚项，<span class="math inline">\(C \left( \lambda, S_{train} \right)=MSE_{train}-\beta \sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \alpha_{n m} \log \alpha_{nm}\)</span>，其中<span class="math inline">\(M\)</span>为音素总数，<span class="math inline">\(N_m\)</span> 为该音素的音素片段数</li><li><font color="green">存在的问题：音素片段数目不确定，attention如何实现？</font></li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7176bacfcf429dccb31d01b15693550f?method=download&shareKey=ec352fbca4d4bd034b6e439a23d35347" width="709px"></p><h3 id="韵律评分">1.1.2. 韵律评分</h3><ul><li>英语的重音等时性</li><li>传统方案<ul><li>评分特征<ul><li>相邻元音音程时长的平均差 $ rPVI_V=_{k=1}<sup>{K_{V}-1}|d(<em>{k}^{(V)})-d(</em>{k+1}</sup>{(V)})| $，其中，<span class="math inline">\(d\left(\tau_{k}\right)\)</span>为第k个元音音程的时长，<span class="math inline">\(K_V\)</span>为元音音程总数</li><li>相邻非元音音程时长的平均差 <span class="math inline">\(rPVI_C\)</span></li><li>$ CCI_V= <em>{k=1}^{K</em>{V}-1}|-| $ ，其中 <span class="math inline">\(l_{k}\)</span> 为第k个元音音程中的元音个数，<span class="math inline">\(\frac{d\left(\tau_{k}\right)}{l_{k}}\)</span>表示第k个元音音程中各元音的时长均值</li><li><span class="math inline">\(\mathrm{CCI}_{C}\)</span></li></ul></li><li>音程示例<img src="https://note.youdao.com/yws/api/personal/file/WEB62a1368e558eb842e642101204135967?method=download&shareKey=e218a8dfbcec2c97cdde02a678016c95" width="710px"></li></ul></li><li>端到端方案<ul><li>采用attention学习音程中不同子段的重要性（学习忽略对齐错误导致的时长异常的子段），与该音程时长拼接$ x_k=$</li><li>采用序列模型（BLSTM或transformer）学习元音音程特征序列 <span class="math inline">\(x_{1: K_{V}}^{(V)}\)</span>、非元音音程特征序列。</li><li>分别在元音音程深层特征序列<span class="math inline">\(h_{1:K_{V}}^{(V)}\)</span>、非元音音程深层特征序列上加attention。</li><li>拼接 <span class="math inline">\(\tilde{\boldsymbol{h}} =\left[\tilde{\boldsymbol{h}}^{(V)},\tilde{\boldsymbol{h}}^{(C)}\right]\)</span>，采用1层全连接评分。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB1662f02ff97ca28c83a9ac353cebcb29?method=download&shareKey=20178e325c8829035111c33474cf80cf" width="711px"></li></ul><h3 id="语调评分">1.1.3. 语调评分</h3><ul><li>重读单词基频高</li><li>传统方案<ul><li>方案一：浊音区域的基频的均值、中位数、最大值、1/4分位数、3/4分位数，采用1层全连接评分</li><li>方案二：考虑清音音素和静音，对于各音素，分别采用基频、浊音概率计算上述统计值并拼接，采用sequence-to-vector模型评分</li><li>方案三：采用最小二乘cosine拟合基频包络（DCT），清音区域插值，提取相应的系数向量采用DNN评分</li></ul></li><li>端到端方案<ul><li>输入：基频、浊音概率序列、position embedding。multi-headsequence-to-vector attention</li><li>考虑到长音频不适合用帧序列特征，采用sequence-to-vector模型学习各音素的特征表示，再采用sequence-to-vector模型预测分数</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBab93c416e54a88463f04bac213c506cf?method=download&shareKey=61fe567f298dc2bbc0a50a246d336ac2" width="500px"></p><h3 id="文本评分">1.1.4. 文本评分</h3><p>采用BERT提取word embedding，采用LSTM with attention评分[223]</p><h3 id="系统结构-1">1.1.5. 系统结构</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb09fd6a33bd460fc514eb7c6d99d211a?method=download&shareKey=b60236ca8e8979ca0069a830a9f487d2" width="706px"></p><h2 id="总分">1.2. 总分</h2><ul><li>方案一：各维度分均值</li><li>方案二：拼接各维度分打分器倒数第二层的输入表示，输入全连接网络评总分</li><li>方案三：各维度分的加权和：采用维度分打分器的中间表示、全连接网络计算attention系数</li></ul><h2 id="评价">1.3. 评价</h2><ul><li>数据集英语水平测试数据，测试含简答、读8句话、根据提示自由表述，对每个说话人评总分0-6。</li><li>基线：Gaussian Process评分器[154]</li><li>实验结果<ul><li><font color="red">*由于维度分没有人工标注，采用总分进行近似</font></li><li>*由于模型对随机初始化较敏感，统计5次训练的模型的均值、标准差</li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>发音</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3913cedf044c88366820bbf5c4d5c58e?method=download&shareKey=84242511a73adfa55e575ec8c8744835"></td><td>输入特征：MFCC vs PLP，性能差异较小；<br>音素片段的sequence-to-vector模型，BLSTM + 最后一层输出additiveattention，性能最好；<br> 在训练集-测试集匹配/不匹配的配置下，端到端模型性能均最好，一方面端到端模型可以学习更有表征能力的特征，另一方面泛化能力更好；<br>观察人工分-机器分散点图，端到端模型存在低分打高，但对特定人工分，机器分分布更集中；<br>tunability：分别采用音素距离KL散度、采用说话人分类任务训练得到的x-vector、采用评分任务训练得到的deep音素距离特征，后者在L1分类任务上性能最好</td></tr><tr><td>韵律</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfc777e01dd42196937bed56940b389d7?method=download&shareKey=df673e504d499f435ecd21407b97e996"></td><td>sequence-to-vector模型：BLSTM + 最后一层输出additive attention；<br>存在明显的低分打高</td></tr><tr><td>语调</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3270f41bcc6d831669450c9f25110614?method=download&shareKey=ce5ceff13afb5b18939ea898aff4de27"></td><td>相较于multi-head attention，attention LSTM评分效果更好；<br>不存在明显的打分偏移，基于基频统计特征的DNN存在明显的低分打高</td></tr><tr><td>发音分-音素距离-音素片段attention分布</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB34df75f218c23024b00dc4e3f3b088f3?method=download&shareKey=02a7f78a56973a438be39d7228dfea90"></td><td>大多数情况下，attention权重接近均匀分布，仅排除少量异常值；<br>余下的大部分只关注一个或少量具有代表性的音素片段；<br>剩下的取中间值的较少，可能是随机现象，也可能是关注发音特别错误或特别好的音素片段</td></tr><tr><td>维度分相关性、互补性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBad03336b0f416faef0441aabed50706e?method=download&shareKey=2853e4e6318acb5b4860f2242397b24a"></td><td>发音评分与文本评分有一定的相关性：发音中缺失的音素与说话长度、单词的丰富程度等有关；<br>各维度分互补</td></tr><tr><td>评分模型系统偏差</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4ef2e51c6d3e8128e12d0f861d029b1e?method=download&shareKey=9a212a7e25487cd248ad343aba51cb85"></td><td><font color="red">韵律分：高水平数据中才与总分相关，无法区分中低水平说话人的韵律水平；<br>发音和文本特征较难区分高水平数据；<br>发音分低分打低：可能由于ASR错误率更高</font></td></tr><tr><td>总分</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB28847721cde7c6b5b25325a1baf7b812?method=download&shareKey=ba4b6bf4eae0cde16e8ed2c6d66dc33f"></td><td>各维度分的加权和（采用attention计算加权系数）效果最好</td></tr></tbody></table></li></ul><h2 id="其它">1.4. 其它</h2><ul><li>Kyriakopoulos K. Deep Learning for Automatic Assessment and Feedbackof Spoken English[D]. University of Cambridge, 2022.<ul><li>5.1节：理论推导了采用总分标注数据，训练多维度评分模型的可行性</li></ul></li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2018). A deeplearning approach to assessing non-native pronunciation of english usingphone distances. In Proceedings of the Annual Conference of theInternational Speech Communication Association, INTERSPEECH, volume2018, pages 1626–1630.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2019). A deeplearning approach to automatic characterisation of rhythm in non-nativeenglish speech. In INTERSPEECH, pages 1836–1840.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2020). Automaticdetection of accent and lexical pronunciation errors in spontaneousnon-native english speech. Interspeech.</li></ul><h1 id="multi-aspect-multi-granularity">2. 2022 Multi-AspectMulti-Granularity</h1><ul><li>作者：MIT 人工智能实验室（CSAIL）、平安科技研究院（PAII Inc.）</li><li>发表信息：ICASSP 2022</li><li><font color="red">代码：https://github.com/YuanGongND/gopt</font>(Goodness Of Pronunciation feature-based Transformer)</li><li><font color="red">创新点<ul><li>联合训练音素、单词、句子级各维度分及总分</li><li>采用BERT风格非层级的标准Transformer 架构</li></ul></font></li></ul><h2 id="系统结构-2">2.1. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB915a9e1bc3fbd6be8224b3a8122dbfbd?method=download&amp;shareKey=6c0e39f6c5b19fa1a9892f30ad805242" alt="系统结构"> + 声学模型 + TDNN-F，训练集：960h Librispeech，用KaldiLibrispeech S5 recipe训练 + PAII-A：自研AM，452h L1 + 1696h L2 +PAII-B：995h L1 + 6591h L2 + 输入 + GOP特征：84维（42个音素，log phoneposterior、log posterior ratio），经过1层线性层降维至24维 +正确发音phone embedding，24维。 + 音素序列填充5个[cls]token，对应句子级各维度分、总分 + 位置embedding，24维，可训练 +采用标准Transformer encoder结构，但减为3层，embedding 24维 +评分：各个评分分别采用1层24*1的线性层，layernormalization。<font color="green">单词分：训练时反向传播至该单词的各个音素，推断时取其各个音素的输出的均值。</font></p><h2 id="评价-1">2.2. 评价</h2><ul><li>数据集：speechocean762（类别不均衡，主要为高分），单词、句子评分缩放至0-2，与音素一致</li><li>评价指标：主要为PCC（Pearson相关系数）</li><li>基线：speechocean762实现的RF（随机森林）、SVR（支持向量回归），[21]transferlearning、LSTM（模型深度、维度等与GOPT一致，LSTM最后一个token的输出作为句子表示）</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe86bc0edfdbde5c5f9c1462cd7bc2797?method=download&amp;shareKey=a13ff3549e54e584ec7ebb674f8d68f3" alt="实验结果"> *用不同的随机种子重复5次实验，统计均值、标准差</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe04116ccf077c827de0f320a76641997?method=download&shareKey=7c437dbe136f2f972ca8c513e65879df" width="50%"></p><ul><li>实验结论<ul><li><font color="red">GOPT除单词重音、句子完整度评分性能较差（可能与speechocean762训练集中句子完整度分布不均有关）外，其它任务可提供SOTA效果</font></li><li>采用PAII-A，音素、单词评分性能提升，但句子评分性能下降</li><li>联合训练音素、单词、句子评分模型，相对于分别训练，各模型性能都有提升。</li><li>正确发音phone embedding对提升模型性能有帮助</li><li>继续加宽或加深模型结构，性能无提升（训练集较小）</li><li>采用PAII-A、PAII-B，评分性能相当</li></ul></li></ul><h2 id="其它-1">2.3. 其它</h2><ul><li>Gong Y, Chen Z, Chu I H, et al. Transformer-Based Multi-AspectMulti-Granularity Non-Native English Speaker PronunciationAssessment[C]//ICASSP 2022-2022 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2022:7262-7266.</li></ul><h1 id="multi-granularity">3. 2020 Multi-Granularity</h1><ul><li>作者：腾讯智能平台产品部、北京语言大学</li><li>发表信息：Interspeech 2020</li><li><font color="red">创新点<ul><li>考虑音素、单词、句子评分间的层次关系和上下文，提出一种分层网络结构，联合评分</li><li>采用半监督训练，利用无标注数据训练音素检错</li></ul></font></li></ul><h2 id="系统结构-3">3.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEB15d8915a312dd0f51572a305759e4465?method=download&shareKey=06c0c6e07038fab09d0b18a838356915" width="782px"></p></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li>音素检错<ul><li>输入：GOP（通过强制对齐计算）、音素embedding、位置embedding（B、I、E、S分别表示单词开头、中间、末尾、单音素词。<font color="red">音素发音因其在单词中的位置而异</font>）、类别embedding（C、V分别表示辅音、元音。<font color="red">单词中元音和辅音的重要性不同</font>）</li><li>模型结构：BLSTM。半监督学习</li></ul></li><li>单词评分<font color="red">单词中每个音素对最终单词得分的贡献不同，采用attention机制。</font><span class="math inline">\(U_{p}=\tanh \left(w *O_{p}+b\right)\)</span>, <span class="math inline">\(\alpha_{p}=\frac{\exp \left(U_{p}^{T}U_{w}\right)}{\sum_{q \in w} \exp \left(U_{q}^{T}U_{w}\right)}\)</span>, <span class="math inline">\(S_{w}=\sum_{p \in w}\alpha_{p} O_{p}\)</span> <font color="green"> 其中, <span class="math inline">\(O_p\)</span> 为音素<span class="math inline">\(p\)</span>的评分, <span class="math inline">\(U_w\)</span>是随机初始化的向量,可以作为单词上下文的记忆单元。</font></li><li>句子评分<ul><li><font color="red">不同属性（如词性、音素个数）的单词对句子得分贡献不同。</font></li><li>输入：word层输出、词性、单词长度</li><li>模型结构：BLSTM+MLP，sigmoid回归。</li></ul></li><li>multitask：<span class="math inline">\(L_{total}=(1-w)\timesL_{sent}+ w\times L_{phoneme}\)</span>，<span class="math inline">\(L_{sent}\)</span>其中为句子评分的均方误差损失，<span class="math inline">\(L_{phoneme}\)</span>为PUNU损失。</li></ul></div><ul><li>半监督 - PUNU (positive unlabeled and negative unlabeled) learning<ul><li><p>正样本：native发音；负样本：GOP较低的L2学习者发音；unlabeled数据：剩下的L2发音。</p></li><li><p>损失函数如下：</p><p><span class="math inline">\(R_{\mathrm{PUNU}}^{\gamma}(g)=(1-\gamma)R_{\mathrm{PU}}(g)+\gamma R_{\mathrm{NU}}(g)\)</span> <span class="math inline">\(R_{\mathrm{PU}}(g)=\theta_{\mathrm{P}}E_{\mathrm{P}}[l(g(x),1)]+E_{\mathrm{U}}[l(g(x),-1)]-\theta_{\mathrm{P}}E_{\mathrm{P}}[l(g(x),-1)]\)</span> <span class="math inline">\(R_{\mathrm{NU}}(g)=\theta_{\mathrm{N}}E_{\mathrm{N}}[l(g(x),-1)]+E_{\mathrm{U}}[\mathrm{l}(g(x),1)]-\theta_{\mathrm{N}} E_{\mathrm{N}}[l(g(x), 1)]\)</span></p><p>其中, <span class="math inline">\(g\)</span> 为任意决策函数, <span class="math inline">\(l\)</span> 为 loss 函数, <span class="math inline">\(\theta_P\)</span>、<span class="math inline">\(\theta_N\)</span> 为正负样本的先验概率, <span class="math inline">\(E_U\)</span>、<span class="math inline">\(E_P\)</span>、<span class="math inline">\(E_N\)</span>分别表示未标记数据、正类、负类（边际）的损失期望。</p></li></ul></li></ul><h2 id="评价-2">3.2. 评价</h2><ul><li><p>数据集</p><ul><li>Timit +22998英语句子，1000中国说话人，16-20岁。句子评分、单词评分、音素检错标注量分别为8998、4000、10000句。句子平均单词数为13。标注音素量99568。</li><li>1-5分，3人评分取均值。3人评一致性：计算某一评分员的评分与剩余评分员的平均分之间的PCC，句子、词级分别为0.78、0.76。</li><li>检错3人投票，<font color="green">3人评一致性：随机挑选1000句，计算任意两标注员的Kappa系数，平均0.65，95%置信度区间(0.647,0.653)，p-value小于0.1%，一致性较高。</font></li></ul></li><li><p>训练集：7998句non-native数据，有评分。5000句native数据，无评分。</p></li><li><p>测试集：4000句，标注了39808个音素、1000词、1000句。错误音素占比约14%。</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>音素错误分布</th><th>单词分分布</th><th>句子分分布</th></tr></thead><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB85b5a6de565f9c520658835b95786ba8?method=download&amp;shareKey=8632f81f22963f5f07fd6cb3e9eef9d2"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2d0fd1ee7ade8549ac8bcab6226f37e1?method=download&amp;shareKey=2f0a145baa086763a420d45b143c03a9"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4d0c8cbceb3537c0867f876ff2f2f3e7?method=download&amp;shareKey=6e744f4ae6c8a59b0bb97a0674605191"></td></tr></tbody></table></li><li><p>实验结果 实验 | 基线 | 实验结果 | 实验结论 ---|---|---|---句子评分 |2BLSTM+MLP，后一BLSTM的输入为音素BLSTM最后一个隐含单元的输出拼接、词性、单词长度|<img src="https://note.youdao.com/yws/api/personal/file/WEBa5476cf9e1f6ac5f1ee3c5cdf4c56b69?method=download&shareKey=1c586ae1cc1ff426900c1b9e0fccfd67"><br> *STL：single task learning |单词层attention、multitask学习可提升评分性能 单词评分 |BLSTM+MLP：去掉上述句子评分BLSTM。SL：用3000个单词评分标注数据训练 |<img src="https://note.youdao.com/yws/api/personal/file/WEB30a1747796429bff94888d09a7eb6ce1?method=download&shareKey=fd537dcca8a48cdd697826af548cdbd4">|对比前两行：attention机制有收益；<br>最后一行：PCC较高，仅用句子、音素级标注信息，仍能学到单词分信息音素检错 | SL：用59760个音素检错标注训练 |<img src="https://note.youdao.com/yws/api/personal/file/WEB8f8bcea9b4f939f430dd5b5985a7b779?method=download&shareKey=767fa915db6e5fe6e9c036fb5461cd0b">|半监督学习未召回略差于有监督学习，虚警相差较小。<font color="green">加无标注数据效果变差？</font></p></li></ul><h2 id="其它-2">3.3. 其它</h2><ul><li>Lin B, Wang L, Feng X, et al. Automatic scoring at multi-granularityfor L2 pronunciation[J]. Proc. Interspeech 2020, 2020: 3022-3026.</li></ul><h1 id="ets-monologue-and-dialogue">4. 2019 ETS monologue and dialogue</h1><ul><li>作者：ETS</li><li>发表信息：ICASSP 2019</li><li><font color="red">创新点<ul><li>采用基于attention的BLSTM对自由表述的3个维度评分：内容（话题相关度、得体性）、<del>组织（语篇结构和连贯性）、</del>语用（词汇、语法）、delivery（发音、重音、流利度、语调）</li><li>采用BLSTM或MemN2N（端到端记忆网络）编码提示文本或多轮对话的历史信息</li></ul></font></li></ul><h2 id="系统结构-4">4.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb737aee697fb953d540bee20a2fa9f48?method=download&shareKey=bd923154f67ce2976488da77c835431c"></p></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li>内容<ul><li>word embedding层：用Google's Word2Vec初始化，模型训练时优化</li><li>采用BLSTM将提示文本的词序列编码为固定长度的向量<span class="math inline">\(v^{p}\)</span>，与回答中各个词的词向量<span class="math inline">\(e_{t}^{r}\)</span>拼接</li></ul></li><li>语用特征：POS：词性one-hot向量；DEP：句法依存标签，如主语、宾语；Morph（形态）。采用spaCy提取，分别19、51、248维</li><li>发音：采用non-native ASR模型识别，nativeASR模型<font color="red">强制对齐</font>。8维特征：时长、音调、强度、静音或停顿时长、non-nativeASR模型后验概率、nativeASR模型后验概率、识别结果LM分、ASR置信度分，取各帧平均（实验对比音素、音节、词级特征）</li><li>评分模型<ul><li>维度分：<font color="green">feed-forwardattention层输出向量的均值。</font>多轮对话：对每个回答评维度分，整个对话的维度分取多轮对话的均值</li><li>总分：3个维度分拼接，经过1层全连接层</li></ul></li></ul></div><ul><li><p>采用MemN2N（端到端记忆网络）编码多轮对话的历史信息</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB7bf9ff8d8f680293f1103d2ad9247e0d?method=download&shareKey=e1ee2a6611af41883a37a25ee02708ba" width="414px"></p><p>拼接<span class="math inline">\(e_{t}^{r}\)</span>、<span class="math inline">\(v^{p}\)</span>、<span class="math inline">\(a^{p}\cdot v_{h}^{p}\)</span>、<span class="math inline">\(a^{r} \cdotv_{h}^{r}\)</span>，其中，<span class="math inline">\(v_{h}^{p}\)</span>、<span class="math inline">\(v_{h}^{r}\)</span>分别表示历史提示、历史回答，<span class="math inline">\(a^{p}\)</span>、<span class="math inline">\(a^{r}\)</span>分别表示对应的attention向量</p></li><li><p>语用特征示例<img src="https://note.youdao.com/yws/api/personal/file/WEB8b9848c574cb881e11ef9f4ba799a455?method=download&shareKey=c9480e8e26bb1c13902d745c6f37e3d5" width="410px"></p></li></ul><h2 id="评价-3">4.2. 评价</h2><ul><li><p>数据集</p><ul><li><img src="https://note.youdao.com/yws/api/personal/file/WEB4c9a9144977d69a7f73545ff490b5f49?method=download&shareKey=ecb0b2fae6f26aa5092f69916a3ce29c" width="414px"></li><li>monologue：delivery、内容、语用，0-4分</li><li>对话：整个对话的总分，考虑熟练程度和任务完成情况</li></ul></li><li><p>声学模型</p><ul><li>识别模型：基于iVector的BLSTM，960hnon-native数据。LM用提示文本自适应</li><li>强制对齐：960h LibriSpeech</li><li>提取语用、内容特征时过滤filler words、重复的partial words</li></ul></li><li><p>超参：BLSTM 128维。dropout=0.5。100 epochs、batch size64。MemN2N：记忆前10轮提示与回答，memory size 20</p></li><li><p>基线</p><ul><li>评分特征：SpeechRater，超过100个</li><li>回归模型：Logistic回归、AdaBoost、决策树、GradientBoost、SVM、随机森林等。其中，随机森林效果最好。</li></ul></li><li><p>实验结果</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBda84d306717e91a49a5487ae38050729?method=download&shareKey=687b24e9378b54aaa8e828c1589709e4" width="417px"></p><p>*预测的维度分与总分计算相关度，无人工标注</p></li><li><p>实验结论</p><ul><li>内容：结合提示信息，评分效果较好。采用MemN2N可进一步提高多轮对话内容评分与人工分的相关度</li><li>发音采用音节级特征较好（音素级或音节级LM分：采用所属单词的LM分）</li><li>输入所有特征计算总分，效果更好，神经网络可以学习各维度特征间的关系</li></ul></li></ul><h2 id="其它-3">4.3. 其它</h2><ul><li>Qian Y, Lange P, Evanini K, et al. Neural approaches to automatedspeech scoring of monologue and dialogue responses[C]//ICASSP 2019-2019IEEE international conference on acoustics, speech and signal processing(ICASSP). IEEE, 2019: 8112-8116.</li><li>展望：可解释性、诊断</li></ul><h1 id="ets-prompt-aware">5. 2018 ETS prompt-aware</h1><ul><li><p>方案同上</p></li><li><p>基线：SVR评分模型。采用C-rater系统提取特征：</p><ul><li>2-5阶 character n-gram</li><li>词级 1-2阶 n-gram</li><li>回答的字符数</li><li>句法依赖：采用Zpar dependency parser提取</li><li>Prompt bias</li></ul></li><li><p>LM：口语测试转写文本（超过5百万词）训练的LM、提示文本训练的LM线性插值</p></li><li><p>实验结果</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>prompt-aware<br>Siamese LSTM:用于评分前先进行离题检测，分类准确度97.3%。Manhattan distance</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa24523c00cf49bcef30e9181fffc1655?method=download&amp;shareKey=ad09059e7e16a6f1de9fd0f54b8d4e64" alt="模型结构对比"></td><td>prompt-encoder可以学到离题信息</td></tr><tr><td>模型结构</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB724c8edd670b9460906fb9935b2c7a9d?method=download&amp;shareKey=69cae0a1c815483183f70678579416b5" alt="模型结构对比"></td><td>fine-tune word embedding、attention机制、prompt encoder都有收益</td></tr><tr><td>对比传统模型</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf1396007c43b6dd584ac28e18b137e3e?method=download&amp;shareKey=baf3230953f5f4a36a8581b2f325123a" alt="对比基线"></td><td>promptbias特征收益较小；<br>相较于采用人工转写，采用ASR识别结果时集外题相对集内题的效果下降更显著；<br>promptencoder（最后两列）有收益，特别是在集外题上</td></tr></tbody></table></li></ul><h2 id="其它-4">5.1. 其它</h2><ul><li>Qian Y, Ubale R, Mulholland M, et al. A prompt-aware neural networkapproach to content-based scoring of non-native spontaneousspeech[C]//2018 IEEE spoken language technology workshop (SLT). IEEE,2018: 979-986.</li><li>内容分<ul><li>LSA (Latent Semantic Analysis)：对各任务分别训练LSA模型，计算识别的词序列与训练集中高分数据的cosine相似度，SVD（奇异值分解）降维</li><li>CVA (Content VectorAnalysis)：按人工分将训练集分组，计算cosine相似度。</li><li>考虑识别单词的置信度分，使模型对识别错误更鲁棒</li><li>multi-task训练BLSTM：打分+word embedding，指定分数的词向量更有区分性&gt; Alikaniotis D, Yannakoudakis H, Rei M. Automatic text scoring usingneural networks[J]. arXiv preprint arXiv:1606.04289, 2016.</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的语音评分</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="ets-acoustics-transcription">1. 2018 ETS acoustics +transcription</h1><ul><li>作者：ETS</li><li>发表信息：ICASSP 2018</li><li><font color="red">创新点：提示无关的神经网络评分模型（BD-LSTM +attention），输入识别文本wordembedding、各单词的后验概率及声学特征，输出评分</font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd31cc498c578d02c7497165e2e58a513?method=download&amp;shareKey=3bd314af1bfa5a11c60bfded4a5d2cd2" alt="系统结构"> + 声学模型：DNN-HMM，训练集：819h non-native自发语音 +评分模型输入 + lexical模型：识别文本wordembedding序列。采用预训练的Glove模型，OOV采用全0向量，300维，训练评分模型时fine-tune+ 声学模型：每个词的声学模型后验概率、时长、pitch均值、intensity均值 +评分模型 + 1D CNN + 参考[14]，采用3种尺寸的卷积核<span class="math inline">\(\left(conv_{size}-1, conv_{size},conv_{size}+1\right)\)</span>，用于覆盖不同的感受野。各<span class="math inline">\(conv_{n}\)</span>个卷积核 + input -&gt; dropout<span class="math inline">\(dp_{CNN}1\)</span> -&gt; 卷积层 -&gt; maxpooling（沿时间轴） -&gt; dropout <span class="math inline">\(dp_{CNN}2\)</span> + BD-LSTM + input -&gt; dropout<span class="math inline">\(dp_{RNN}1\)</span> -&gt; BD-LSTM -&gt;两个方向的隐层状态拼接 -&gt; dropout <span class="math inline">\(dp_{RNN}2\)</span> + BD-LSTM + attention + input-&gt; dropout -&gt; BD-LSTM -&gt; attention -&gt; dropout +超参tuning：采用Hyperopt Python包实现的Tree Parzen Estimation(TPE)方法[23]。<span class="math inline">\(conv_{size}=4, conv_{n}=100,dp_{CNN}1=dp_{RNN}1=0.25, dp_{CNN}2=dp_{RNN}2=0.5, LSTM_{dim}^{lex}=128,LSTM_{dim}^{ac}=32\)</span></p><h2 id="评价">1.2. 评价</h2><ul><li><p>数据集：训练集 2930，开发集 731，测试集 1827。4分制。</p></li><li><p>传统模型（基线）</p><ul><li><p>随机森林、GBT（Gradient Boosting Tree）、SVR（Support VectorRegression）。其中，GBT模型人-机评分相关度最高。</p></li><li><p>评分特征</p><figure><img src="https://note.youdao.com/yws/api/personal/file/WEB2b3d68c3d17278fcc388c60f44fbca7b?method=download&amp;shareKey=93dba1172f8282c23aea663f0988cfed" alt="评分特征"><figcaption aria-hidden="true">评分特征</figcaption></figure><table><colgroup><col style="width: 50%"><col style="width: 50%"></colgroup><thead><tr><th>类别</th><th>特征示例</th></tr></thead><tbody><tr><td>流利度</td><td>单词数/秒、单词数/段、静音段个数、静音段平均时长、长停顿（&gt;0.5s）频率、有声停顿（uh、um）个数</td></tr><tr><td>韵律、语调、重音</td><td>韵律事件（prominences and boundarytones）的占比、之间的平均距离、距离的平均差，元音、辅音、音节时长的占比、标准差、PairwiseVariability Index</td></tr><tr><td>发音</td><td>native AM强制对齐计算likelihood、ASR词级置信度均值、在native语料上统计各元音的时长均值，计算测试数据元音时长与参考值的差值的均值</td></tr><tr><td>语法</td><td></td></tr><tr><td>用词</td><td>多样性、复杂度</td></tr></tbody></table></li></ul></li><li><p>实验结果</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB80c4a6aa4b8e87255eb450053d7e42d9?method=download&shareKey=19bfa2868abd6d958f9a91870d239584" width="50%"></p><p>*相较于传统评分模型采用n-gram提取评分特征，wordembedding可提供更丰富的信息</p></li><li><p>展望：可解释性、更多声学特征、其它attention机制</p></li></ul><h2 id="其它">1.3. 其它</h2><ul><li>Chen L, Tao J, Ghaffarzadegan S, et al. End-to-end neural networkbased automated speech scoring[C]//2018 IEEE international conference onacoustics, speech and signal processing (ICASSP). IEEE, 2018:6234-6238.</li><li>Yu Z, Ramanarayanan V, Suendermann-Oeft D, et al. Usingbidirectional LSTM recurrent neural networks to learn high-levelabstractions of sequential features for automated scoring of non-nativespontaneous speech[C]//2015 IEEE Workshop on Automatic SpeechRecognition and Understanding (ASRU). IEEE, 2015: 338-345.</li><li>Taghipour K, Ng H T. A neural approach to automated essayscoring[C]//Proceedings of the 2016 conference on empirical methods innatural language processing. 2016: 1882-1891. CNN（提取局部上下文信息）+RNN（提取长时信息）+ mean over time回归（利用全文信息）</li></ul><h1 id="chinese-english-interpretation口语翻译">2. 2022 Chinese-EnglishInterpretation口语翻译</h1><ul><li>作者：广东外语外贸大学</li></ul><h2 id="系统结构-1">2.1. 系统结构</h2><ul><li><p>流畅度分：语速</p></li><li><p><font color="red">关键字、内容、语法：采用Bert预训练模型、BiLSTM、attention机制</font></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2a27b65f57024957f23441b7cf64123f?method=download&shareKey=d00680ca591a47f6ec40226486adbc31" width="50%"></p></li><li><p>采用随机森林回归器融合4个维度分计算总分</p></li></ul><h2 id="其它-1">2.2. 其它</h2><ul><li>Li X, Li X, Chen S, et al. Neural-based automatic scoring model forChinese-English interpretation with a multi-indicator assessment[J].Connection Science, 2022, 34(1): 1638-1653.</li></ul><h1 id="word-scoring">3. 2022 Word Scoring</h1><ul><li>作者：字节跳动</li><li><font color="red">创新点<ul><li>数据增强：给定词典中的音素序列，从训练数据相应的音素级特征中随机抽样来伪造单词样本，单词分取音素GOP均值</li><li>采用MFCC、ASR AM deep feature进行评分</li></ul></font></li></ul><figure><img src="https://note.youdao.com/yws/api/personal/file/WEBe00917195f9a8da3492d09eff12bc9f6?method=download&amp;shareKey=55db844dfc0db67bba44ab887f6bea6b" alt="数据增强"><figcaption aria-hidden="true">数据增强</figcaption></figure><h2 id="其它-2">3.1. 其它</h2><ul><li>Fu K, Gao S, Wang K, et al. Improving Non-native Word-levelPronunciation Scoring with Phone-level Mixup Data Augmentation andMulti-source Information[J]. arXiv preprint arXiv:2203.01826, 2022.</li></ul><h1 id="弃2020-automated-chinese-language-proficiency-scoring-by-utilizing-siamese-convolutional-neural-network-and-fusion-based-approach">4. 【弃】2020Automated chinese language proficiency scoring by utilizing siameseconvolutional neural network and fusion based approach</h1><ul><li>论文质量较差，弃</li><li>自制数据集</li></ul><h2 id="系统结构-2">4.1. 系统结构</h2><ul><li>native speakers' key points、测试者语音，提取100*300维向量 -&gt;分别送入权重共享的卷积层 -&gt; pooling层 -&gt; 计算cosine相似度 -&gt;线性层输出分数</li><li>人工设计的特征：详见SpeechRaterv5.0。提取tf-idf特征，计算测试语音、同一单词人工分4分的训练语音的cosine相似度</li></ul><h2 id="其它-3">4.2. 其它</h2><ul><li>Kwong A, Muzamal J H, Zhang P Y, et al. Automated chinese languageproficiency scoring by utilizing siamese convolutional neural networkand fusion based approach[C]//2020 International Conference onEngineering and Emerging Technologies (ICEET). IEEE, 2020: 1-6.</li><li>语音评测系统质量控制[37, 39-42]</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>端到端语音评分</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ping-fen/duan-dao-duan/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ping-fen/duan-dao-duan/</url>
      
        <content type="html"><![CDATA[<h1 id="self-supervised">1. 2022 Self-Supervised</h1><ul><li>作者：AI Lab, Kakao Enterprise（韩国公司）</li><li><font color="red">创新点：采用自监督学习模型提取深层声学表示</font></li></ul><h2 id="系统结构">1.1. 系统结构</h2><div style="display: inline-block; width: 60%;"><figure><img src="https://note.youdao.com/yws/api/personal/file/WEB4422119d06867cb6f80ed36536834260?method=download&amp;shareKey=74b198b7631782b93ae09c5a823fa71e" alt="系统结构"><figcaption aria-hidden="true">系统结构</figcaption></figure></div><div style="display: inline-block; vertical-align: bottom; width: 39%;"><p>*GAP：global average pooling</p><ul><li>用L2数据+CTC fine-tune 自监督学习模型（wav2vec 2.0、HuBERT）</li><li>取各transformer层的上下文表示的均值</li><li>输入文本字符序列，采用BLSTM评分</li></ul></div><h2 id="评价">1.2. 评价</h2><ul><li>数据集：KESL（韩国英语二语学习者，根据发音、准确度、重音、停顿、语调等评总分）、Speechocean762</li><li>基线<ul><li>自研DNN-HMM AM</li><li>Agg: SpeechRater time-aggregated特征。Seq: 时间序列特征，eGeMAPSset[33]（包含MFCC、响度、音调、jitter、shimmer）的均值、方差，各特征均值方差归一化，采用OpenSmile提取</li><li>评分模型：2层全连接</li></ul></li><li>自监督学习模型：采用Fairseq上的，finetune 150k步，batch size=8</li><li>评分模型：embedding 64维，BLSTM 1层，128维</li><li><img src="https://note.youdao.com/yws/api/personal/file/WEB7540a07fff0e50902ae1f03f916265d2?method=download&shareKey=a683224dcdddbd810b26b9422f1bc3ee" width="383px"></li><li>实验结论<ul><li>相较于传统评分模型，本文提出的模型在各测试集上的评分效果有一致的提升。其中finetune之后的HuBERTLarge模型效果最好</li><li>相较于采用自监督学习模型的CNN层特征、较高层的特征表示，采用各transformer层的上下文表示的均值，效果较好</li><li>相较于线性回归、MLP，BLSTM模型评分效果较好，可以学习时间序列特征</li></ul></li><li><font color="green">存在的问题：无Speechocean762准确度、完整度、总分评分效果</font></li></ul><h2 id="其它">1.3. 其它</h2><ul><li>Kim E, Jeon J J, Seo H, et al. Automatic Pronunciation Assessmentusing Self-Supervised Speech Representation Learning[J]. arXiv preprintarXiv:2204.03863, 2022.</li></ul><h1 id="multi-encoder">2. 2021 Multi-Encoder</h1><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：ICASSP 2021</li><li>被引用次数：2</li><li><font color="red">创新点：端到端朗读评分，输出句子级、单词级分数</font></li></ul><h2 id="系统结构-1">2.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><figure><img src="https://note.youdao.com/yws/api/personal/file/WEBa24e448d76d182c1bf2d2df35649895b?method=download&amp;shareKey=161bc7c16a5682e139a3565471ea8c50" alt="系统结构"><figcaption aria-hidden="true">系统结构</figcaption></figure></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li>音频encoder<ul><li>输入：80维Mel-filterbank + 1阶差分 + 2阶差分 + pitch +位置embedding</li><li>CNN（提取局部特征、降采样） +transformer（每一层transformer由1层multi-head attention +1层position-wise全连接组成），12层</li></ul></li><li>文本encoder<ul><li>输入：采用sentencepiece生成的子词序列 + 位置embedding</li><li>vanilla transformer encoder结构，2层</li></ul></li><li>词级表示 $h1_{word( i ) }=Attention ( h_{word( i ) }, h_{audio},h_{audio} ) + h_{word( i ) } $</li><li>单词评分：1层全连接 + sigmoid</li><li>句子评分<ul><li>输入：所有单词表示的average pooling，拼接所有单词的平均分</li><li>1层全连接 + ReLU + 1层全连接 + sigmoid</li></ul></li></ul></div><ul><li>训练流程<ul><li>audio encoder + decoder，ASR训练。训练集：960h LibriSpeech + 1000hL2</li><li>冻结audioencoder，预训练。将上述训练集中的词随机替换40%为口语考试中的高频词，不匹配的单词label为0，否则为1。</li><li>采用专家标注数据finetune，采用multi-task学习（单词评分、句子评分）。训练集：10000句、15000词；测试集：1000句、1000词。3位专家打分，1-5，归一化为0-1</li></ul></li></ul><h2 id="实验结果">2.2. 实验结果</h2><ul><li>基线<ul><li>GBT，输入特征为各音素的平均GOP分、各音素在句中的平均位置（B-1，I-2，E-3，S-4）、各音素的数量、句中元音时长的平均差</li><li>BD-LSTM withattention，声学特征为音素GOP、元音时长的平均差，文本特征为音素embedding、音素位置embedding、预训练的GloVeword embedding</li></ul></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3897f724e308dd3f11f1d5bd91998a10?method=download&shareKey=43276c4d04b452c03fa73d45cb868411" width="393px"></p><h2 id="其它-1">2.3. 其它</h2><ul><li>Lin B, Wang L. Attention-Based Multi-Encoder Automatic PronunciationAssessment[C]//ICASSP 2021-2021 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2021:7743-7747.</li></ul><h1 id="transfer-learning">3. 2021 Transfer Learning</h1><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：Interspeech 2021</li><li>被引用次数：6</li><li><font color="red">创新点：当前基于DNN-HMM的GOP计算存在许多局限性。相较于传统方案采用ASRAM计算GOP等评分特征，提出直接采用AM提取的深层特征进行评分。迁移学习：采用ASR预训练模型，用评分任务fine-tune。相较于AM参数量，可采用特定的评分任务快速finetune评分模型</font></li></ul><h2 id="系统结构-2">3.1. 系统结构</h2><div style="display: inline-block; width: 50%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe06a9dd97f1e8da8052a976b113ce875?method=download&shareKey=398ca0c53b023a081355e6265d4e5e4c"></p></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li>输入：ASRAM提取的深层特征、<font color="red">强制对齐信息</font></li><li>基于强制对齐，对于各音素，对应帧的深层特征取均值，+ phoneembedding</li><li>multi-head self-attention + 非线性变换，得到词级表示</li><li>self-attention + 非线性变换，得到句级表示</li><li>sigmoid，输出句级评分</li></ul></div><ul><li>训练流程<ul><li>预训练11层TDNN-HMMASR声学模型，深层特征256维，非线性变换降为32维</li><li>采用大量合成数据，用基于GOP的评分模型打分（low-quality），预训练评分模型</li><li>采用少量指定任务的专家标注数据fine-tune</li></ul></li></ul><h2 id="评价-1">3.2. 评价</h2><ul><li><p>数据集</p><ul><li>ASR：960h LibriSpeech + 1000h L2</li><li>合成数据：50000句</li><li>fine tune数据<ul><li>11000句（含1000句测试集），平均词数13，3位专家打分1-5</li><li>Speechocean762（50%用作测试集，分数缩放至0-1）</li></ul></li></ul></li><li><p>模型</p><ul><li>LayerNorm</li><li>参数量：AM 8165047；评分模型 20289</li></ul></li><li><p>实验结果</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>对比基于GOP的SOTA：<br>输入音素GOP、embedding，2层BLSTM+MLP；<br>输入音素GOP均值、各音素的平均位置（BIES）、句子音素总数，GBT</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2584d9889200577d4e05c1c40b773378?method=download&amp;shareKey=2ff0f42e16e7e3bd70ff3579ca55fa1d" alt="table2"></td><td></td></tr><tr><td>对比输入特征<br>STPs：考虑转移概率</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB41fd8d0028b57c6ac522716778d9bf76?method=download&amp;shareKey=640db70f420eb058d02bf9acf6e4736c" alt="table3"></td><td><font color="green">采用音素GOP特征，当前模型效果差于2BLSTM+MLP？</font></td></tr><tr><td>冻结AM参数、3阶段训练</td><td></td><td>始终冻结AM参数+第2阶段预训练+finetune效果最好。评分模型参数量约为AM的0.2%，finetune高效</td></tr><tr><td>self-attention机制：对比替换为平均</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB448228dfdb5c057f5ff3834eb5266409?method=download&amp;shareKey=a776ada79534cc24c131ec6d40c2d626" alt="table6"></td><td></td></tr></tbody></table></li></ul><h2 id="其它-2">3.3. 其它</h2><ul><li>Lin B, Wang L. Deep Feature Transfer Learning for AutomaticPronunciation Assessment[C]//Interspeech. 2021: 4438-4442.</li></ul><h1 id="multi-modal">4. 2020 Multi Modal</h1><ul><li>作者：印度、哥伦比亚大学、SLTI公司</li><li>被引用次数：9</li><li><font color="red">创新点：提出了基于注意力融合的多模态端到端自发语音评分</font></li></ul><h2 id="系统结构-3">4.1. 系统结构</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4b48b1cf6ca4f879e1ddd50f955e5c96?method=download&shareKey=b833b0104a23cae2ad5f1e60d0e4287a" width="740px"></p><ul><li>音频<ul><li>输入：log-scaled mel谱，采样频率16000Hz，帧长2048、hop size 512，melband 128个，归一化，切分为固定帧数（128）的段</li><li>CNN+BLSTM，每组(2*Conv1D+MaxPooling)后，filters数目double</li></ul></li><li>文本<ul><li>non-native ASR，word embedding：300维gloveembedding初始化，OOV词初始化为全0向量，模型训练时优化</li></ul></li><li>拼接音频、文本表示<font color="red">（识别时需要输出单词起止时间）</font>，<span class="math inline">\(h^{m}=[h^{a}, h^{t}]\)</span>，attention <span class="math display">\[e_{t}=h_{t}^{m} w_{a} ; a_{t}^{m}=\frac{\exp\left(e_{t}\right)}{\sum_{i=1}^{T} \exp \left(e_{i}\right)} ;c^{m}=\sum_{t=1}^{T} a_{t}^{m} h_{t}^{m}\]</span></li></ul><h2 id="实验结果-1">4.2. 实验结果</h2><ul><li><p>数据集：题目难度、评分标准遵循Common European Framework ofReference(CEFR)标准，英语口语等级A2-C1（根据题目难度确定可以得到的最高分），人工分包含5个等级：A2、LB1、HB1、LB2、HB2，其中L、H分别表示low、high</p></li><li><p>为每个题目分别训练模型</p></li><li><p>评价指标：Quadratic Weighted Kappa(QWK)，衡量两个序列间的一致性。通常取值范围为0-1，若两者一致性低于随机分布，也可以为负数。</p><p><span class="math display">\[\kappa=1-\frac{\sum_{i, j} W_{i, j}O_{i, j}}{\sum_{i, j} W_{i, j} E_{i, j}}\]</span></p><p>其中，<span class="math inline">\(W\)</span>、<span class="math inline">\(O\)</span>、<span class="math inline">\(E\)</span>均为<span class="math inline">\(N\times N\)</span>的矩阵，<span class="math inline">\(N\)</span>为评分类别数，<span class="math inline">\(W_{i,j}=\frac{(i-j)^{2}}{(N-1)^{2}}\)</span>，<span class="math inline">\(O_{i, j}\)</span>表示评分为<span class="math inline">\(i\)</span>、人工分为<span class="math inline">\(j\)</span>的样本数，<font color="green"> <span class="math inline">\(E_{i,j}\)</span>表示分数期望的histogram矩阵（计算方式：人工分的histogram向量与机器分的histogram向量的外积，归一化使其与矩阵<span class="math inline">\(O\)</span>有相同的和）。QWK不适用于类别不均衡的数据，对样本数少的类特别敏感</font>。</p></li><li><p>采用hyperopt包优化每个分数之间round的阈值，以最大化QWK</p></li><li><p>实验结果 <img src="https://note.youdao.com/yws/api/personal/file/WEB17d08d0c67c81e666db7ed3145e80ade?method=download&amp;shareKey=0f1ddebcfd19bc8468516e55f866ff3e" alt="实验结果"> *MMAF: Multi-modal with Attention Fusion； BDRCNNAttn[A]、BDLSTMAttn [T]：单模态基线，attention权重仅赋予音频或文本</p></li><li><p>实验结论：多模态注意力融合比单模态网络效果更好</p></li><li><p>存在的问题：未与SOTA基线对比</p></li></ul><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>attention分布、与题目难度的关系</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc6463effb413ddf4d7584db5ddd33cd9?method=download&amp;shareKey=52862b06a3026ef3a7ef93d9430d3785" alt="attention分布1"></td><td>文本:音频attention权重比约为85:15 <br> 低难度题目（prompt1），attention权重向音频特征倾斜；高难度题目（prompt4），attention权重向识别文本特征倾斜 <br>评分越高，attention权重越向识别文本特征倾斜</td></tr><tr><td>attention分布定性分析</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfafd3a21834490651a22fd68420efd64?method=download&amp;shareKey=8958497f6fba8feaa1a0a1dd1eee2f86" alt="attention分布2"></td><td>对含uh、um等填充词，falsestarts，停顿的片段，attention权重偏向音频，对含关键信息的流利片段，attention权重偏向文本，如图2（prompt4主题为smoking）</td></tr></tbody></table><h2 id="其它-3">4.3. 其它</h2><ul><li>Grover M S, Kumar Y, Sarin S, et al. Multi-modal automated speechscoring using attention fusion[J]. arXiv preprint arXiv:2005.08182,2020.</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeNet论文</title>
      <link href="/blog/yin-pin/gong-ju-bao/wenet/lun-wen/"/>
      <url>/blog/yin-pin/gong-ju-bao/wenet/lun-wen/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>cleanup_segmentation</title>
      <link href="/blog/yin-pin/gong-ju-bao/kaldi/cleanup-segmentation.mm/"/>
      <url>/blog/yin-pin/gong-ju-bao/kaldi/cleanup-segmentation.mm/</url>
      
        <content type="html"><![CDATA[<div class="markmap-container" style="height:800px">  <svg data="{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;run_cleanup_segmentation.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;clean_and_segment_data.sh &lt;br/&gt; 采用转写文本构建语言模型，选取和转写文本编辑距离最小的解码路径；&lt;br/&gt;若识别为连续的重复，non-scored words间的错误，与sil、fix、OOV相邻的删除错误，修正转写文本；&lt;br/&gt; 挑选识别正确的片段，并加一系列限定条件 &lt;br/&gt; &lt;font style=background:green&gt; 输入&amp;lt;srcdir&amp;gt;：SAT GMM模型目录，或fMLLR对齐结果&lt;/font&gt; &lt;br/&gt; clean_and_segment_data_nnet3.sh 区别：功能一致，用NNET3模型解码&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;make_biased_lm_graphs.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;取转写文本中top_n_words（默认值100）个高频词，unigram概率=频次/高频词总频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;make_biased_lms.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;根据min-words-per-graph（默认值100）将音频文本分组&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;make_one_biased_lm.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;统计第n阶的count&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;CompletelyDiscountLowCountStates&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;GetHistToTotalCount：统计各历史/前缀（长度&amp;gt;=2）出现的频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;对第n至3阶，若历史/前缀出现的频次&amp;lt;min_count（默认值10），删除该项，并回退&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;ApplyBackoff：对第2至n阶，各项频次折扣discounting-constant（默认值0.3），累加给backoff_symbol，相应的低1阶的频次+1&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;AddTopWords：添加高频词unigram，频次为unigram总频次*概率&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;PrintAsFst：ngram概率=折扣后的ngram概率+折扣的概率*低1阶的概率，打印FST&quot;}]}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;compile-train-graphs-fsts：生成group HCLG&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;decode_segmentation.sh：gmm-latgen-faster&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;analyze_lats.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;lattice-depth-per-frame：lattice中经过各帧的弧数&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[22,23]},&quot;v&quot;:&quot;lattice-best-path：获取 1best 识别和对齐&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[23,24]},&quot;v&quot;:&quot;ali-to-phones --write-lengths=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[24,25]},&quot;v&quot;:&quot;analyze_phone_length_stats.py 统计各音素时长分布&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[25,26]},&quot;v&quot;:&quot;ali-to-phones --per-frame=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[26,27]},&quot;v&quot;:&quot;analyze_lattice_depth_stats.py 统计各音素lattice depth分布&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[28,29]},&quot;v&quot;:&quot;lattice_oracle_align.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[29,30]},&quot;v&quot;:&quot;lattice-oracle：获取和转写文本编辑距离最小的解码路径&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[30,31]},&quot;v&quot;:&quot;get_ctm&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[31,32]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-align-words-lexicon 对齐词边界&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[32,33]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-1best 获取最优路径（消歧符可能导致上述lattice有多条路径）&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[33,34]},&quot;v&quot;:&quot;nbest-to-ctm 打印utt_id, channel, start, dur, word_id&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[34,35]},&quot;v&quot;:&quot;align-text&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[35,36]},&quot;v&quot;:&quot;wer_per_utt_details.pl：打印utt-id、编辑距离、转写词数、解码结果、转写&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[36,37]},&quot;v&quot;:&quot;wer_per_spk_details.pl&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[37,38]},&quot;v&quot;:&quot;wer_ops_details.pl：打印各个词识别为正确、插入、删除、替换的频次&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[38,39]},&quot;v&quot;:&quot;get_ctm_edits.py：打印utt_id, channel, start, dur, 识别的单词, 置信度(始终为1），转写的单词, 编辑类型 &lt;br/&gt;（其中，sil表示无对应的转写单词且识别为&amp;lt;eps&amp;gt; sil；若转写单词不在词典中且识别为&amp;lt;unk&amp;gt;则编辑类型为cor）&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[40,41]},&quot;v&quot;:&quot;modify_ctm_edits.py：若识别为non-scored words（!sil, &amp;lt;eps&amp;gt;, &amp;lt;spoken_noise&amp;gt;, &amp;lt;unk&amp;gt;）间的替换、插入、删除，&lt;br/&gt;或者识别为连续的重复（如转写文本为a，识别为a a；或转写文本为a b，识别为a b a b），&lt;br/&gt;将转写文本中的词替换为识别文本中的，前者编辑类型改为fix，后者编辑类型改为cor&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[42,43]},&quot;v&quot;:&quot;taint_ctm_edits.py：识别错误前后连续相邻的sil、fix、OOV识别为&amp;lt;unk&amp;gt; 标记为tainted；若该识别错误为删除且前/后有tainted，删除该行&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[44,45]},&quot;v&quot;:&quot;segment_ctm_edits.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[45,46]},&quot;v&quot;:&quot;默认参数&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[46,47]},&quot;v&quot;:&quot;片段最短时长0.5s、新片段最短时长1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[47,48]},&quot;v&quot;:&quot;tainted词最大时长0.05s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[48,49]},&quot;v&quot;:&quot;片段首尾静音最大时长0.5s（若导致不满足片段最短时长或新片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[49,50]},&quot;v&quot;:&quot;片段首尾non-scored word最大时长0.5s（若导致不满足片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[50,51]},&quot;v&quot;:&quot;片段内单个静音段最大时长2s、片段内单个non-scored word&lt;font style=background:green&gt;（除OOV，实现未考虑）&lt;/font&gt;最大时长2s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[51,52]},&quot;v&quot;:&quot;如果片段首尾靠近识别错误，填充0.05s &amp;lt;unk&amp;gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[52,53]},&quot;v&quot;:&quot;tainted词+填充的unk占片段时长的最大比例0.1&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[53,54]},&quot;v&quot;:&quot;根据上一条规则分段时，分割点静音段或non-scored word的最短时长0.1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[54,55]},&quot;v&quot;:&quot;合并重叠或相邻的片段时，若片段间删除的转写词数&amp;lt;=1，保留&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[55,56]},&quot;v&quot;:&quot;GetSegmentsForUtterance&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[56,57]},&quot;v&quot;:&quot;ComputeSegmentCores：挑选仅含cor、fix、sil的片段，至少有一个词识别为cor且不为OOV，不包含tainted&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[57,58]},&quot;v&quot;:&quot;PossiblyAddTaintedLines：若边界识别为cor并且单词非non-scored word，前后相邻的1个词为tainted，扩充该词&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[58,59]},&quot;v&quot;:&quot;PossiblySplitSegment：根据片段内静音最大时长、片段内non-scored word最大时长分段，将识别为sil或转写文本为non-scored word的词均分&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[59,60]},&quot;v&quot;:&quot;PossiblyTruncateBoundaries：根据片段首尾静音最大时长、片段首尾non-scored word最大时长 截断片段首尾sil或non-scored word&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[60,61]},&quot;v&quot;:&quot;RelaxBoundaryTruncation：片段首尾非tainted截断：若满足片段时长&amp;gt;=片段最短时长、新片段最短时长，不撤销；若全撤销后仍不满足，全撤销；否则放宽截断比例至正好满足条件。&lt;br/&gt;令b=1-a，则length_with_truncation + (length_with_relaxed_boundaries - length_with_truncation) * b = length_cutoff &lt;br/&gt; start_keep_proportion = orig_start_keep_proportion + (1-orig_start_keep_proportion) * b&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[61,62]},&quot;v&quot;:&quot;PossiblyAddUnkPadding：若片段首尾识别为cor并且非non-scored word，填充unk_padding：不超过音频起止时刻；若填充时长&amp;lt; 0.5*unk_padding，不填充&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[62,63]},&quot;v&quot;:&quot;删除不满足新片段最短时长、片段最短时长的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[63,64]},&quot;v&quot;:&quot;PossiblyTruncateStartForJunkProportion：若片段起始 (unk_padding+tainted词时长)/(时长&amp;gt;min_split_point_duration，第一个识别为sil或识别为cor的non_scored_word前的时长) &amp;gt;= max_junk_proportion，删除该词前的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[64,65]},&quot;v&quot;:&quot;PossiblyTruncateEndForJunkProportion：同上，处理片段末尾&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[65,66]},&quot;v&quot;:&quot;ContainsAtLeastOneScoredNonOovWord：片段包含至少一个识别为cor、非OOV的scored_word，否则删除&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[66,67]},&quot;v&quot;:&quot;若片段首尾(unk_padding+tainted词)时长占比&amp;gt;max_junk_proportion，删除该片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[67,68]},&quot;v&quot;:&quot;合并重叠或相连的片段：若重叠片段包含的识别为del的词数&amp;gt;合并时保留的最大删除词数，则text不包含这些词&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[68,69]},&quot;v&quot;:&quot;AccWordStatsForUtterance：统计转写文本中各个词的词频、不被包含在分段中的比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[69,70]},&quot;v&quot;:&quot;WriteSegmentsForUtterance：写分段转写文本text和segment文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[70,71]},&quot;v&quot;:&quot;PrintDebugInfoForUtterance：写ctm文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[71,72]},&quot;v&quot;:&quot;PrintSegmentStats：打印音频总数，被完全丢弃的音频数，总时长，每一步处理后segment数目、相对于原始数据的时长比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[72,73]},&quot;v&quot;:&quot;PrintWordStats&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[74,75]},&quot;v&quot;:&quot;创建数据文件夹：统计padding=训练集（音频时长-特征时长）最高频的值，segment结束时刻+=padding &lt;br/&gt; 主要处理feats.scp、vad.scp，cmvn.scp需要重新生成&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[76,77]},&quot;v&quot;:&quot;重新计算CMVN&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[78,79]},&quot;v&quot;:&quot;对齐清洗后数据&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[80,81]},&quot;v&quot;:&quot;重训GMM模型&quot;}]}"/></div><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 工具包 </category>
          
          <category> Kaldi </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>评分标准</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ping-fen/ping-fen-biao-zhun/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ping-fen/ping-fen-biao-zhun/</url>
      
        <content type="html"><![CDATA[<h1 id="考纲">1. 考纲</h1><ul><li><a href="http://www.neea.edu.cn/res/Home/1901/d15ec0514666ac280810099f9595b557.pdf">普通高等学校招生全国统一考试大纲-2019</a><ul><li>语音项目<ul><li>基本读音：辅音连缀、成音节、单词重音</li><li>连读、失去爆破、弱读、同化</li><li>意群与停顿、语调、句子重音、节奏</li></ul></li></ul></li><li>英语口语等级考试三级考试大纲-2018（湖北省教育考试院）<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a><ul><li>短文朗读：语音语调（权重0.6）、流利程度（权重0.4）</li><li>情景提问：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>情景应答：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>连续表达：语音语调（权重0.2）、语法词汇（权重0.3）、流利程度（0.2）、交际能力（权重0.3）</li><li>分档：4档，百分制 &gt;=85、&gt;=75、&gt;=60、&lt;60</li></ul></li><li>广东高考-2011：模仿朗读、情景问答、故事复述<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li><li>广西高考-2021 <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a><ul><li>模仿朗读：语音语调（权重0.5）、流利度（权重0.3）、完整度（权重0.2）</li><li>口头表达：内容（权重0.5）、语法（0.27）、语音语调+流利度（0.23）</li></ul></li><li>2019年江苏省初中英语听力口语自动化考试纲要：朗读短文（4档）、情景问答（2档）、话题简述（4档）<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li><li>宁波市2022年初中学业水平考试英语听力口语自动化考试说明：朗读短文（4档）、情景问答（3档）、话题简述（4档）<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li><li><a href="http://jyj.panjin.gov.cn/2019_09/24_00/content-65069.html">盘锦中考-2019</a>：朗读（4档）、情景问答（3档）</li><li><a href="http://edu.wenzhou.gov.cn/art/2022/3/3/art_1341152_59021518.html">温州中考-2022</a>：篇章朗读、情景问答、说话（无评分标准介绍）</li></ul><h1 id="维度分考察点">2. 维度分考察点</h1><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>维度分</th><th>考察点</th><th>备注</th><th>分档描述</th></tr></thead><tbody><tr><td>发音准确度</td><td>语音、声调/单词重音</td><td>与完整度无关，自由表述题中与正确答案无关<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></td><td>5档：<br>语音、语调清晰、准确；<br>有错误，但不影响理解；<br>有错误，且有时影响理解；<br>有多处错误，且影响理解；<br>表现出较严重发音困难，且严重影响理解<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></td></tr><tr><td>流畅度</td><td>语速、停顿次数、重复<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></td><td>与朗读的内容无关<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a><font style="background: green">（回读）</font></td><td>5档：<br>朗读自然流利，语速适中，有节奏感<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>无语流中断，停顿和反复现象很少<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>；<br>基本流畅；<br>部分话语不够流畅；<br>话语大部分不流畅; <br> 不流畅<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></td></tr><tr><td>标准度/韵律</td><td>无中式口音，能灵活地运用连读、重读、失去爆破等发音技巧，节奏良好，感情充沛<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a><br>意群停顿、升降调、句子重音<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></td><td></td><td></td></tr><tr><td>完整度</td><td>朗读题：已读内容占提示文本的比例<br>自由表述题：要点覆盖率</td><td></td><td>5档：<br>内容丰富，完整、连贯；<br>内容基本完整、偶尔不够连贯；<br>有部分陈述不够完整，有时不连贯；<br>大部分陈述不完整，或不连贯；严重缺乏完整性和连贯性<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></td></tr><tr><td>语法</td><td>人称、单复数、时态、语态、动词的及物性；<br>词汇、短语、语法结构使用<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></td><td></td><td>5档：<br>能用合适的词汇、短语、语法结构组织话语；<br>个别地方出现错误；<br>少量错误；<br>大部分不正确；<br>不能正确使用<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></td></tr></tbody></table><h1 id="音标朗读">3. 音标朗读</h1><h1 id="单词朗读">4. 单词朗读</h1><h1 id="短文朗读">5. 短文朗读</h1><h2 id="总分与维度分">5.1. 总分与维度分</h2><blockquote><p>成人句子：total_score = (0.6*accuracy_score + fluency_score*0.3 +standard_score*0.1)* integrity_score/100</p><p>成人篇章：total_score = (0.5*accuracy_score + fluency_score*0.3+standard_score*0.2)* integrity_score/100 <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p></blockquote><h1 id="单项选择">6. 单项选择</h1><p>用户只能按事先设定的固定答案作答；只有读正确答案并且发音正确、完整，才有得分；用户回答多个选项，以后面的回答为准。<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p><h1 id="情景问答">7. 情景问答</h1><h2 id="题型说明">7.1. 题型说明</h2><p>先描述一段场景，然后从描述的场景中提出一个问题，让回答者根据听到的场景回答问题<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p><h2 id="示例">7.2. 示例</h2><p><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a><a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> <pre class="line-numbers language-none"><code class="language-none">&quot;para&quot;（描述情景的文本）: &quot;It&#39;s unbelievable. He looks stupid, but in fact, he is such a great and humorous actor. What&#39;s going on? You know what? Mr. Bean graduate from Oxford University. Exactly, I am also very crazy about Mr. Bean. He is really a funny guy and he does have a great sense of humor. In my eyes, he is a genius. I really admire him. I couldn&#39;t agree more, and it&#39;s his giftedness and hard works that make him succeed. After seeing his interesting films, I feel cheerful and excited, he brings happiness to us. Yes, I hope we can bring laughter to people too, just like Mr. Bean. I can&#39;t wait to see more his films after class. But first thing first, let&#39;s get our homework done.&quot;,&quot;quest_ans&quot;（提问问题的文本）: &quot;What makes Mr. Bean so successful?&quot;,&quot;lm&quot;（可能的正确回答；每个text表示一种正确的回答）: [    &#123;&quot;text&quot;: &quot;It&#39;s his giftedness and hard works that make him succeed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;is talent and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works makes Mr. Bean so successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His talent and hard work makes him successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His body language is so funny, he makes people laugh, feel happy and relaxed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;Hard work and giftedness.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and he is very hard working.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His gift and hard works.&quot;&#125;],&quot;key&quot;（关键点可能的表述方式；关键点对打分的影响很大）:    [[&quot;giftedness&quot;, &quot;gift&quot;, &quot;talent&quot;], &quot;hard work&quot;],&quot;unkey&quot;（错误答案，用户发音命中其中任一错误答案，对得分影响很大，得分会较低）:[&quot;no&quot;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="评分标准">7.3. 评分标准</h2><h3 id="广东高考">7.3.1. 广东高考</h3><p><a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> | | 权重 | 分档描述 ---|---|---信息 | 75% |1.5分：按照要求传递了信息；<br>1分：基本按照要求传递信息（漏了一、两点次要信息、或添加了无关信息）；<br>0分：不能按照要求传递信息语言 | 25% |0.5分：不影响理解所表达的信息；<br>0分：导致不能理解所表达的信息</p><h3 id="宁波中考">7.3.2. 宁波中考</h3><p><a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> 分值 | 评分标准 ---|--- 1 |意思明白，表达清楚，语音、语调正确，词语、语法合乎规范。 0.5 |意思基本明白，表达基本清楚，语音、语调基本正确，词语、语法有错误。 0 |答非所问，或错误很多，不能达意。</p><h3 id="其它版本">7.3.3. 其它版本</h3><ul><li>江苏中考（2档）<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>、深圳中考、衢州中考</li></ul><h3 id="维度分">7.3.4. 维度分</h3><p>驰声：内容、语法、发音、流利度</p><p>先声：完整度、发音、流利度，只建议展示总分</p><h1 id="故事复述">8. 故事复述</h1><p><a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p><h2 id="题型说明-1">8.1. 题型说明</h2><p>Retelling(故事复述)，要求考生先听一段大约两分钟的独白，录音播放两遍。考生准备一分钟之后开始复述所听的内容。要求考生尽可能使用自己的语言复述，而且复述内容应涵盖尽可能多的原文信息点。选取的独白其体裁主要以记述文和议论文为主。</p><h2 id="示例-1">8.2. 示例</h2><p><font style="background: yellow">（300词左右）</font></p><p><strong>A Young Man’s Present</strong></p><p>A young man who lived in London was in love with a beautiful girl.Soon she became his girlfriend. The man was very poor while the girl wasrich. The young man wanted to give her a present on her birthday. Hewanted to buy something beautiful for her, but he had no idea how to doit, as he had very little money. The next morning he went to a shop.There were many fine things: rings, gold watches, diamonds — but allthese things were too expensive. There was one thing he could not takehis eyes off. It was a beautiful vase. That was a suitable present forhis girlfriend. He had been looking at the vase for half an hour whenthe manager of the shop noticed him. The young man looked so pale, sadand unhappy that the manager asked what had happened to him.</p><p>The young man told him everything, The manager felt sorry for him anddecided to help him. He came up with a good idea. The manager pointed tothe corner of the shop. To his great surprise the young man saw a vasebroken into many pieces. The manager said: “I can help you. I shallorder my worker to pack it and take it to your girlfriend. When heenters the room, he will drop it.”</p><p>On the birthday of his girlfriend the young man was very excited.</p><p>Everything happened as had been planned. The worker brought in thevase, and as he entered the room, he dropped it. There was horror oneverybody’s face. When the vase was unpacked the guests saw that eachpiece was packed separately.</p><h2 id="评分标准-1">8.3. 评分标准</h2><table><thead><tr><th></th><th>内容</th><th>语言</th><th>流利</th><th>语音</th></tr></thead><tbody><tr><td>权重</td><td>50%</td><td>16.7%</td><td>20.8%</td><td>12.5%</td></tr><tr><td>考察点</td><td>原文信息点被覆盖的比例</td><td>语法</td><td></td><td>语音语调</td></tr></tbody></table><p>考生不按话题规定内容表述或套背内容毫不相干的范文：0分<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></p><h1 id="话题简述">9. 话题简述</h1><h1 id="信度效度">10. 信度、效度</h1><ul><li>信度：多位专家打分，分数是否一致<ul><li>人工评分平均相关度、平均误差：计算评测员1与其它评测员的平均分的相关度、平均误差，作为评测员1的评分性能；以此类推；取多名评测员的平均作为人工评分性能</li></ul></li><li>效度：分数能否真实反映学生水平</li></ul><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr><ol><li id="fn1"><p>http://www.hbea.edu.cn/html/2018-11/12162.html<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2"><p>https://baike.baidu.com/item/广东高考英语听说考试/2678957<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4"><p>https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5"><p>http://nbeea.nbedu.net.cn/ckfile/files/宁波市2022年初中学业水平考试英语听力口语自动化考试说明.pdf<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn6"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn7"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn8"><p>https://www.xfyun.cn/doc/Ise/IseAPI.html<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn9"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn10"><p>http://nbeea.nbedu.net.cn/ckfile/files/宁波市2022年初中学业水平考试英语听力口语自动化考试说明.pdf<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn11"><p>http://www.hbea.edu.cn/html/2018-11/12162.html<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn12"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn13"><p>https://www.xfyun.cn/doc/Ise/IseAPI.html<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn14"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn15"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn16"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn17"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn18"><p>https://www.xfyun.cn/doc/Ise/IseAPI.html<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn19"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn20"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn21"><p>https://open.singsound.com/doc/engine?type=engine-en-en.sent.score<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn22"><p>https://www.chivox.com/opendoc/#/ChineseDoc/coreEn<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn23"><p>https://baike.baidu.com/item/广东高考英语听说考试/2678957<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn24"><p>http://nbeea.nbedu.net.cn/ckfile/files/宁波市2022年初中学业水平考试英语听力口语自动化考试说明.pdf<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn25"><p>https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn26"><p>https://baike.baidu.com/item/广东高考英语听说考试/2678957<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn27"><p><a href="https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf">广西高考-2021（招生考试院）</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>英文口语评测功能</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/</url>
      
        <content type="html"><![CDATA[<h1 id="技术文档">1. 技术文档</h1><ul><li>科大讯飞：https://www.xfyun.cn/doc/Ise/IseAPI.html#接口说明</li><li>驰声：https://www.chivox.com/opendoc/#/ChineseDoc/coreEn/</li><li>先声：https://open.singsound.com/doc/engine?type=engine-en-en.word.score</li></ul><h1 id="题型">2. 题型</h1><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr><th>题型</th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音标朗读</td><td></td><td>√ 自定义文本音标</td><td>√</td></tr><tr><td>单词朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>单词纠音（音素识别）</td><td></td><td>√</td><td>√</td></tr><tr><td>句子朗读</td><td>√</td><td>√</td><td>√ 支持音频对比，返回音量、语调、语速相关度得分</td></tr><tr><td>句子纠音</td><td></td><td>√</td><td></td></tr><tr><td>篇章朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>背诵</td><td></td><td>√ 篇章朗读题型背诵模式，要求严格按顺序朗读</td><td>√</td></tr><tr><td>句子选读</td><td></td><td></td><td>√ 返回实际朗读的是第几个句子</td></tr><tr><td>自然拼读</td><td></td><td></td><td>√ <font style="background: green">（文本形式不明确）</font></td></tr><tr><td>选择</td><td>√</td><td>√ 支持单选、多选</td><td>√<br>支持扩展选择题：用户可以在事先设定的固定答案基础上扩展表述；引擎检测到读的更像哪个答案，就会有对应的得分。支持设置错误关键词。<br>支持设置解码网络：常规、精简（解码速度快，但效果下降；当参考文本较多时，可以设置）</td></tr><tr><td>问答</td><td>√</td><td>√</td><td>√ 支持设置错误关键词</td></tr><tr><td>复述、口头翻译、看图说话、口头作文</td><td>√</td><td>√</td><td>√</td></tr><tr><td>自由识别</td><td></td><td>√</td><td></td></tr></tbody></table><h1 id="功能配置">3. 功能配置</h1><table style="width:100%;"><colgroup><col style="width: 22%"><col style="width: 22%"><col style="width: 33%"><col style="width: 22%"></colgroup><thead><tr><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>区分英美式发音</td><td></td><td>支持K12词汇</td><td>√</td></tr><tr><td>自定义音标</td><td>√ 支持指定数字的读法</td><td>√</td><td>√</td></tr><tr><td>集外词</td><td></td><td>√</td><td>√</td></tr><tr><td>人群定制</td><td>传入group（成人、中学、小学）、grade（年级，junior、middle、senior）</td><td>自适应少儿、成人群体</td><td>儿童单词、句子为单独的题型；看图说话、复述支持设置，影响打分松紧度</td></tr><tr><td>松紧调节</td><td>仅中文评测支持，3档</td><td>线性调节</td><td>朗读题 5档；问答题 0.8-1.5线性调节</td></tr><tr><td>实时评测</td><td></td><td>√</td><td>√</td></tr></tbody></table><h1 id="评测结果">4. 评测结果</h1><table><colgroup><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr><th></th><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音频级</td><td>总分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>发音准确度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>流畅度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>标准度/韵律分</td><td>√</td><td>意群（sense）、重读、升降调</td><td>√ 意群、重读、升降调占比分别为50%、25%、25%</td></tr><tr><td></td><td>完整度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>语法分</td><td></td><td>√</td><td></td></tr><tr><td></td><td>识别文本</td><td>自由表述题</td><td>句子纠音：若错读、增读的单词在参考文本内，正常识别，否则标记为unk。<br>自由识别：识别文本带标点符号，支持逗号、句号、问号、感叹号</td><td></td></tr><tr><td></td><td>关键词/要点命中</td><td></td><td>√</td><td>√</td></tr><tr><td>句子级</td><td>句末升降调检错</td><td></td><td>√</td><td>√</td></tr><tr><td>词级</td><td>评分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td>√ 不区分回读</td><td>正确、漏读、回读</td></tr><tr><td></td><td>浊化</td><td></td><td>√</td><td></td></tr><tr><td></td><td>连读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>失去爆破</td><td></td><td>√</td><td></td></tr><tr><td></td><td>重读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>意群停顿</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>字母-音素对应</td><td></td><td>√</td><td>√</td></tr><tr><td>音节级</td><td>评分</td><td>√</td><td></td><td>√</td></tr><tr><td></td><td>发音检错</td><td>√</td><td></td><td></td></tr><tr><td></td><td>重音检错</td><td>√ 检测重读音节是否重读</td><td>√ 检测音节是否重读</td><td>√</td></tr><tr><td>音素级</td><td>评分</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音检错</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音诊断</td><td></td><td>√ 音素识别</td><td></td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td></td><td></td></tr></tbody></table><ul><li>音标、单词朗读仅发音准确度维度</li><li>标准度/韵律分：科大讯飞：文本单词数&gt;=5时才有</li><li>语法分：仅自由表述题有</li><li>驰声<ul><li>选择题、AITalk：返回置信度得分，可由应用层根据题目难易设置阈值（通常为75）判断结果是否正确</li></ul></li><li>先声<ul><li>句子朗读：统计各音素出现的次数、平均发音得分</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 产品 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>小妇人</title>
      <link href="/blog/books/xiao-fu-ren/"/>
      <url>/blog/books/xiao-fu-ren/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 书籍 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>和平战士</title>
      <link href="/blog/videos/he-ping-zhan-shi/"/>
      <url>/blog/videos/he-ping-zhan-shi/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 视频 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>定风波</title>
      <link href="/blog/words/ding-feng-bo/"/>
      <url>/blog/words/ding-feng-bo/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 文字 </category>
          
          <category> 诗词 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch</title>
      <link href="/blog/ji-qi-xue-xi/kuang-jia/pytorch/"/>
      <url>/blog/ji-qi-xue-xi/kuang-jia/pytorch/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 框架 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CTC</title>
      <link href="/blog/ji-qi-xue-xi/loss/ctc/"/>
      <url>/blog/ji-qi-xue-xi/loss/ctc/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> loss </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>音标</title>
      <link href="/blog/yin-pin/yu-yin-xue/yin-biao/"/>
      <url>/blog/yin-pin/yu-yin-xue/yin-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="符号表">1. 符号表</h1><h2 id="辅音">1.1. 辅音</h2><table><thead><tr><th>辅音</th><th>单词</th><th>音标</th></tr></thead><tbody><tr><td>p</td><td>pen</td><td>/pen/</td></tr><tr><td>b</td><td>bad</td><td>/bæd/</td></tr><tr><td>t</td><td>tea</td><td>/tiː/</td></tr><tr><td>d</td><td>did</td><td>/dɪd/</td></tr><tr><td>k</td><td>cat</td><td>/kæt/</td></tr><tr><td>ɡ</td><td>get</td><td>/ɡet/</td></tr><tr><td>tʃ</td><td>chain</td><td>/tʃeɪn/</td></tr><tr><td>dʒ</td><td>jam</td><td>/dʒæm/</td></tr><tr><td>f</td><td>fall</td><td>/fɔːl/</td></tr><tr><td>v</td><td>van</td><td>/væn/</td></tr><tr><td>θ</td><td>thin</td><td>/θɪn/</td></tr><tr><td>ð</td><td>this</td><td>/ðɪs/</td></tr><tr><td>s</td><td>see</td><td>/siː/</td></tr><tr><td>z</td><td>zoo</td><td>/zuː/</td></tr><tr><td>ʃ</td><td>shoe</td><td>/ʃuː/</td></tr><tr><td>ʒ</td><td>vision</td><td>/ˈvɪʒn/</td></tr><tr><td>h</td><td>hat</td><td>/hæt/</td></tr><tr><td>m</td><td>man</td><td>/mæn/</td></tr><tr><td>n</td><td>now</td><td>/naʊ/</td></tr><tr><td>ŋ</td><td>sing</td><td>/sɪŋ/</td></tr><tr><td>l</td><td>leg</td><td>/leɡ/</td></tr><tr><td>r</td><td>red</td><td>/red/</td></tr><tr><td>j</td><td>yes</td><td>/jes/</td></tr><tr><td>w</td><td>wet</td><td>/wet/</td></tr></tbody></table><h2 id="元音">1.2. 元音</h2><table style="width:100%;"><colgroup><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"><col style="width: 7%"></colgroup><thead><tr><th>牛津</th><th>单词</th><th>音标</th><th>备注</th><th>朗文</th><th>-</th><th>-</th><th>-</th><th>cambridge</th><th>-</th><th>-</th><th>-</th><th>-</th></tr></thead><tbody><tr><td>ʌ</td><td>cup</td><td>/kʌp/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɑː</td><td>father</td><td>/ˈfɑːðə(r)/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɒ</td><td>got</td><td>/ɡɒt/</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔː</td><td>saw</td><td>/sɔː/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ə</td><td>about</td><td>/əˈbaʊt/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɜː</td><td>fur</td><td>/fɜː(r)/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪ</td><td>sit</td><td>/sɪt/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>i</td><td>happy</td><td>/ˈhæpi/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>iː</td><td>see</td><td>/siː/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊ</td><td>put</td><td>/pʊt/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>u</td><td>actual</td><td>/ˈæktʃuəl/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>uː</td><td>too</td><td>/tuː/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>e</td><td>bed</td><td>/bed/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>æ</td><td>cat</td><td>/kæt/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aɪ</td><td>my</td><td>/maɪ/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eɪ</td><td>say</td><td>/seɪ/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔɪ</td><td>boy</td><td>/bɔɪ/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aʊ</td><td>now</td><td>/naʊ/</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>əʊ</td><td>go</td><td>/ɡəʊ/</td><td></td><td>British English</td><td>oʊ</td><td>note</td><td>American English</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪə</td><td>near</td><td>/nɪə(r)/</td><td>British English</td><td><del>British English</del></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eə</td><td>hair</td><td>/heə(r)/</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊə</td><td>pure</td><td>/pjʊə(r)/</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>ɒː</td><td>dog</td><td>American English</td><td>ɚ</td><td>mother</td><td></td><td>American English，轻音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>iə</td><td>peculiar</td><td></td><td>ɝ</td><td>worm</td><td></td><td>American English，重读音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>t̬</td><td>butter</td><td>/ˈbʌt̬.ɚ/</td><td>American English</td><td>[ɾ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>l̩</td><td>little</td><td>/ˈlɪt.l̩/</td><td></td><td>[ɫ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>ə</sup>l,<sup>ə</sup>m,<sup>ə</sup>n</td><td></td><td>/leɪb.<sup>ə</sup>l/</td><td></td><td>(ə)l …</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>r</sup></td><td>four apples</td><td>fɔː<sup>r</sup> + ˈæp.l ̩z = fɔːˈræp.l ̩z</td><td>British English，在元音前时才发音</td><td>(r)</td></tr></tbody></table><h2 id="备注">1.3. 备注</h2><ul><li><p>/i/可以发成/iː/或/ɪ/或两者之间折中的音；/u/代表/uː/和/ʊ/之间的弱元音</p><ul><li>存在单词，含有两个仅/iː/、/ɪ/差异（或仅/uː/、/ʊ/差异）的音标，但不合并为/i/（或/u/），如real /ˈriːəl/、/rɪəl/ （room /ruːm/ /rʊm/）</li><li>/iə/、/uə/：区别于/ɪə/、/ʊə/，如 alien /ˈeɪ-li-ən/、video/ˈvɪ-di-əʊ/，actual /ˈæk-tʃu-əl/、continuo/kən-ˈtɪn-ju-əʊ/，https://english.stackexchange.com/questions/433103/what-is-i%c9%99-in-english#</li></ul></li><li><p>/ɒ/只出现在英式英语中，美式英语的发音通常是/ɔː/或/ɑː/</p></li><li><p>(r)：只有紧跟的是下一个单词开头的元音时，英式发音才会出现/r/，如faraway；否则省略/r/。对于美式英语来说，所有的/r/都应该发音。</p></li><li><p>/l̩/ 、/n̩/、(/m̩)/：成音节，如final /ˈfaɪnl/，发音为[<sup>ə</sup>l]or [<sup>ə</sup>n]</p></li><li><p>音位变体</p><ul><li><p>/t/音素还包含闪音[ɾ]和glottal stop [ʔ]。</p><p>[ɾ]发音像快速的/d/，美式发音，在很多拼写为-t-或-tt-的单词中，在元音或/r/之后、非重读元音或音节/l/之前，如city/ˈsɪt̮ɪ /; parting /ˈpɑrt̮ɪŋ /; little /ˈlɪt̮l /；</p><p>英美式发音有时会用glottal stop [ʔ](声带短暂的闭合)来表达/t/，比如football /ˈfʊtbɔːl/和button/ˈbʌtn/。</p></li><li><p>/l/在元音之前或中间时（如 like）与在其他位置时（如 full [ɫ]）发音不同</p></li><li><p>/r/ [ɹ]如red</p></li></ul></li><li><p>/x/：摩擦音，如苏格兰的loch、爱尔兰的lough /lɒx/</p></li><li><p>˜：鼻元音，可能在某些源自法语的单词中保留，如penchant/ˈpɒ̃ʃɒ̃/</p></li><li><p>有的发音标注了强、弱形式，给出的第一个发音通常代表最常用的；但是当一个单词被强调时，应该采用strongform；当单词位于句子末尾时，也通常使用strong form。如can /kən/, strongform /kæn/</p></li><li><p>重读音节相对loud、持续时间长、发音清晰，并且可以通过音调被注意到。重读音节通常不包含弱元音/ə/、/i/或/u/。</p></li><li><p>英语音素 48 vs 44 &gt;传统语音学认为：英语有48个音素。一个音素对应一个音标，所以共有48个国际音标。这也是我们国内更加熟知的一种音标体系。&gt; &gt; 现代语音学认为：英语有44个音素。因为现代语音学认为 /tr/ /dr//ts/ /dz/ 这四个不是独立的音素，而是辅音连缀。 &gt; &gt;https://zhuanlan.zhihu.com/p/31484071</p><blockquote><p>/ts/、/dz/、/tr/、/dr/ 如果作为辅音连缀简单地连读，影响正确发音</p><p>https://www.hjenglish.com/yinbiao/p776611/</p></blockquote><ul><li>/tr/、/dr/<ul><li>分开读，连读影响重音：如 outrun /ˌaʊtˈrʌn/</li><li>连读：如 restroom /ˈrestruːm/、outrage /ˈaʊtreɪdʒ/、bedroom/ˈbedruːm/、handwriting /ˈhændraɪtɪŋ/</li></ul></li><li>/ts/、/dz/<ul><li>分开读，在不同音节：如 itself /ɪtˈself/、outside/ˌaʊtˈsaɪd/、outset/ˈaʊtset/、Watson /ˈwɑːtsən/、curtsy /ˈkɜːtsi/</li><li>连读，在同一音节：如 Pittsburg /ˈpɪtsˌbərg/、sportsman/ˈspɔːtsmən/、swordsman /ˈsɔːdzmən/</li><li>yangtze /ˈjæŋtsi/、Lindsey /ˈlɪndzi/</li></ul></li></ul></li></ul><h2 id="参考">1.4. 参考</h2><ul><li>https://www.oxfordlearnersdictionaries.com/about/english/pronunciation_english</li><li>https://www.oxfordlearnersdictionaries.com/about/pronunciation_american_english</li><li>https://www.ldoceonline.com/howtouse.html</li><li>https://dictionary.cambridge.org/help/phonetics.html</li><li>https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/英語國際音標</li><li>https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/DJ音標</li><li>https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/KK音標</li><li>音素、音标、国际音标（IPA）、DJ音标、KK音标、宽式标音等https://en-yinbiao.xiao84.com/study</li><li>音标特殊字符unicode编码：http://www.fmddlmyy.cn/text65.html（注意ɡ、ː、ˈ、ˌ）</li></ul><h1 id="词典">2. 词典</h1><ul><li><p>主流英语词典：朗文、牛津、剑桥、柯林斯、韦式https://www.jianshu.com/p/26d12a32f048</p></li><li><p>朗文交际9000词https://github.com/MuhammadYaseenKhan/Longman-Communication</p></li><li><p>韦式词典</p><p>https://www.merriam-webster.com/assets/mw/static/pdf/help/guide-to-pronunciation.pdf</p><p>https://mdx.mdict.org/六大知名词典/韦氏_Merriam-Webster/Merriam-Webster/Merriam-Webster/</p></li><li><p>数据堂英语发音词典：https://m.datatang.com/news/info/aboutus/451</p></li></ul><h1 id="arpabet音素集">3. ARPAbet音素集</h1><figure><img src="https://note.youdao.com/yws/api/personal/file/WEB3fda55097eb7eb054863211dc53ea73d?method=download&amp;shareKey=fecd6d9867afa9c0ab9874d271e9ce46" alt="wiki"><figcaption aria-hidden="true">wiki</figcaption></figure><p>元音区分是否重读，重音标记：0表示非重音，1表示主重音，2表示次重音</p><h2 id="arpabet-to-ipa">3.1. arpabet-to-ipa</h2><p>https://github.com/wwesantos/arpabet-to-ipa/blob/master/src/App.php</p><h2 id="与48个音素的区别">3.2. 与48个音素的区别</h2><ul><li>无短元音/ɒ/，美式发音中多为/ɑː/或/ɔː/，如lot、long</li><li>无双元音/ɪə/、/eə/、/ʊə/，用两个单元音表示</li><li>无4个辅音连缀，用两个音标表示</li><li>/ʌ/、/ə/共用符号AH，美式英语中常为多发音，如but</li></ul><h2 id="cmucarnegie-mellon-university词典">3.3. CMU(Carnegie MellonUniversity)词典</h2><p>http://www.speech.cs.cmu.edu/cgi-bin/cmudict/</p><p>CMU词典用于北美英语，采用ARPAbet音素集，元音区分是否重读</p><p>39个音素：</p><p>元音(15)：AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH,UW</p><p>辅音(24)：B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, P, R, S, SH, T,TH, V, W, Y, Z, ZH</p><h1 id="发音学习视频">4. 发音学习视频</h1><ul><li>《BBC音标教程》Alex https://www.bilibili.com/video/BV127411n7nj</li><li>《英语语音》屠蓓 https://www.bilibili.com/video/BV1EP4y1p7p3</li><li>《美语从头学：美语音标》赖世雄https://www.bilibili.com/video/BV1qo4y1S7tY/</li><li>英语兔 https://space.bilibili.com/483162496/channel/series</li></ul><h1 id="频谱">5. 频谱</h1><p>https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html</p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GOP</title>
      <link href="/blog/yin-pin/yu-yin-ping-ce/gop/"/>
      <url>/blog/yin-pin/yu-yin-ping-ce/gop/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音评测 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>序列区分性训练</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>解码器</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/jie-ma-qi/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/jie-ma-qi/</url>
      
        <content type="html"><![CDATA[<h1 id="参数">1. 参数</h1><ul><li>beam: 每一帧保留的token的cost与当前帧最优cost的最大允许差值。越大，解码越慢，内存占用越大，但越准确。默认值16.0<font color="green">（即路径概率约为1.1e-07= 1/exp(16) 倍）</font>。</li><li>max_active:每一帧可以激活的最大状态数。越大，解码越慢，内存占用越大，但越准确。默认值7000。</li><li>min_active: 每一帧需要激活的最小状态数。默认值200。</li><li><a href="#lattice_beam">lattice_beam</a>: LatticeDecoder中，跳转弧到达当前最后一帧active tokenA的最优路径的cost与到达A的最优路径的cost的最大允许差值（对于当前最后一帧的多个activetokens，其一满足即可）。默认值6.0<font color="green">（即路径概率约为0.0025= 1/exp(6) 倍）</font>。</li><li><a href="#beam_delta">beam_delta</a>:FasterDecoder中，用于根据前一帧估计当前帧的<code>adaptive_beam</code>。默认值0.5。</li><li>prune_interval: LatticeDecoder中，每隔多少帧裁剪过去帧的tokens。默认值20。</li><li><a href="#prune_scale">prune_scale</a>: LatticeDecoder中，若token的extracost变大（假设上一次PruneActiveTokens时，该token计算extracost对应的是当时最后一帧的tokenA，当A不在到达当前最后一帧的最优路径上时，该token的extracost就会变大）（变化量，而非值）超过<code>lattice_beam * prune_scale</code>，则需要判断之前的帧是否需要裁剪。越大，则向前回溯得越少（内存与解码速度的均衡）。默认值0.1。</li><li><a href="#hash_ratio">hash_ratio</a>：FasterDecoder中，用于设置存储tokens的哈希表的容量。默认值2。</li><li>length_penalty：LatticeFasterDecoder中，每跳转到一个新的状态（排除了状态自旋、epsilon弧），graphcost加上length_penalty。&gt;0则倾向于短句子，&lt;0时倾向于长句子。默认值0.0。</li></ul><h1 id="数据结构">2. 数据结构</h1><ul><li>DecodableInterface:为了最小化解码器和声学模型代码之间的交互，创建了该基类。DecodableInterface对象可以视为LogLikelihood矩阵。<ul><li>BaseFloat LogLikelihood(int32 frame, int32 index): 返回索引(frame,index)对应的声学log-likelihood。若已计算，则直接返回值；否则调用声学模型计算。可能还包含减log_priors、乘acoustic_scale等处理。<ul><li>acoustic_scale：默认值0.1。</li></ul></li><li>int32 NumFramesReady():根据已计算的声学特征，可得到的LogLikelihood帧数（声学模型可能包含降采样、特征上下文等处理）。</li><li>bool IsLastFrame(int32 frame):是否为最后一帧（若为online模式，则须输入结束）。</li></ul></li><li>Decoder<ul><li>构造decoder需要传入fst和相关配置参数。</li></ul></li><li>Token（以下主要以Lattice Decoder为例）<ul><li>每一帧、每个激活的状态各由一个Token表示。</li><li>BaseFloat tot_cost: 从句子开头到当前状态的AM + LM cost (ac_cost +graph_cost)。<ul><li>vector&lt;BaseFloat&gt; cost_offsets_: 每一帧，AM log-likelihoods加的 offset，使其接近0，减小roundoff的误差。</li></ul></li><li>BaseFloat extra_cost: 认为当前最后一帧的各activetoken都有可能在最终的最优路径上，extra_cost均为0；其它帧token的extracost为：经过该token到达当前最后一帧activetoken（计为A）的最优路径，与到达A的最优路径的cost差值；对于当前最后一帧的多个activetokens，取这些cost差值的最小值。在PruneForwardLinks(Final)中计算。</li><li>ForwardLink<stdtoken> *links:该token到下一帧的跳转弧（或跳转到当前帧的epsilon弧）的单链表<ul><li>fst::StdArc::Label ilabel</li><li>fst::StdArc::Label olabel</li><li>BaseFloat acoustic_cost: cost_offset - logLikelihood</li><li>BaseFloat graph_cost:arc.weight，包含状态转移模型概率、词典发音概率、静音概率、LM概率等</li><li>Token *next_tok：跳转到的下一个token</li><li>ForwardLink *next：单链表结构</li></ul></stdtoken></li><li>Token *backpointer:到达当前token的最优路径上的上一token。用于LatticeFasterOnlineDecoder中高效地GetBestPath。</li><li>Token *next: 该帧的下一token</li><li>非Lattice Decoder中还包含：<ul><li>int32 ref_count_:该token跳转到的token的数量+1。当该token不再在到达当前最后一帧的各activetoken的最优路径上时（ref_count==1），删除到达该token的路径上的所有无用token（TokenDelete）。</li><li>Token *prev_: 路径中的上一个token，唯一，用于回溯。<ul><li>LatticeDecoder记录路径中的下一token，链表，多个。<font color="green">更高效</font>。GetBestPath中，先顺序遍历每一帧的activetokens 生成lattice，再采用 openfst 的 ShortestPath 返回最优路径。</li></ul></li><li>arc_.nextstate 当前状态</li></ul></li></ul></li></ul><h1 id="解码器类型">3. 解码器类型</h1><h2 id="simpledecoder">3.1. SimpleDecoder</h2><ul><li>采用<code>unordered_map&lt;StateId, Token*&gt; cur_toks_, prev_toks_</code>分别存储当前帧和前一帧的active tokens。</li><li>Viterbi beam search。</li></ul><p>核心流程如下： <pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (num_frames_decoded_ &lt; target_frames_decoded) &#123;    &#x2F;&#x2F; note: ProcessEmitting() increments num_frames_decoded_    ClearToks(prev_toks_);  &#x2F;&#x2F; 删除历史无用路径，并清空prev_toks_    cur_toks_.swap(prev_toks_);    ProcessEmitting(decodable);  &#x2F;&#x2F; 处理发射弧，将token从上一帧传播到当前帧    ProcessNonemitting();  &#x2F;&#x2F; 处理当前帧token的epsilon弧    PruneToks(beam_, &amp;cur_toks_);  &#x2F;&#x2F; 再次裁剪（因为ProcessEmitting中的cutoff是动态更新的）&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="fasterdecoder">3.2. FasterDecoder</h2><ul><li>采用<code>HashList&lt;StateId, Token*&gt; toks_</code>存储tokens，其元素可以通过哈希表访问，也可以通过单链表访问。token传播时，先清空哈希表，前一帧的activetokens仅可以通过单链表访问，当前帧的active tokens还可以通过哈希表访问。<ul><li>创建新token时，需要查询对应的状态是否已存在token，相较于unordered_map的遍历查找，采用哈希表查找速度更快。</li></ul></li><li>采用<code>max-active</code>、<code>min-active</code> 调整裁剪。</li></ul><p>核心流程如下： <pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (num_frames_decoded_ &lt; target_frames_decoded) &#123;    double weight_cutoff &#x3D; ProcessEmitting(decodable);    ProcessNonemitting(weight_cutoff);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="biglmfasterdecoder">3.3. BiglmFasterDecoder</h2><ul><li>大语言模型：弧数&gt;百万。由于内存限制，很难构建解码图。</li><li><font color="red"> 两种处理方式 </font><ul><li>先采用小LM生成lattice，再用大LM对lattice进行rescore。（小、大指ngram阶数）</li><li>采用“biglm” decoder：先采用小LM <code>G</code>构建HCLG，解码时动态地compose 大LM <span class="math inline">\(G^{\prime}\)</span> 与小LM的差。<span class="math inline">\(HCLG \circ G^{-} \circ G^{\prime}\)</span>，其中<span class="math inline">\(G^{-}\)</span>为对<code>G</code>中的权重取负数得到。<ul><li>实现：未采用通用的compose算法，而是采用 <span class="math inline">\((HCLG\text {中的状态}, G^{-} \circ G^{\prime}\text{中的状态})\)</span> 状态空间，查找指定状态上具有指定输入标签的弧。</li><li>缺点：解码较慢。<font color="green">相较于beam相同、但没有biglm部分的解码器，biglm decoder 耗时几乎为2倍(疑问：对比时，普通解码器LM大小与biglmdecoder中的<code>G</code>相当，还是与<span class="math inline">\(G^{\prime}\)</span>相当？)</font>。</li></ul></li></ul></li><li>效果：大语言模型的HCLG &gt; biglm decoder &gt; rescore。<ul><li>rescore：采用小LM生成lattice时最优路径有可能被裁剪掉。</li><li>biglm decoder：<font color="green"> only updates the 'good' languagemodel score every time it crosses a word（ChatGPT:只有在完整的单词被解码时才会更新语言模型分，可以通过减少语言模型需要被评估的次数来加速解码，但是，与在每个声学帧或每个子单元上更新语言模型分相比，可能会导致稍微不太准确的解码结果。）</font></li></ul></li></ul><h2 id="lattice-decoder">3.4. Lattice Decoder</h2><ul><li>对比<ul><li>one-best解码：只保留到达active状态的最优路径。</li><li>lattice解码：若存在多条可行路径到达同一active状态，均保留。</li></ul></li><li>采用<code>vector&lt;TokenList&gt; active_toks_</code>记录每一帧的activetokens链表头节点。</li><li>采用lattice beam进一步裁剪。</li></ul><p>核心流程如下： <pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">InitDecoding();while (NumFramesDecoded() &lt; target_frames_decoded) &#123;    if (NumFramesDecoded() % config_.prune_interval &#x3D;&#x3D; 0)      PruneActiveTokens(config_.lattice_beam * config_.prune_scale);    double weight_cutoff &#x3D; ProcessEmitting(decodable);    ProcessNonemitting(weight_cutoff);&#125;FinalizeDecoding();  &#x2F;&#x2F; 可选，采用lattice beam进行裁剪<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="latticefasteronlinedecoder">3.5. LatticeFasterOnlineDecoder</h2><ul><li>支持在使用endpointing或其它需要频繁获取最佳路径的场景下，利用backpointer高效计算GetBestPath()。</li></ul><h2 id="latticeincrementaldecoder">3.6. LatticeIncrementalDecoder</h2><ul><li>普通解码器LatticeFasterDecoder，用于长语音实时应用时，每次获取lattice时的lattice确定化可能会花费相当多的时间，从而引入延迟。LatticeIncrementalDecoder将lattice确定化分散到整个解码过程中。</li><li><font color="green">TODO: 实现</font></li><li>术语<ul><li>chunk：对 chunk帧原始lattice分别进行确定化。长度至少为1个单词，如20帧。</li></ul></li></ul><h1 id="代码解析">4. 代码解析</h1><h2 id="decodablenumframesready">4.1. Decodable::NumFramesReady()</h2><p>由于声学模型输入特征可能包含上下文，音频中间和末尾的NumFramesReady计算方式可能不一样。</p><h2 id="cost_cutoff-processemittingdecodable">4.2. cost_cutoff =ProcessEmitting(decodable)</h2><p>将令牌从前一帧传播到当前帧。</p><ul><li>GetCutoff：计算上一帧的cutoff<font color="red">(cur_cutoff)</font>。<ul><li>返回值：同时满足<code>beam</code>、<code>max_active</code>、<code>min_active</code>约束的cost临界值。</li><li>adaptive_beam: 自适应估计的有效beamwidth。若根据<code>max_active</code>或<code>min_active</code>调整了beam，则为同时满足上述约束的<span class="math inline">\(beam^{\prime}\)</span> + <span id="beam_delta"><code>beam_delta</code></span>，否则为<code>beam</code>。该值通常 &gt;=实际有效的beam width，认为采用beam_delta=0.5（默认值）时该估计值比实际值更严格的情况不会经常发生。</li></ul></li><li><span id="hash_ratio">PossiblyResizeHash(tok_cnt)：若上一帧的token数<code>tok_cnt</code><code>* hash_ratio &gt; toks_.Size()</code>，<code>toks_</code>扩容。</span></li><li>初始化当前帧的cutoff<font color="red">(next_cutoff)</font>：取上一帧的beststate，对其在fst上的各发射弧，计算cost，cutoff取上述cost的最小值 +adaptive_beam。<ul><li>cost_offsets_[frame] = - best_elem-&gt;val-&gt;tot_cost (best_elem:上一帧的)</li></ul></li><li>对于前一帧的各active token (tok)，若满足不被裁剪的条件<font color="red">（cur_cutoff）</font>，处理其对应状态的各发射弧（即输入标签非0- epsilon），若满足不被裁剪的条件<font color="red">（next_cutoff）</font>，则FindOrAddToken，并将新ForwardLink添加至tok-&gt;links头部。动态更新当前帧cutoff。<ul><li>FindOrAddToken：若当前帧已存在与该状态关联的token，判断是否更新tot_cost、backpointer。</li></ul></li></ul><h2 id="processnonemittingcost_cutoff">4.3. ProcessNonemitting(cost_cutoff)</h2><ul><li>对于当前帧满足cost_cutoff约束的各token，处理其对应状态的各非发射弧（即输入标签为0-epsilon）。若仍满足cost_cutoff约束，则FindOrAddToken，并将新ForwardLink添加至tok-&gt;links头部。</li><li>采用队列（queue）遍历含epsilon弧的token。若FindOrAddToken中新添加的token或 更新了tot_cost的token 也含Epsilon弧，也加入queue。<ul><li>DeleteForwardLinks(tok)：若 tok 存在 ForwardLinks，则一定为 epsilon弧，且再次在queue中，则 tot_cost减小了，因此DeleteForwardLinks，并重新处理其所有的 epsilon 弧。</li></ul></li></ul><h2 id="pruneactivetokens">4.4. PruneActiveTokens</h2><p>逆序遍历每一帧（不包含当前帧），如果该帧还未被该函数访问过(newTokenList)，或者下一帧存在token extra_cost变化超过<span id="prune_scale"> delta （= config_.lattice_beam *config_.prune_scale）</span>，对该帧token的forward link进行裁剪（若link_extra_cost &gt; <span id="lattice_beam">config_.lattice_beam</span>），并删除下一帧 forward link 均被裁剪了的token。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">&#x2F;&#x2F; delta &#x3D; config_.lattice_beam * config_.prune_scalevoid PruneActiveTokens(BaseFloat delta) &#123;    int32 cur_frame_plus_one &#x3D; NumFramesDecoded();    &#x2F;&#x2F; 保留当前最后一帧的所有token，认为其均有可能走到最后(extra cost均为0)。    for (int32 f &#x3D; cur_frame_plus_one - 1; f &gt;&#x3D; 0; f--) &#123;        &#x2F;&#x2F; TokenList初始化时must_prune_forward_links默认为true        &#x2F;&#x2F; 或者，下一帧存在token extra_costs_changed        if (active_toks_[f].must_prune_forward_links) &#123;            bool extra_costs_changed &#x3D; false, links_pruned &#x3D; false;            PruneForwardLinks(f, &amp;extra_costs_changed, &amp;links_pruned, delta);            if (extra_costs_changed &amp;&amp; f &gt; 0)  &#x2F;&#x2F; 存在token extra_cost变化超过delta                active_toks_[f-1].must_prune_forward_links &#x3D; true;            if (links_pruned)  &#x2F;&#x2F; 存在link被裁剪                active_toks_[f].must_prune_tokens &#x3D; true;            active_toks_[f].must_prune_forward_links &#x3D; false;        &#125;        &#x2F;&#x2F; 必须先裁剪掉前一帧的forward links，再裁剪token，从而避免“悬挂”的forward links        if (f+1 &lt; cur_frame_plus_one &amp;&amp; active_toks_[f+1].must_prune_tokens) &#123;            &#x2F;&#x2F; 删除extra_cost为无穷的token            PruneTokensForFrame(f+1);            active_toks_[f+1].must_prune_tokens &#x3D; false;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>PruneForwardLinks:</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">&#x2F;&#x2F; 遍历该帧所有token，若extra_cost **变化** 均不超过delta，循环停止。&#x2F;&#x2F; 由于存在epsilon弧，而链表不保证拓扑顺序，所以需要while循环。while (changed) &#123;    changed &#x3D; false;    &#x2F;&#x2F; 对该帧的所有token    for (Token *tok &#x3D; active_toks_[frame_plus_one].toks; tok !&#x3D; NULL; tok &#x3D; tok-&gt;next) &#123;        ForwardLinkT *link, *prev_link &#x3D; NULL;        BaseFloat tok_extra_cost &#x3D; std::numeric_limits&lt;BaseFloat&gt;::infinity();        &#x2F;&#x2F; 对该token的所有link        for (link &#x3D; tok-&gt;links; link !&#x3D; NULL; ) &#123;            Token *next_tok &#x3D; link-&gt;next_tok;            &#x2F;&#x2F; a &#x3D; tok-&gt;tot_cost + link-&gt;acoustic_cost + link-&gt;graph_cost 经过该link到达next_tok的tot_cost            &#x2F;&#x2F; b &#x3D; a - next_tok-&gt;tot_cost 上述tot_cost与到达next_tok的最优路径的cost差值（不会随着后续帧变化）            &#x2F;&#x2F; link_extra_cost &#x3D; next_tok-&gt;extra_cost + b 经过该link到达当前最后一帧active token（计为A）的最优路径，与到达A的最优路径的cost差值            BaseFloat link_extra_cost &#x3D; next_tok-&gt;extra_cost + ((tok-&gt;tot_cost + link-&gt;acoustic_cost + link-&gt;graph_cost) - next_tok-&gt;tot_cost);            &#x2F;&#x2F; if ... 则删除该link            if (link_extra_cost &gt; config_.lattice_beam) &#123;                ForwardLinkT *next_link &#x3D; link-&gt;next;                if (prev_link !&#x3D; NULL) prev_link-&gt;next &#x3D; next_link;                else tok-&gt;links &#x3D; next_link;                delete link;                link &#x3D; next_link;                *links_pruned &#x3D; true;            &#125; else &#123;                if (link_extra_cost &lt; tok_extra_cost)                    tok_extra_cost &#x3D; link_extra_cost;                prev_link &#x3D; link;                link &#x3D; link-&gt;next;            &#125;        &#125;        &#x2F;&#x2F; tok_extra_cost: 若该token的所有forward link均被裁剪，则为无穷大；否则为link_extra_cost的最小值        &#x2F;&#x2F; 若存在token extra_cost变化量超过delta，则changed为true        if (fabs(tok_extra_cost - tok-&gt;extra_cost) &gt; delta)            changed &#x3D; true;        tok-&gt;extra_cost &#x3D; tok_extra_cost;    &#125;    if (changed) *extra_costs_changed &#x3D; true;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>通常每一帧有数千个activetokens，因此定期（prune_interval）采用lattice_beam进行裁剪，而不是等到话语结束才裁剪，否则会OOM。</li><li>裁剪算法的时间复杂度接近线性，而不是<span class="math inline">\(O(T^2)\)</span>。对于非常长的语句，通常仅向后裁剪1-2s。另外，绝大多数token在该算法第一次访问时就被删除了。</li><li>https://kaldi-asr.org/doc/lattices.html#lattices_generation</li></ul></blockquote><h2 id="finalizedecoding">4.5. FinalizeDecoding</h2><p>采用 lattice beam 进行裁剪。与 PruneActiveTokens 的区别： +PruneForwardLinksFinal：最后一帧特殊处理。 +<font color="red">若存在token在final状态，则不在final状态的tokenextra_cost 为无穷大；否则，不考虑final权重。</font> + 若link_extra_cost（epsilon弧） &gt; lattice_beam，则删除该link。 + token的 extra cost 为 经过该token的最优路径 与 全局最优路径 的cost差值。若&gt; lattice_beam，则设为无穷大。 + 逆序遍历、处理所有帧，不需考虑delta、must_prune_tokens 等条件。</p><h2 id="getbestpath">4.6. GetBestPath</h2><ul><li><font color="red">参数：use_final_probs。若为true，且存在token在final状态，则仅考虑所有在final状态的token；否则，不考虑final权重。</font></li><li>非lattice解码器：从cost最小的token开始回溯，返回线性FST，<font color="green">最后调用RemoveEpsLocal</font>。</li><li>lattice 解码器<ul><li>GetRawLattice：根据active_toks_中每一帧的activetokens，及token间的forwardlink，生成FST。<ul><li><font color="green">TopSortTokens：对一帧的token进行拓扑排序，使得生成的fst上，epsilon弧的起始状态的id&lt; 结束状态的id。</font></li></ul></li><li>ShortestPath：调用 OpenFst 的 ShortestPath 算法。</li><li>由于epsilon弧，弧数可能比帧数多。</li></ul></li></ul><h2 id="getlattice">4.7. GetLattice</h2><ul><li><font color="green">TODO: 实现</font></li><li>lattice确定化：只保留每个单词序列的最优路径。确定化前的lattice可用于声学rescore，很大；确定化后的lattice可用于计算句子级的置信度分、LM rescore。</li></ul><h1 id="参考资源">5. 参考资源</h1><ul><li>https://kaldi-asr.org/doc/decoders.html</li><li>https://kaldi-asr.org/doc/lattices.html</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/blog/yin-pin/yu-yin-shi-bie/yu-yan-mo-xing/"/>
      <url>/blog/yin-pin/yu-yin-shi-bie/yu-yan-mo-xing/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 音频 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
