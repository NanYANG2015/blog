<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>WeNet代码</title>
      <link href="/blog/yu-yin/gong-ju-bao/wenet/dai-ma/"/>
      <url>/blog/yu-yin/gong-ju-bao/wenet/dai-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="IterableDataset-Processor">1. IterableDataset - Processor</h1><ul><li>根据输入list生成IterableDataset<ul><li>数据类型<ul><li>raw：json信息列表，存储key、wav(、start、end)、txt</li><li>shard：压缩包列表。随机：压缩包间随机，压缩包内部顺序遍历</li></ul></li><li>训练集按rank、worker_id划分数据；CV集各卡均用全集</li></ul></li><li>tokenize<ul><li><font color="green">non_lang_syms：用[……]、&lt;……&gt;、{……}分割文本（如’12[12]34&lt;34&gt;56{56}{78}’分割为[‘12’, ‘[12]’, ‘34’, ‘&lt;34&gt;’, ‘56’, ‘{56}’, ‘’, ‘{78}’, ‘’]），转大写。若token在non_lang_syms中，直接使用；否则BPE&#x2F;按空格分割&#x2F;按字符分割</font></li><li>BPE<pre class="line-numbers language-none"><code class="language-none">import sentencepiecesp &#x3D; sentencepiece.SentencePieceProcessor()sp.load(bpe_model)sp.encode_as_pieces(word)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>symbol_table：token转id</li></ul></li><li>filter：过滤过短或过长的样本<ul><li>max_length(10240)、min_length(10)：音频帧数，帧移10ms</li><li>token_max_length(200)、token_min_length(1)</li><li>min_output_input_ratio(0.0005)、max_output_input_ratio(1)</li></ul></li><li>resample</li><li>speed_perturb：各样本随机变速[0.9, 1.0, 1.1]。默认不使用</li><li><font color="green">提取fbank&#x2F;mfcc</font>：<font color="red">音频要求16bit</font><pre class="line-numbers language-none"><code class="language-none">torchaudio.compliance.kaldi as kaldifeat &#x3D; kaldi.fbank(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, energy_floor&#x3D;0.0, sample_frequency&#x3D;sample_rate)feat &#x3D; kaldi.mfcc(waveform, num_mel_bins&#x3D;num_mel_bins, frame_length&#x3D;frame_length, frame_shift&#x3D;frame_shift, dither&#x3D;dither, num_ceps&#x3D;num_ceps, high_freq&#x3D;high_freq, low_freq&#x3D;low_freq, sample_frequency&#x3D;sample_rate)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li>spec_aug：频谱mask，num_t_mask、num_f_mask、max_t、max_f，max_w未使用。默认使用</li><li>spec_sub：频谱替换，num_t_sub、max_t，随机替换为该时刻前的特征。默认不使用</li><li>shuffle：：<font color="red">局部shuffle，shuffle_size默认10000</font>。默认使用</li><li>sort：局部按帧数排序，便于帧数相近的样本放到同一batch，sort_size&#x3D;500。默认使用</li><li>batch<ul><li>静态batch：除最后一个batch，batch size固定</li><li>动态batch：按max_frames_in_batch（含padding）拼batch</li></ul></li><li>padding：batch内样本按帧数降序排列，特征补0，label补-1</li><li>torchaudio<pre class="line-numbers language-none"><code class="language-none">torchaudio.backend.sox_io_backend.info(wav_file).sample_ratetorchaudio.load(wav_file)torchaudio.backend.sox_io_backend.load(filepath&#x3D;wav_file, num_frames&#x3D;end_frame - start_frame, frame_offset&#x3D;start_frame)torchaudio.transforms.Resample(orig_freq&#x3D;sample_rate, new_freq&#x3D;resample_rate)(waveform)wav, _ &#x3D; torchaudio.sox_effects.apply_effects_tensor(waveform, sample_rate, [[&#39;speed&#39;, str(speed)], [&#39;rate&#39;, str(sample_rate)]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="模型结构">2. 模型结构</h1><h2 id="encoder">2.1. encoder</h2><div class="mermaid">  flowchart TD    classDef font_red color:red;    classDef font_green color:green;    输入("输入：x: [B, T, D], <br> mask_pad: [B, 1, T]，有效帧对应1，拼batch时补的帧对应0")    输入 --> CMVN    CMVN --> 下采样    subgraph 下采样        不下采样        4倍下采样        6倍下采样        8倍下采样        subgraph 不下采样            direction TB            Linear1("Linear(input_size, output_size)")            Linear1 --> LayerNorm1 --> Dropout1        end        subgraph 4倍下采样            direction TB            Conv2d1("Conv2d(1, output_size, 3, 2)")            Conv2d2("Conv2d(output_size, output_size, 3, 2)")            Linear2("Linear(output_size * (((input_size - 1) // 2 - 1) // 2), output_size)")            mask_pad1("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2]")            Conv2d1 --> ReLU1(ReLU) --> Conv2d2 --> ReLU2(ReLU) --> Linear2        end        subgraph 6倍下采样            direction TB            Conv2d3("Conv2d(1, output_size, 3, 2)")            Conv2d4("Conv2d(output_size, output_size, 5, 3)")            Linear3("Linear( , output_size)")            mask_pad2("mask_pad = mask_pad[:, :, :-2:2][:, :, :-4:3]")            Conv2d3 --> ReLU3(ReLU) --> Conv2d4 --> ReLU4(ReLU) --> Linear3        end        subgraph 8倍下采样            direction TB            Conv2d5("Conv2d(1, output_size, 3, 2)")            Conv2d6("Conv2d(output_size, output_size, 3, 2)")            Conv2d7("Conv2d(output_size, output_size, 3, 2)")            Linear4("Linear( , output_size)")            mask_pad3("mask_pad = mask_pad[:, :, :-2:2][:, :, :-2:2][:, :, :-2:2]")            Conv2d5 --> ReLU5(ReLU) --> Conv2d6 --> ReLU6(ReLU) --> Conv2d7 --> ReLU7(ReLU) --> Linear4        end        end    下采样 --> 位置embedding    subgraph 位置embedding        pos_emd("pos_emb(pos, 2i) = sin(pos/(10000^(2i/dim))) <br> pos_emb(pos, 2i+1) = cos(pos/(10000^(2i/dim))) <br> 拼batch计算时，offset小于0的位置采用pos_emb[0]"):::font_green        位置embedding1("x = x * math.sqrt(dim) + pos_emb"):::font_green        相对位置embedding("x *= math.sqrt(dim)"):::font_green        无位置embedding("x不变，pos_emb全0")        Dropout2("dropout(x), dropout(pos_emb)")        tmp1(" ") --> pos_emd        pos_emd -- 位置embedding --> 位置embedding1 --> Dropout2        pos_emd -- 相对位置embedding --> 相对位置embedding --> Dropout2        tmp1 -- 无位置embedding --> 无位置embedding --> Dropout2    end    位置embedding --> 计算chunk_mask    计算chunk_mask --> transformer*num_blocks -->tmp2    计算chunk_mask --> conformer*num_blocks -->tmp2    tmp2(" ") -- normalize_before -->LayerNorm2("LayerNorm")    subgraph transformer*num_blocks        subgraph transformer_attention            tmp3(" ") --> normalize_before1{"normalize_before"}            normalize_before1 -- Yes --> LayerNorm3("LayerNorm") --> MultiHeadedAttention1("MultiHeadedAttention"):::font_red            normalize_before1 -- No --> MultiHeadedAttention1            MultiHeadedAttention1 --> concat_after1{"concat_after"}            concat_after1 -- Yes --> concat_after2("linear(concat(x, att(x)))") --> tmp4(" ")            concat_after1 -- No --> Dropout3("Dropout") --> tmp4            tmp3 -- + --> tmp4            tmp4 --> normalize_before2{"normalize_before"}            normalize_before2 -- Yes --> tmp5(" ")            normalize_before2 -- No --> LayerNorm4("LayerNorm") --> tmp5        end        subgraph transformer_FeedForward            tmp5 --> normalize_before3{"normalize_before"}            normalize_before3 -- Yes --> LayerNorm5("LayerNorm") --> PositionwiseFeedForward1("PositionwiseFeedForward: <br> w2(dropout(activation(w1*x+b1)))+b2 <br> w1:[input, hidden] w2:[hidden, input]"):::font_red            normalize_before3 -- No --> PositionwiseFeedForward1            PositionwiseFeedForward1 --> Dropout4("Dropout")            tmp5 -- + --> Dropout4            Dropout4 --> normalize_before4{"normalize_before"}            normalize_before4 -- Yes --> tmp6(" ")            normalize_before4 -- No --> LayerNorm6("LayerNorm") --> tmp6        end    end    subgraph conformer*num_blocks        subgraph conformer_macaron            tmp7(" ") --> macaron{"macaron"}            macaron -- Yes --> normalize_before5{"normalize_before"}            macaron -- No --> tmp8(" ")            normalize_before5 -- Yes --> LayerNorm7("LayerNorm") --> PositionwiseFeedForward2("PositionwiseFeedForward"):::font_red            normalize_before5 -- No --> PositionwiseFeedForward2            PositionwiseFeedForward2 --> Dropout5("Dropout") --> scale("*0.5")            tmp7 -- + --> scale            scale --> normalize_before6{"normalize_before"}            normalize_before6 -- Yes --> tmp8            normalize_before6 -- No --> LayerNorm8("LayerNorm") --> tmp8        end        subgraph conformer_attention            tmp8 --> normalize_before7{"normalize_before"}            normalize_before7 -- Yes --> LayerNorm9("LayerNorm") --> tmp9(" ")            normalize_before7 -- No --> tmp9            subgraph Attention                direction TB                计算qkv("q、k、v均采用Linear计算 <br> 更新att_cache: 历史[k, v]，(1, head, chunk_size * num_decoding_left_chunks + T, d_k * 2)"):::font_red                计算attention系数("计算attention系数：采用了chunk_mask <br> Dropout"):::font_red                attention("scaled dot product self-attention")                Linear5("Linear(output_size, output_size)")                Rel("pos_bias_u、pos_bias_v: [head, d_k]，xavier_uniform_初始化，可训练 <br> softmax{[(q + pos_bias_u)k + (q + pos_bias_v)(W*pos_emb)]/math.sqrt(d_k)}v <br> rel_shift: 未采用"):::font_red                tmp9 --> 计算qkv -- MultiHeadedAttention --> 计算attention系数 --> attention --> Linear5                计算qkv -- RelPositionMultiHeadedAttention --> Rel --> Linear5            end            Linear5 --> concat_after3{"concat_after"}            concat_after3 -- Yes --> concat_after4("linear(concat(x, att(x)))") --> tmp10(" ")            concat_after3 -- No --> Dropout6("Dropout") --> tmp10            tmp8 -- + --> tmp10            tmp10 --> normalize_before8{"normalize_before"}            normalize_before8 -- Yes --> tmp11(" ")            normalize_before8 -- No --> LayerNorm10("LayerNorm") --> tmp11        end        subgraph conformer_conv            tmp11 --> conv{"conv"}            conv -- Yes --> normalize_before9{"normalize_before"}            normalize_before9 -- Yes --> LayerNorm11("LayerNorm") --> tmp12(" ")            normalize_before9 -- No --> tmp12            tmp12 --> mask_pad4("mask_pad的帧均置0"):::font_red --> causal{"causal"}            causal -- Yes -->            padding("第一次计算时左侧填充(kernel_size-1)帧0，<br> 后续从cache中取缓存的历史有效输入"):::font_red --> pointwise_conv1            causal -- No --> pointwise_conv1            pointwise_conv1("pointwise_conv: 1D卷积，channel扩充为2倍"):::font_red --> GLU("GLU，channel减半"):::font_red --> depthwise_conv            click GLU "https://pytorch.org/docs/stable/generated/torch.nn.functional.glu.html"            depthwise_conv("nn.Conv1d(channels, channels, kernel_size, stride=1, padding=padding, groups=channels, bias=bias) <br> 若非causal，左右填充(kernel_size - 1) // 2帧0"):::font_red            depthwise_conv --> BatchNorm1d/LayerNorm --> activation("activation，如SiLU/swish")            click activation "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"            activation --> pointwise_conv2("pointwise_conv: 1D卷积"):::font_red --> mask_pad5("mask_pad的帧均置0"):::font_red -->Dropout7("Dropout")            tmp11 -- + --> Dropout7            Dropout7 --> normalize_before10{"normalize_before"}            normalize_before10 -- Yes --> tmp13(" ")            normalize_before10 -- No --> LayerNorm12("LayerNorm") --> tmp13            conv -- No --> tmp13        end        subgraph conformer_FeedForward            tmp13 --> normalize_before11{"normalize_before"}            normalize_before11 -- Yes --> LayerNorm13("LayerNorm") --> PositionwiseFeedForward3("PositionwiseFeedForward"):::font_red            normalize_before11 -- No --> PositionwiseFeedForward3            PositionwiseFeedForward3 --> Dropout8("Dropout") --> macaron2{"macaron"}            macaron2 -- Yes --> scale2(*0.5) --> tmp14(" ")            macaron2 -- No --> tmp14(" ")            tmp13 -- + --> tmp14            tmp14 --> normalize_before12{"normalize_before"}            normalize_before12 -- Yes --> tmp15(" ")            normalize_before12 -- No --> LayerNorm14("LayerNorm") --> tmp15            tmp15--> conv2{"conv"}            conv2 -- Yes --> LayerNorm15("LayerNorm") --> tmp16(" ")            conv2 -- No --> tmp16(" ")        end    end</div><h2 id="decoder">2.2. decoder</h2><div class="mermaid">  flowchart TD    classDef font_red color:red;    classDef font_green color:green;    subgraph TransformerDecoder        tmp(" ") -- train --> 输入1("输入label、label长度") --> tgt_mask("mask padding、未来输出") --> embedding --> +位置embedding --> transformer*num_blocks        tmp -- decode --> 输入2("输入上一时刻解码结果") --> embedding        transformer*num_blocks --> normalize_before1{"normalize_before"}        normalize_before1 -- Yes --> LayerNorm1("LayerNorm") --> tmp1(" ")        normalize_before1 -- No --> tmp1        tmp1 --> output{"use_output_layer"}        output -- Yes --> Linear1("Linear(attention_dim, vocab_size)") --> train_decode{"train/decode"}        train_decode -- train --> tmp2(" ")        train_decode -- decode --> log_softmax --> tmp2        output -- No --> tmp2        subgraph transformer*num_blocks            train("train：缓存encoder各block的输出")        end    end    subgraph BiTransformerDecoder        tmp3(" ") -- train --> 输出left_decoder与right_decoder结果        tmp3 -- decode --> 采用left_decoder    end</div><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的语音评分</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen/shen-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="基于深度学习的语音评分">1. 基于深度学习的语音评分</h1><div class="markmap-container" style="height:200px">  <svg data="{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;系统结构&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;声学特征&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;采用预训练的ASR模型（如自监督学习模型wav2vec 2.0、HuBERT，transformer等）提取声学表示&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;伪造数据/低质量数据预训练评分模型&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;专家标注数据fine tune&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;文本特征&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;word/subword/character embedding&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;朗读文本/参考文本+识别文本&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;attention&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;多维度分&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;流式、延迟&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;可解释性、诊断反馈&quot;}]}"/></div><h2 id="2022-Self-Supervised">1.1. 2022 Self-Supervised</h2><ul><li>作者：AI Lab, Kakao Enterprise（韩国公司）</li><li><font color="red">创新点：采用自监督学习模型提取深层声学表示</font></li></ul><h3 id="系统结构">1.1.1. 系统结构</h3><div style="display: inline-block; width: 60%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4422119d06867cb6f80ed36536834260?method=download&shareKey=74b198b7631782b93ae09c5a823fa71e" alt="系统结构"></p></div><div style="display: inline-block; vertical-align: bottom; width: 39%;">*GAP：global average pooling<ul><li>用L2数据+CTC fine-tune 自监督学习模型（wav2vec 2.0、HuBERT）</li><li>取各transformer层的上下文表示的均值</li><li>输入文本字符序列，采用BLSTM评分</li></ul></div><h3 id="评价">1.1.2. 评价</h3><ul><li>数据集：KESL（韩国英语二语学习者，根据发音、准确度、重音、停顿、语调等评总分）、Speechocean762</li><li>基线<ul><li>自研DNN-HMM AM</li><li>Agg: SpeechRater time-aggregated特征。Seq: 时间序列特征，eGeMAPS set[33]（包含MFCC、响度、音调、jitter、shimmer）的均值、方差，各特征均值方差归一化，采用OpenSmile提取</li><li>评分模型：2层全连接</li></ul></li><li>自监督学习模型：采用Fairseq上的，finetune 150k步，batch size&#x3D;8</li><li>评分模型：embedding 64维，BLSTM 1层，128维</li><li><img src="https://note.youdao.com/yws/api/personal/file/WEB7540a07fff0e50902ae1f03f916265d2?method=download&shareKey=a683224dcdddbd810b26b9422f1bc3ee" width="383px"></li><li>实验结论<ul><li>相较于传统评分模型，本文提出的模型在各测试集上的评分效果有一致的提升。其中finetune之后的HuBERT Large模型效果最好</li><li>相较于采用自监督学习模型的CNN层特征、较高层的特征表示，采用各transformer层的上下文表示的均值，效果较好</li><li>相较于线性回归、MLP，BLSTM模型评分效果较好，可以学习时间序列特征</li></ul></li><li><font color="green">存在的问题：无Speechocean762 准确度、完整度、总分评分效果</font></li></ul><h3 id="其它">1.1.3. 其它</h3><ul><li>Kim E, Jeon J J, Seo H, et al. Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning[J]. arXiv preprint arXiv:2204.03863, 2022.</li></ul><h2 id="2021-Multi-Encoder">1.2. 2021 Multi-Encoder</h2><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：ICASSP 2021</li><li>被引用次数：2</li><li><font color="red">创新点：端到端朗读评分，输出句子级、单词级分数</font></li></ul><h3 id="系统结构-1">1.2.1. 系统结构</h3><div style="display: inline-block; width: 50%;"><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa24e448d76d182c1bf2d2df35649895b?method=download&shareKey=161bc7c16a5682e139a3565471ea8c50" alt="系统结构"></p></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>音频encoder</p><ul><li>输入：80维Mel-filterbank + 1阶差分 + 2阶差分 + pitch + 位置embedding</li><li>CNN（提取局部特征、降采样） + transformer（每一层transformer由1层multi-head attention + 1层position-wise全连接组成），12层</li></ul></li><li><p>文本encoder</p><ul><li>输入：采用sentencepiece生成的子词序列 + 位置embedding</li><li>vanilla transformer encoder结构，2层</li></ul></li><li><p>词级表示 $h1_{word\left ( i \right ) }&#x3D;Attention \left ( h_{word\left ( i \right ) }, h_{audio}, h_{audio} \right ) + h_{word\left ( i \right ) } $</p></li><li><p>单词评分：1层全连接 + sigmoid</p></li><li><p>句子评分</p><ul><li>输入：所有单词表示的average pooling，拼接所有单词的平均分</li><li>1层全连接 + ReLU + 1层全连接 + sigmoid</li></ul></li></ul></div><li><p>训练流程</p><ul><li>audio encoder + decoder，ASR训练。训练集：960h LibriSpeech + 1000h L2</li><li>冻结audio encoder，预训练。将上述训练集中的词随机替换40%为口语考试中的高频词，不匹配的单词label为0，否则为1。</li><li>采用专家标注数据finetune，采用multi-task学习（单词评分、句子评分）。训练集：10000句、15000词；测试集：1000句、1000词。3位专家打分，1-5，归一化为0-1</li></ul></li><h3 id="实验结果">1.2.2. 实验结果</h3><ul><li>基线<ul><li>GBT，输入特征为各音素的平均GOP分、各音素在句中的平均位置（B-1，I-2，E-3，S-4）、各音素的数量、句中元音时长的平均差</li><li>BD-LSTM with attention，声学特征为音素GOP、元音时长的平均差，文本特征为音素embedding、音素位置embedding、预训练的GloVe word embedding</li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB3897f724e308dd3f11f1d5bd91998a10?method=download&shareKey=43276c4d04b452c03fa73d45cb868411" width="393px"><h3 id="其它-1">1.2.3. 其它</h3><ul><li>Lin B, Wang L. Attention-Based Multi-Encoder Automatic Pronunciation Assessment[C]&#x2F;&#x2F;ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 7743-7747.</li></ul><h2 id="2021-Transfer-Learning">1.3. 2021 Transfer Learning</h2><ul><li>作者：腾讯智能平台产品部</li><li>发表信息：Interspeech 2021</li><li>被引用次数：6</li><li><font color="red">创新点：当前基于DNN-HMM的GOP计算存在许多局限性。相较于传统方案采用ASR AM计算GOP等评分特征，提出直接采用AM提取的深层特征进行评分。迁移学习：采用ASR预训练模型，用评分任务fine-tune。相较于AM参数量，可采用特定的评分任务快速fine tune评分模型</font></li></ul><h3 id="系统结构-2">1.3.1. 系统结构</h3><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEBe06a9dd97f1e8da8052a976b113ce875?method=download&shareKey=398ca0c53b023a081355e6265d4e5e4c"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>输入：ASR AM提取的深层特征、<font color="red">强制对齐信息</font></p></li><li><p>基于强制对齐，对于各音素，对应帧的深层特征取均值，+ phone embedding</p></li><li><p>multi-head self-attention + 非线性变换，得到词级表示</p></li><li><p>self-attention + 非线性变换，得到句级表示</p></li><li><p>sigmoid，输出句级评分</p></li></ul></div><li><p>训练流程</p><ul><li>预训练11层TDNN-HMM ASR声学模型，深层特征256维，非线性变换降为32维</li><li>采用大量合成数据，用基于GOP的评分模型打分（low-quality），预训练评分模型</li><li>采用少量指定任务的专家标注数据fine-tune</li></ul></li><h3 id="评价-1">1.3.2. 评价</h3><ul><li><p>数据集</p><ul><li>ASR：960h LibriSpeech + 1000h L2</li><li>合成数据：50000句</li><li>fine tune数据<ul><li>11000句（含1000句测试集），平均词数13，3位专家打分1-5</li><li>Speechocean762（50%用作测试集，分数缩放至0-1）</li></ul></li></ul></li><li><p>模型</p><ul><li>LayerNorm</li><li>参数量：AM 8165047；评分模型 20289</li></ul></li><li><p>实验结果</p><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>对比基于GOP的SOTA：<br>输入音素GOP、embedding，2层BLSTM+MLP；<br>输入音素GOP均值、各音素的平均位置（BIES）、句子音素总数，GBT</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2584d9889200577d4e05c1c40b773378?method=download&shareKey=2ff0f42e16e7e3bd70ff3579ca55fa1d" alt="table2"></td><td></td></tr><tr><td>对比输入特征<br>STPs：考虑转移概率</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB41fd8d0028b57c6ac522716778d9bf76?method=download&shareKey=640db70f420eb058d02bf9acf6e4736c" alt="table3"></td><td><font color="green">采用音素GOP特征，当前模型效果差于2BLSTM+MLP？</font></td></tr><tr><td>冻结AM参数、3阶段训练</td><td></td><td>始终冻结AM参数+第2阶段预训练+finetune效果最好。评分模型参数量约为AM的0.2%，finetune高效</td></tr><tr><td>self-attention机制：对比替换为平均</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB448228dfdb5c057f5ff3834eb5266409?method=download&shareKey=a776ada79534cc24c131ec6d40c2d626" alt="table6"></td><td></td></tr></tbody></table></li></ul><h3 id="其它-2">1.3.3. 其它</h3><ul><li>Lin B, Wang L. Deep Feature Transfer Learning for Automatic Pronunciation Assessment[C]&#x2F;&#x2F;Interspeech. 2021: 4438-4442.</li></ul><h2 id="2020-Multi-Modal">1.4. 2020 Multi Modal</h2><ul><li>作者：印度、哥伦比亚大学、SLTI公司</li><li>被引用次数：9</li><li><font color="red">创新点：提出了基于注意力融合的多模态端到端自发语音评分</font></li></ul><h3 id="系统结构-3">1.4.1. 系统结构</h3><img src="https://note.youdao.com/yws/api/personal/file/WEB4b48b1cf6ca4f879e1ddd50f955e5c96?method=download&shareKey=b833b0104a23cae2ad5f1e60d0e4287a" width="740px"><ul><li>音频<ul><li>输入：log-scaled mel谱，采样频率16000Hz，帧长2048、hop size 512，mel band 128个，归一化，切分为固定帧数（128）的段</li><li>CNN+BLSTM，每组(2*Conv1D+MaxPooling)后，filters数目double</li></ul></li><li>文本<ul><li>non-native ASR，word embedding：300维glove embedding初始化，OOV词初始化为全0向量，模型训练时优化</li></ul></li><li>拼接音频、文本表示<font color="red">（识别时需要输出单词起止时间）</font>，$h^{m}&#x3D;[h^{a}, h^{t}]$，attention<br>$$e_{t}&#x3D;h_{t}^{m} w_{a} ; a_{t}^{m}&#x3D;\frac{\exp \left(e_{t}\right)}{\sum_{i&#x3D;1}^{T} \exp \left(e_{i}\right)} ; c^{m}&#x3D;\sum_{t&#x3D;1}^{T} a_{t}^{m} h_{t}^{m}$$</li></ul><h3 id="实验结果-1">1.4.2. 实验结果</h3><ul><li><p>数据集：题目难度、评分标准遵循Common European Framework of Reference (CEFR)标准，英语口语等级A2-C1（根据题目难度确定可以得到的最高分），人工分包含5个等级：A2、LB1、HB1、LB2、HB2，其中L、H分别表示low、high</p></li><li><p>为每个题目分别训练模型</p></li><li><p>评价指标：Quadratic Weighted Kappa (QWK)，衡量两个序列间的一致性。通常取值范围为0-1，若两者一致性低于随机分布，也可以为负数。</p><p>$$\kappa&#x3D;1-\frac{\sum_{i, j} W_{i, j} O_{i, j}}{\sum_{i, j} W_{i, j} E_{i, j}}$$</p><p>其中，$W$、$O$、$E$ 均为$N\times N$的矩阵，$N$为评分类别数，$W_{i, j}&#x3D;\frac{(i-j)^{2}}{(N-1)^{2}}$，$O_{i, j}$表示评分为$i$、人工分为$j$的样本数，<font color="green"> $E_{i, j}$表示分数期望的histogram矩阵（计算方式：人工分的histogram向量与机器分的histogram向量的外积，归一化使其与矩阵$O$有相同的和）。QWK不适用于类别不均衡的数据，对样本数少的类特别敏感</font>。</p></li><li><p>采用hyperopt包优化每个分数之间round的阈值，以最大化QWK</p></li><li><p>实验结果<br><img src="https://note.youdao.com/yws/api/personal/file/WEB17d08d0c67c81e666db7ed3145e80ade?method=download&shareKey=0f1ddebcfd19bc8468516e55f866ff3e" alt="实验结果"><br>*MMAF: Multi-modal with Attention Fusion； BDRCNNAttn [A]、BDLSTMAttn  [T]：单模态基线，attention权重仅赋予音频或文本</p></li><li><p>实验结论：多模态注意力融合比单模态网络效果更好</p></li><li><p>存在的问题：未与SOTA基线对比</p></li></ul><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>attention分布、与题目难度的关系</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBc6463effb413ddf4d7584db5ddd33cd9?method=download&shareKey=52862b06a3026ef3a7ef93d9430d3785" alt="attention分布1"></td><td>文本:音频attention权重比约为85:15 <br> 低难度题目（prompt 1），attention权重向音频特征倾斜；高难度题目（prompt 4），attention权重向识别文本特征倾斜 <br> 评分越高，attention权重越向识别文本特征倾斜</td></tr><tr><td>attention分布定性分析</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfafd3a21834490651a22fd68420efd64?method=download&shareKey=8958497f6fba8feaa1a0a1dd1eee2f86" alt="attention分布2"></td><td>对含uh、um等填充词，false starts，停顿的片段，attention权重偏向音频，对含关键信息的流利片段，attention权重偏向文本，如图2（prompt 4主题为smoking）</td></tr></tbody></table><h3 id="其它-3">1.4.3. 其它</h3><ul><li>Grover M S, Kumar Y, Sarin S, et al. Multi-modal automated speech scoring using attention fusion[J]. arXiv preprint arXiv:2005.08182, 2020.</li></ul><h2 id="2022-Cambridge-PhD-Thesis">1.5. 2022 Cambridge PhD Thesis</h2><ul><li>发表信息：剑桥大学博士论文</li><li><font color="red">创新点：由于大量、可靠的多维度人工评分较难获取，采用总分标注数据，通过限定输入特征进行端到端多维度评分。相较于特征提取-评分两阶段模型，端到端模型能更好地拟合人工分，且对不同的数据集、不同的任务，泛化性能更好</font></li><li><font color="red">存在的问题：需要强制对齐信息；仅自制数据集上的评分效果；由于没有维度分人工标注，采用总分统计维度分评分效果 </font></li></ul><h3 id="系统结构-4">1.5.1. 系统结构</h3><h4 id="发音评分">1.5.1.1. 发音评分</h4><ul><li>采用音素距离特征，与发音人属性（音色、性别等）不相关</li><li>传统方案：采用单高斯模型建模各音素的发音，计算各音素模型间的对称KL散度，拼接为$\frac{1}{2} p\left ( p-1 \right )$维的向量，并取$\log\left ( d+1 \right )$。对于短语音，包含缺失音素的KL散度设为-1。输入一层全连接进行评分。<ul><li>缺点：每个说话者需要大量数据训练高斯模型；可能丢弃发音相关的信息；未考虑音素的发音过程、同一音素在不同上下文中发音可能不同、音素对于评分的重要性取决于上下文；存在错误的识别、强制对齐</li></ul></li><li>端到端方案：将各音素片段的帧序列编码为固定长度的embedding，对应同一音素的音素片段采用attention（学习忽略对齐错误的音素片段、关注发音错误的片段）加权平均得到音素embedding，计算音素embedding间的欧式距离。输入一层全连接进行评分。<ul><li>各模块初始化<ul><li>双胞胎双向RNN：+sigmoid，判断两音素是否一致，预训练</li><li>评分全连接层：采用基线模型初始化</li></ul></li><li>端到端fine-tuning</li><li>为了模型收敛，损失函数需要加attention权重熵的惩罚项，$C \left( \lambda, S_{train} \right)&#x3D;MSE_{train} -\beta \sum_{m&#x3D;1}^{M} \sum_{n&#x3D;1}^{N_{m}} \alpha_{n m} \log \alpha_{n m}$，其中$M$为音素总数，$N_m$ 为该音素的音素片段数</li><li><font color="green">存在的问题：音素片段数目不确定，attention如何实现？</font></li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB7176bacfcf429dccb31d01b15693550f?method=download&shareKey=ec352fbca4d4bd034b6e439a23d35347" width="709px"><h4 id="韵律评分">1.5.1.2. 韵律评分</h4><ul><li><p>英语的重音等时性</p></li><li><p>传统方案</p><ul><li>评分特征<ul><li>相邻元音音程时长的平均差 $ rPVI_V&#x3D;\frac{1}{K_{V}-1} \sum_{k&#x3D;1}^{K_{V}-1}\left|d\left(\tau_{k}^{(V)}\right)-d\left(\tau_{k+1}^{(V)}\right)\right| $ ，其中，$d\left(\tau_{k}\right)$ 为第k个元音音程的时长，$K_V$ 为元音音程总数</li><li>相邻非元音音程时长的平均差 $rPVI_C$</li><li>$ CCI_V&#x3D;\frac{1}{K_{V}-1} \sum_{k&#x3D;1}^{K_{V}-1}\left|\frac{d\left(\tau_{k}\right)}{l_{k}}-\frac{d\left(\tau_{k+1}\right)}{l_{k+1}}\right| $ ，其中 $l_{k}$ 为第k个元音音程中的元音个数，$\frac{d\left(\tau_{k}\right)}{l_{k}}$ 表示第k个元音音程中各元音的时长均值</li><li>$\mathrm{CCI}_{C}$</li></ul></li><li>音程示例<img src="https://note.youdao.com/yws/api/personal/file/WEB62a1368e558eb842e642101204135967?method=download&shareKey=e218a8dfbcec2c97cdde02a678016c95" width="710px"></li></ul></li><li><p>端到端方案</p><ul><li>采用attention学习音程中不同子段的重要性（学习忽略对齐错误导致的时长异常的子段），与该音程时长拼接 $ x_k&#x3D;\left[\sum_{m&#x3D;1}^{M^{(k)}} \alpha_{m} d\left(v_{m}^{(k)}\right), d\left(\tau_{k}\right)\right] $</li><li>采用序列模型（BLSTM或transformer）学习元音音程特征序列 $x_{1: K_{V}}^{(V)}$ 、非元音音程特征序列。</li><li>分别在元音音程深层特征序列$h_{1: K_{V}}^{(V)}$、非元音音程深层特征序列上加attention。</li><li>拼接 $\tilde{\boldsymbol{h}} &#x3D; \left[\tilde{\boldsymbol{h}}^{(V)}, \tilde{\boldsymbol{h}}^{(C)}\right]$，采用1层全连接评分。</li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEB1662f02ff97ca28c83a9ac353cebcb29?method=download&shareKey=20178e325c8829035111c33474cf80cf" width="711px"></li></ul><h4 id="语调评分">1.5.1.3. 语调评分</h4><ul><li>重读单词基频高</li><li>传统方案<ul><li>方案一：浊音区域的基频的均值、中位数、最大值、1&#x2F;4分位数、3&#x2F;4分位数，采用1层全连接评分</li><li>方案二：考虑清音音素和静音，对于各音素，分别采用基频、浊音概率计算上述统计值并拼接，采用sequence-to-vector模型评分</li><li>方案三：采用最小二乘cosine拟合基频包络（DCT），清音区域插值，提取相应的系数向量采用DNN评分</li></ul></li><li>端到端方案<ul><li>输入：基频、浊音概率序列、position embedding。multi-head sequence-to-vector attention</li><li>考虑到长音频不适合用帧序列特征，采用sequence-to-vector模型学习各音素的特征表示，再采用sequence-to-vector模型预测分数</li></ul></li></ul><img src="https://note.youdao.com/yws/api/personal/file/WEBab93c416e54a88463f04bac213c506cf?method=download&shareKey=61fe567f298dc2bbc0a50a246d336ac2" width="500px"><h4 id="文本评分">1.5.1.4. 文本评分</h4><p>采用BERT提取word embedding，采用LSTM with attention评分[223]</p><h4 id="系统结构-5">1.5.1.5. 系统结构</h4><img src="https://note.youdao.com/yws/api/personal/file/WEBb09fd6a33bd460fc514eb7c6d99d211a?method=download&shareKey=b60236ca8e8979ca0069a830a9f487d2" width="706px"><h3 id="总分">1.5.2. 总分</h3><ul><li>方案一：各维度分均值</li><li>方案二：拼接各维度分打分器倒数第二层的输入表示，输入全连接网络评总分</li><li>方案三：各维度分的加权和：采用维度分打分器的中间表示、全连接网络计算attention系数</li></ul><h3 id="评价-2">1.5.3. 评价</h3><ul><li><p>数据集<br>英语水平测试数据，测试含简答、读8句话、根据提示自由表述，对每个说话人评总分0-6。</p></li><li><p>基线：Gaussian Process评分器[154]</p></li><li><p>实验结果</p><ul><li><font color="red">*由于维度分没有人工标注，采用总分进行近似</font></li><li>*由于模型对随机初始化较敏感，统计5次训练的模型的均值、标准差</li></ul><table><thead><tr><th>实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>发音</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3913cedf044c88366820bbf5c4d5c58e?method=download&shareKey=84242511a73adfa55e575ec8c8744835"></td><td>输入特征：MFCC vs PLP，性能差异较小；<br> 音素片段的sequence-to-vector模型，BLSTM + 最后一层输出additive attention，性能最好；<br> 在训练集-测试集 匹配&#x2F;不匹配的配置下，端到端模型性能均最好，一方面端到端模型可以学习更有表征能力的特征，另一方面泛化能力更好；<br>观察人工分-机器分散点图，端到端模型存在低分打高，但对特定人工分，机器分分布更集中；<br> tunability：分别采用音素距离KL散度、采用说话人分类任务训练得到的x-vector、采用评分任务训练得到的deep 音素距离特征，后者在L1分类任务上性能最好</td></tr><tr><td>韵律</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBfc777e01dd42196937bed56940b389d7?method=download&shareKey=df673e504d499f435ecd21407b97e996"></td><td>sequence-to-vector模型：BLSTM + 最后一层输出additive attention；<br> 存在明显的低分打高</td></tr><tr><td>语调</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB3270f41bcc6d831669450c9f25110614?method=download&shareKey=ce5ceff13afb5b18939ea898aff4de27"></td><td>相较于multi-head attention，attention LSTM评分效果更好；<br> 不存在明显的打分偏移，基于基频统计特征的DNN存在明显的低分打高</td></tr><tr><td>发音分-音素距离-音素片段attention分布</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB34df75f218c23024b00dc4e3f3b088f3?method=download&shareKey=02a7f78a56973a438be39d7228dfea90"></td><td>大多数情况下，attention权重接近均匀分布，仅排除少量异常值；<br> 余下的大部分只关注一个或少量具有代表性的音素片段；<br> 剩下的取中间值的较少，可能是随机现象，也可能是关注发音特别错误或特别好的音素片段</td></tr><tr><td>维度分相关性、互补性</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBad03336b0f416faef0441aabed50706e?method=download&shareKey=2853e4e6318acb5b4860f2242397b24a"></td><td>发音评分与文本评分有一定的相关性：发音中缺失的音素与说话长度、单词的丰富程度等有关；<br> 各维度分互补</td></tr><tr><td>评分模型系统偏差</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4ef2e51c6d3e8128e12d0f861d029b1e?method=download&shareKey=9a212a7e25487cd248ad343aba51cb85"></td><td><font color="red">韵律分：高水平数据中才与总分相关，无法区分中低水平说话人的韵律水平；<br> 发音和文本特征较难区分高水平数据；<br> 发音分低分打低：可能由于ASR错误率更高</font></td></tr><tr><td>总分</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB28847721cde7c6b5b25325a1baf7b812?method=download&shareKey=ba4b6bf4eae0cde16e8ed2c6d66dc33f"></td><td>各维度分的加权和（采用attention计算加权系数）效果最好</td></tr></tbody></table></li></ul><h3 id="其它-4">1.5.4. 其它</h3><ul><li>Kyriakopoulos K. Deep Learning for Automatic Assessment and Feedback of Spoken English[D]. University of Cambridge, 2022.<ul><li>5.1节：理论推导了采用总分标注数据，训练多维度评分模型的可行性</li></ul></li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2018). A deep learning approach to assessing non-native pronunciation of english using phone distances. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 2018, pages 1626–1630.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2019). A deep learning approach to automatic characterisation of rhythm in non-native english speech. In INTERSPEECH, pages 1836–1840.</li><li>Kyriakopoulos, K., Knill, K. M., and Gales, M. J. (2020). Automatic detection of accent and lexical pronunciation errors in spontaneous non-native english speech. Interspeech.</li></ul><h2 id="2022-Multi-Aspect-Multi-Granularity">1.6. 2022 Multi-Aspect Multi-Granularity</h2><ul><li>作者：MIT 人工智能实验室（CSAIL）、平安科技研究院（PAII Inc.）</li><li>发表信息：ICASSP 2022</li><li><font color="red">代码：<a href="https://github.com/YuanGongND/gopt">https://github.com/YuanGongND/gopt</a></font> (Goodness Of Pronunciation feature-based Transformer)</li><li><font color="red">创新点<ul><li>联合训练音素、单词、句子级各维度分及总分</li><li>采用BERT风格非层级的标准Transformer 架构</li></ul></font></li></ul><h3 id="系统结构-6">1.6.1. 系统结构</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEB915a9e1bc3fbd6be8224b3a8122dbfbd?method=download&shareKey=6c0e39f6c5b19fa1a9892f30ad805242" alt="系统结构"></p><ul><li>声学模型<ul><li>TDNN-F，训练集：960h Librispeech，用Kaldi Librispeech S5 recipe训练</li><li>PAII-A：自研AM，452h L1 + 1696h L2</li><li>PAII-B：995h L1 + 6591h L2</li></ul></li><li>输入<ul><li>GOP特征：84维（42个音素，log phone posterior、log posterior ratio），经过1层线性层降维至24维</li><li>正确发音phone embedding，24维。<ul><li>音素序列填充5个[cls] token，对应句子级各维度分、总分</li></ul></li><li>位置embedding，24维，可训练</li></ul></li><li>采用标准Transformer encoder结构，但减为3层，embedding 24维</li><li>评分：各个评分分别采用1层24*1的线性层，layer normalization。<font color="green">单词分：训练时反向传播至该单词的各个音素，推断时取其各个音素的输出的均值。</font></li></ul><h3 id="评价-3">1.6.2. 评价</h3><ul><li>数据集：speechocean762（类别不均衡，主要为高分），单词、句子评分缩放至0-2，与音素一致</li><li>评价指标：主要为PCC（Pearson相关系数）</li><li>基线：speechocean762实现的RF（随机森林）、SVR（支持向量回归），[21]transfer learning、LSTM（模型深度、维度等与GOPT一致，LSTM最后一个token的输出作为句子表示）</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe86bc0edfdbde5c5f9c1462cd7bc2797?method=download&shareKey=a13ff3549e54e584ec7ebb674f8d68f3" alt="实验结果"><br>*用不同的随机种子重复5次实验，统计均值、标准差</p><img src="https://note.youdao.com/yws/api/personal/file/WEBe04116ccf077c827de0f320a76641997?method=download&shareKey=7c437dbe136f2f972ca8c513e65879df" width="50%"><ul><li>实验结论<ul><li><font color="red">GOPT除单词重音、句子完整度评分性能较差（可能与speechocean762训练集中句子完整度分布不均有关）外，其它任务可提供SOTA效果</font></li><li>采用PAII-A，音素、单词评分性能提升，但句子评分性能下降</li><li>联合训练音素、单词、句子评分模型，相对于分别训练，各模型性能都有提升。</li><li>正确发音phone embedding对提升模型性能有帮助</li><li>继续加宽或加深模型结构，性能无提升（训练集较小）</li><li>采用PAII-A、PAII-B，评分性能相当</li></ul></li></ul><h3 id="其它-5">1.6.3. 其它</h3><ul><li>Gong Y, Chen Z, Chu I H, et al. Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment[C]&#x2F;&#x2F;ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022: 7262-7266.</li></ul><h2 id="2020-Multi-Granularity">1.7. 2020 Multi-Granularity</h2><ul><li>作者：腾讯智能平台产品部、北京语言大学</li><li>发表信息：Interspeech 2020</li><li><font color="red">创新点<ul><li>考虑音素、单词、句子评分间的层次关系和上下文，提出一种分层网络结构，联合评分</li><li>采用半监督训练，利用无标注数据训练音素检错</li></ul></font></li></ul><h3 id="系统结构-7">1.7.1. 系统结构</h3><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEB15d8915a312dd0f51572a305759e4465?method=download&shareKey=06c0c6e07038fab09d0b18a838356915" width="782px"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>音素检错</p><ul><li>输入：GOP（通过强制对齐计算）、音素embedding、位置embedding（B、I、E、S分别表示单词开头、中间、末尾、单音素词。<font color="red">音素发音因其在单词中的位置而异</font>）、类别embedding（C、V分别表示辅音、元音。<font color="red">单词中元音和辅音的重要性不同</font>）</li><li>模型结构：BLSTM。半监督学习</li></ul></li><li><p>单词评分<br><font color="red">单词中每个音素对最终单词得分的贡献不同，采用attention机制。</font><br>$U_{p}&#x3D;\tanh \left(w * O_{p}+b\right)$, $\alpha_{p}&#x3D;\frac{\exp \left(U_{p}^{T} U_{w}\right)}{\sum_{q \in w} \exp \left(U_{q}^{T} U_{w}\right)}$, $S_{w}&#x3D;\sum_{p \in w} \alpha_{p} O_{p}$<br><font color="green"> 其中, $O_p$ 为音素$p$的评分, $U_w$是随机初始化的向量, 可以作为单词上下文的记忆单元。</font></p></li><li><p>句子评分</p><ul><li><font color="red">不同属性（如词性、音素个数）的单词对句子得分贡献不同。</font></li><li>输入：word层输出、词性、单词长度</li><li>模型结构：BLSTM+MLP，sigmoid回归。</li></ul></li><li><p>multitask：$L_{total}&#x3D;(1-w)\times L_{sent}+ w\times L_{phoneme}$，$L_{sent}$其中为句子评分的均方误差损失，$L_{phoneme}$为PUNU损失。</p></li></ul></div><li><p>半监督 - PUNU (positive unlabeled and negative unlabeled) learning</p><ul><li><p>正样本：native发音；负样本：GOP较低的L2学习者发音；unlabeled数据：剩下的L2发音。</p></li><li><p>损失函数如下：</p><p>$R_{\mathrm{PUNU}}^{\gamma}(g)&#x3D;(1-\gamma) R_{\mathrm{PU}}(g)+\gamma R_{\mathrm{NU}}(g)$<br>$R_{\mathrm{PU}}(g)&#x3D;\theta_{\mathrm{P}} E_{\mathrm{P}}[l(g(x), 1)]+E_{\mathrm{U}}[l(g(x),-1)]-\theta_{\mathrm{P}} E_{\mathrm{P}}[l(g(x),-1)]$<br>$R_{\mathrm{NU}}(g)&#x3D;\theta_{\mathrm{N}} E_{\mathrm{N}}[l(g(x),-1)]+E_{\mathrm{U}}[\mathrm{l}(g(x), 1)]-\theta_{\mathrm{N}} E_{\mathrm{N}}[l(g(x), 1)]$</p><p>其中, $g$ 为任意决策函数, $l$ 为 loss 函数, $\theta_P$、$\theta_N$ 为正负样本的先验概率, $E_U$、$E_P$、$E_N$ 分别表示未标记数据、正类、负类（边际）的损失期望。</p></li></ul></li><h3 id="评价-4">1.7.2. 评价</h3><ul><li><p>数据集</p><ul><li>Timit + 22998英语句子，1000中国说话人，16-20岁。句子评分、单词评分、音素检错标注量分别为8998、4000、10000句。句子平均单词数为13。标注音素量99568。</li><li>1-5分，3人评分取均值。3人评一致性：计算某一评分员的评分与剩余评分员的平均分之间的PCC，句子、词级分别为0.78、0.76。</li><li>检错3人投票，<font color="green"> 3人评一致性：随机挑选1000句，计算任意两标注员的Kappa系数，平均0.65，95%置信度区间(0.647, 0.653)，p-value小于0.1%，一致性较高。</font></li></ul></li><li><p>训练集：7998句non-native数据，有评分。5000句native数据，无评分。</p></li><li><p>测试集：4000句，标注了39808个音素、1000词、1000句。错误音素占比约14%。</p><table><thead><tr><th>音素错误分布</th><th>单词分分布</th><th>句子分分布</th></tr></thead><tbody><tr><td><img src="https://note.youdao.com/yws/api/personal/file/WEB85b5a6de565f9c520658835b95786ba8?method=download&shareKey=8632f81f22963f5f07fd6cb3e9eef9d2"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB2d0fd1ee7ade8549ac8bcab6226f37e1?method=download&shareKey=2f0a145baa086763a420d45b143c03a9"></td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB4d0c8cbceb3537c0867f876ff2f2f3e7?method=download&shareKey=6e744f4ae6c8a59b0bb97a0674605191"></td></tr></tbody></table></li><li><p>实验结果</p><table><thead><tr><th>实验</th><th>基线</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>句子评分</td><td>2BLSTM+MLP，后一BLSTM的输入为音素BLSTM最后一个隐含单元的输出拼接、词性、单词长度</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa5476cf9e1f6ac5f1ee3c5cdf4c56b69?method=download&shareKey=1c586ae1cc1ff426900c1b9e0fccfd67"> <br> *STL：single task learning</td><td>单词层attention、multitask学习可提升评分性能</td></tr><tr><td>单词评分</td><td>BLSTM+MLP：去掉上述句子评分BLSTM。SL：用3000个单词评分标注数据训练</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB30a1747796429bff94888d09a7eb6ce1?method=download&shareKey=fd537dcca8a48cdd697826af548cdbd4"></td><td>对比前两行：attention机制有收益；<br>最后一行：PCC较高，仅用句子、音素级标注信息，仍能学到单词分信息</td></tr><tr><td>音素检错</td><td>SL：用59760个音素检错标注训练</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB8f8bcea9b4f939f430dd5b5985a7b779?method=download&shareKey=767fa915db6e5fe6e9c036fb5461cd0b"></td><td>半监督学习未召回略差于有监督学习，虚警相差较小。<font color="green">加无标注数据效果变差？</font></td></tr></tbody></table></li></ul><h3 id="其它-6">1.7.3. 其它</h3><ul><li>Lin B, Wang L, Feng X, et al. Automatic scoring at multi-granularity for L2 pronunciation[J]. Proc. Interspeech 2020, 2020: 3022-3026.</li></ul><h2 id="2019-ETS-monologue-and-dialogue">1.8. 2019 ETS monologue and dialogue</h2><ul><li>作者：ETS</li><li>发表信息：ICASSP 2019</li><li><font color="red">创新点<ul><li>采用基于attention的BLSTM对自由表述的3个维度评分：内容（话题相关度、得体性）、 <del>组织（语篇结构和连贯性）、</del> 语用（词汇、语法）、delivery（发音、重音、流利度、语调）</li><li>采用BLSTM或MemN2N（端到端记忆网络）编码提示文本或多轮对话的历史信息</li></ul></font></li></ul><h3 id="系统结构-8">1.8.1. 系统结构</h3><div style="display: inline-block; width: 50%;"><img src="https://note.youdao.com/yws/api/personal/file/WEBb737aee697fb953d540bee20a2fa9f48?method=download&shareKey=bd923154f67ce2976488da77c835431c"></div><div style="display: inline-block; vertical-align: bottom; width: 49%;"><ul><li><p>内容</p><ul><li>word embedding层：用Google’s Word2Vec初始化，模型训练时优化</li><li>采用BLSTM将提示文本的词序列编码为固定长度的向量$v^{p}$，与回答中各个词的词向量$e_{t}^{r}$拼接</li></ul></li><li><p>语用特征：POS：词性one-hot向量；DEP：句法依存标签，如主语、宾语；Morph（形态）。采用spaCy提取，分别19、51、248维</p></li><li><p>发音：采用non-native ASR模型识别，native ASR模型<font color="red">强制对齐</font>。8维特征：时长、音调、强度、静音或停顿时长、non-native ASR模型后验概率、native ASR模型后验概率、识别结果LM分、ASR置信度分，取各帧平均（实验对比音素、音节、词级特征）</p></li><li><p>评分模型</p><ul><li>维度分：<font color="green">feed-forward attention层输出向量的均值。</font>多轮对话：对每个回答评维度分，整个对话的维度分取多轮对话的均值</li><li>总分：3个维度分拼接，经过1层全连接层</li></ul></li></ul></div><li><p>采用MemN2N（端到端记忆网络）编码多轮对话的历史信息</p><img src="https://note.youdao.com/yws/api/personal/file/WEB7bf9ff8d8f680293f1103d2ad9247e0d?method=download&shareKey=e1ee2a6611af41883a37a25ee02708ba" width="414px"><p>拼接$e_{t}^{r}$、$v^{p}$、$a^{p} \cdot v_{h}^{p}$、$a^{r} \cdot v_{h}^{r}$，其中，$v_{h}^{p}$、$v_{h}^{r}$分别表示历史提示、历史回答，$a^{p}$、$a^{r}$分别表示对应的attention向量</p></li><li><p>语用特征示例</p><img src="https://note.youdao.com/yws/api/personal/file/WEB8b9848c574cb881e11ef9f4ba799a455?method=download&shareKey=c9480e8e26bb1c13902d745c6f37e3d5" width="410px"></li><h3 id="评价-5">1.8.2. 评价</h3><ul><li><p>数据集  </p><ul><li><img src="https://note.youdao.com/yws/api/personal/file/WEB4c9a9144977d69a7f73545ff490b5f49?method=download&shareKey=ecb0b2fae6f26aa5092f69916a3ce29c" width="414px"></li><li>monologue：delivery、内容、语用，0-4分</li><li>对话：整个对话的总分，考虑熟练程度和任务完成情况</li></ul></li><li><p>声学模型</p><ul><li>识别模型：基于iVector的BLSTM，960h non-native数据。LM用提示文本自适应</li><li>强制对齐：960h LibriSpeech</li><li>提取语用、内容特征时过滤filler words、重复的partial words</li></ul></li><li><p>超参：BLSTM 128维。dropout&#x3D;0.5。100 epochs、batch size 64。MemN2N：记忆前10轮提示与回答，memory size 20</p></li><li><p>基线</p><ul><li>评分特征：SpeechRater，超过100个</li><li>回归模型：Logistic回归、AdaBoost、决策树、Gradient Boost、SVM、随机森林等。其中，随机森林效果最好。</li></ul></li><li><p>实验结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEBda84d306717e91a49a5487ae38050729?method=download&shareKey=687b24e9378b54aaa8e828c1589709e4" width="417px"><p>*预测的维度分与总分计算相关度，无人工标注</p></li><li><p>实验结论</p><ul><li>内容：结合提示信息，评分效果较好。采用MemN2N可进一步提高多轮对话内容评分与人工分的相关度</li><li>发音采用音节级特征较好（音素级或音节级LM分：采用所属单词的LM分）</li><li>输入所有特征计算总分，效果更好，神经网络可以学习各维度特征间的关系</li></ul></li></ul><h3 id="其它-7">1.8.3. 其它</h3><ul><li>Qian Y, Lange P, Evanini K, et al. Neural approaches to automated speech scoring of monologue and dialogue responses[C]&#x2F;&#x2F;ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2019: 8112-8116.</li><li>展望：可解释性、诊断</li></ul><h2 id="2018-ETS-prompt-aware">1.9. 2018 ETS prompt-aware</h2><ul><li><p>方案同上</p></li><li><p>基线：SVR评分模型。采用C-rater系统提取特征：</p><ul><li>2-5阶 character n-gram</li><li>词级 1-2阶 n-gram</li><li>回答的字符数</li><li>句法依赖：采用Zpar dependency parser提取</li><li>Prompt bias</li></ul></li><li><p>LM：口语测试转写文本（超过5百万词）训练的LM、提示文本训练的LM 线性插值</p></li><li><p>实验结果</p><table><thead><tr><th>对比实验</th><th>实验结果</th><th>实验结论</th></tr></thead><tbody><tr><td>prompt-aware<br>Siamese LSTM: 用于评分前先进行离题检测，分类准确度97.3%。Manhattan distance</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBa24523c00cf49bcef30e9181fffc1655?method=download&shareKey=ad09059e7e16a6f1de9fd0f54b8d4e64" alt="模型结构对比"></td><td>prompt-encoder可以学到离题信息</td></tr><tr><td>模型结构</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEB724c8edd670b9460906fb9935b2c7a9d?method=download&shareKey=69cae0a1c815483183f70678579416b5" alt="模型结构对比"></td><td>fine-tune word embedding、attention机制、prompt encoder都有收益</td></tr><tr><td>对比传统模型</td><td><img src="https://note.youdao.com/yws/api/personal/file/WEBf1396007c43b6dd584ac28e18b137e3e?method=download&shareKey=baf3230953f5f4a36a8581b2f325123a" alt="对比基线"></td><td>prompt bias特征收益较小；<br>相较于采用人工转写，采用ASR识别结果时集外题相对集内题的效果下降更显著；<br>prompt encoder（最后两列）有收益，特别是在集外题上</td></tr></tbody></table></li></ul><h3 id="其它-8">1.9.1. 其它</h3><ul><li>Qian Y, Ubale R, Mulholland M, et al. A prompt-aware neural network approach to content-based scoring of non-native spontaneous speech[C]&#x2F;&#x2F;2018 IEEE spoken language technology workshop (SLT). IEEE, 2018: 979-986.</li><li>内容分<ul><li>LSA (Latent Semantic Analysis)： 对各任务分别训练LSA模型，计算识别的词序列与训练集中高分数据的cosine相似度，SVD（奇异值分解）降维</li><li>CVA (Content Vector Analysis)：按人工分将训练集分组，计算cosine相似度。</li><li>考虑识别单词的置信度分，使模型对识别错误更鲁棒</li><li>multi-task训练BLSTM：打分+word embedding，指定分数的词向量更有区分性<blockquote><p>Alikaniotis D, Yannakoudakis H, Rei M. Automatic text scoring using neural networks[J]. arXiv preprint arXiv:1606.04289, 2016.</p></blockquote></li></ul></li></ul><h2 id="2018-ETS-acoustics-transcription">1.10. 2018 ETS acoustics + transcription</h2><ul><li>作者：ETS</li><li>发表信息：ICASSP 2018</li><li><font color="red">创新点：提示无关的神经网络评分模型（BD-LSTM + attention），输入识别文本word embedding、各单词的后验概率及声学特征，输出评分</font></li></ul><h3 id="系统结构-9">1.10.1. 系统结构</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd31cc498c578d02c7497165e2e58a513?method=download&shareKey=3bd314af1bfa5a11c60bfded4a5d2cd2" alt="系统结构"></p><ul><li>声学模型：DNN-HMM，训练集：819h non-native自发语音</li><li>评分模型输入<ul><li>lexical模型：识别文本word embedding序列。采用预训练的Glove模型，OOV采用全0向量，300维，训练评分模型时fine-tune</li><li>声学模型：每个词的声学模型后验概率、时长、pitch均值、intensity均值</li></ul></li><li>评分模型<ul><li>1D CNN<ul><li>参考[14]，采用3种尺寸的卷积核$\left(conv_{size}-1, conv_{size}, conv_{size}+1\right)$，用于覆盖不同的感受野。各$conv_{n}$个卷积核</li><li>input -&gt; dropout $dp_{CNN}1$ -&gt; 卷积层 -&gt; max pooling（沿时间轴） -&gt; dropout $dp_{CNN}2$</li></ul></li><li>BD-LSTM<ul><li>input -&gt; dropout $dp_{RNN}1$ -&gt; BD-LSTM -&gt; 两个方向的隐层状态拼接 -&gt; dropout $dp_{RNN}2$</li></ul></li><li>BD-LSTM + attention<ul><li>input -&gt; dropout -&gt; BD-LSTM -&gt; attention -&gt; dropout</li></ul></li><li>超参tuning：采用Hyperopt Python包实现的Tree Parzen Estimation (TPE)方法[23]。$conv_{size}&#x3D;4, conv_{n}&#x3D;100, dp_{CNN}1&#x3D;dp_{RNN}1&#x3D;0.25, dp_{CNN}2&#x3D;dp_{RNN}2&#x3D;0.5, LSTM_{dim}^{lex}&#x3D;128, LSTM_{dim}^{ac}&#x3D;32$</li></ul></li></ul><h3 id="评价-6">1.10.2. 评价</h3><ul><li><p>数据集：训练集 2930，开发集 731，测试集 1827。4分制。</p></li><li><p>传统模型（基线）</p><ul><li><p>随机森林、GBT（Gradient Boosting Tree）、SVR（Support Vector Regression）。其中，GBT模型人-机评分相关度最高。</p></li><li><p>评分特征</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2b3d68c3d17278fcc388c60f44fbca7b?method=download&shareKey=93dba1172f8282c23aea663f0988cfed" alt="评分特征"></p><table><thead><tr><th>类别</th><th>特征示例</th></tr></thead><tbody><tr><td>流利度</td><td>单词数&#x2F;秒、单词数&#x2F;段、静音段个数、静音段平均时长、长停顿（&gt;0.5s）频率、有声停顿（uh、um）个数</td></tr><tr><td>韵律、语调、重音</td><td>韵律事件（prominences and boundary tones）的占比、之间的平均距离、距离的平均差，元音、辅音、音节时长的占比、标准差、Pairwise Variability Index</td></tr><tr><td>发音</td><td>native AM 强制对齐计算likelihood、ASR词级置信度均值、在native语料上统计各元音的时长均值，计算测试数据元音时长与参考值的差值的均值</td></tr><tr><td>语法</td><td></td></tr><tr><td>用词</td><td>多样性、复杂度</td></tr></tbody></table></li></ul></li><li><p>实验结果</p><img src="https://note.youdao.com/yws/api/personal/file/WEB80c4a6aa4b8e87255eb450053d7e42d9?method=download&shareKey=19bfa2868abd6d958f9a91870d239584" width="50%"><p>*相较于传统评分模型采用n-gram提取评分特征，word embedding可提供更丰富的信息</p></li><li><p>展望：可解释性、更多声学特征、其它attention机制</p></li></ul><h3 id="其它-9">1.10.3. 其它</h3><ul><li>Chen L, Tao J, Ghaffarzadegan S, et al. End-to-end neural network based automated speech scoring[C]&#x2F;&#x2F;2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018: 6234-6238.</li><li>Yu Z, Ramanarayanan V, Suendermann-Oeft D, et al. Using bidirectional LSTM recurrent neural networks to learn high-level abstractions of sequential features for automated scoring of non-native spontaneous speech[C]&#x2F;&#x2F;2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015: 338-345.</li><li>Taghipour K, Ng H T. A neural approach to automated essay scoring[C]&#x2F;&#x2F;Proceedings of the 2016 conference on empirical methods in natural language processing. 2016: 1882-1891. CNN（提取局部上下文信息）+ RNN（提取长时信息）+ mean over time回归（利用全文信息）</li></ul><h2 id="2022-Chinese-English-Interpretation口语翻译">1.11. 2022 Chinese-English Interpretation口语翻译</h2><ul><li>作者：广东外语外贸大学</li></ul><h3 id="系统结构-10">1.11.1. 系统结构</h3><ul><li><p>流畅度分：语速</p></li><li><p><font color="red">关键字、内容、语法：采用Bert预训练模型、BiLSTM、attention机制</font></p><img src="https://note.youdao.com/yws/api/personal/file/WEB2a27b65f57024957f23441b7cf64123f?method=download&shareKey=d00680ca591a47f6ec40226486adbc31" width="50%"></li><li><p>采用随机森林回归器融合4个维度分计算总分</p></li></ul><h3 id="其它-10">1.11.2. 其它</h3><ul><li>Li X, Li X, Chen S, et al. Neural-based automatic scoring model for Chinese-English interpretation with a multi-indicator assessment[J]. Connection Science, 2022, 34(1): 1638-1653.</li></ul><h2 id="2022-Word-Scoring">1.12. 2022 Word Scoring</h2><ul><li>作者：字节跳动</li><li><font color="red">创新点<ul><li>数据增强：给定词典中的音素序列，从训练数据相应的音素级特征中随机抽样来伪造单词样本，单词分取音素GOP均值</li><li>采用MFCC、ASR AM deep feature进行评分</li></ul></font></li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe00917195f9a8da3492d09eff12bc9f6?method=download&shareKey=55db844dfc0db67bba44ab887f6bea6b" alt="数据增强"></p><h3 id="其它-11">1.12.1. 其它</h3><ul><li>Fu K, Gao S, Wang K, et al. Improving Non-native Word-level Pronunciation Scoring with Phone-level Mixup Data Augmentation and Multi-source Information[J]. arXiv preprint arXiv:2203.01826, 2022.</li></ul><h2 id="【弃】2020-Automated-chinese-language-proficiency-scoring-by-utilizing-siamese-convolutional-neural-network-and-fusion-based-approach">1.13. 【弃】2020 Automated chinese language proficiency scoring by utilizing siamese convolutional neural network and fusion based approach</h2><ul><li>论文质量较差，弃</li><li>自制数据集</li></ul><h3 id="系统结构-11">1.13.1. 系统结构</h3><ul><li>native speakers’ key points、测试者语音，提取100*300维向量 -&gt; 分别送入权重共享的卷积层 -&gt; pooling层 -&gt; 计算cosine相似度 -&gt; 线性层输出分数</li><li>人工设计的特征：详见SpeechRater v5.0。提取tf-idf特征，计算测试语音、同一单词人工分4分的训练语音的cosine 相似度</li></ul><h3 id="其它-12">1.13.2. 其它</h3><ul><li>Kwong A, Muzamal J H, Zhang P Y, et al. Automated chinese language proficiency scoring by utilizing siamese convolutional neural network and fusion based approach[C]&#x2F;&#x2F;2020 International Conference on Engineering and Emerging Technologies (ICEET). IEEE, 2020: 1-6.</li><li>语音评测系统质量控制[37, 39-42]</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
          <category> 评分 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeNet论文</title>
      <link href="/blog/yu-yin/gong-ju-bao/wenet/lun-wen/"/>
      <url>/blog/yu-yin/gong-ju-bao/wenet/lun-wen/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> WeNet </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>cleanup_segmentation</title>
      <link href="/blog/yu-yin/gong-ju-bao/kaldi/cleanup-segmentation.mm/"/>
      <url>/blog/yu-yin/gong-ju-bao/kaldi/cleanup-segmentation.mm/</url>
      
        <content type="html"><![CDATA[<div class="markmap-container" style="height:800px">  <svg data="{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:1,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;run_cleanup_segmentation.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;clean_and_segment_data.sh &lt;br/&gt; 采用转写文本构建语言模型，选取和转写文本编辑距离最小的解码路径；&lt;br/&gt;若识别为连续的重复，non-scored words间的错误，与sil、fix、OOV相邻的删除错误，修正转写文本；&lt;br/&gt; 挑选识别正确的片段，并加一系列限定条件 &lt;br/&gt; &lt;font style=background:green&gt; 输入&amp;lt;srcdir&amp;gt;：SAT GMM模型目录，或fMLLR对齐结果&lt;/font&gt; &lt;br/&gt; clean_and_segment_data_nnet3.sh 区别：功能一致，用NNET3模型解码&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;make_biased_lm_graphs.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;取转写文本中top_n_words（默认值100）个高频词，unigram概率=频次/高频词总频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;make_biased_lms.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;根据min-words-per-graph（默认值100）将音频文本分组&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;make_one_biased_lm.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;统计第n阶的count&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;CompletelyDiscountLowCountStates&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;GetHistToTotalCount：统计各历史/前缀（长度&amp;gt;=2）出现的频次&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:11,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;对第n至3阶，若历史/前缀出现的频次&amp;lt;min_count（默认值10），删除该项，并回退&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;ApplyBackoff：对第2至n阶，各项频次折扣discounting-constant（默认值0.3），累加给backoff_symbol，相应的低1阶的频次+1&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;AddTopWords：添加高频词unigram，频次为unigram总频次*概率&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:9,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;PrintAsFst：ngram概率=折扣后的ngram概率+折扣的概率*低1阶的概率，打印FST&quot;}]}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;compile-train-graphs-fsts：生成group HCLG&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;decode_segmentation.sh：gmm-latgen-faster&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[20,21]},&quot;v&quot;:&quot;analyze_lats.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[21,22]},&quot;v&quot;:&quot;lattice-depth-per-frame：lattice中经过各帧的弧数&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[22,23]},&quot;v&quot;:&quot;lattice-best-path：获取 1best 识别和对齐&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[23,24]},&quot;v&quot;:&quot;ali-to-phones --write-lengths=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[24,25]},&quot;v&quot;:&quot;analyze_phone_length_stats.py 统计各音素时长分布&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[25,26]},&quot;v&quot;:&quot;ali-to-phones --per-frame=true&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[26,27]},&quot;v&quot;:&quot;analyze_lattice_depth_stats.py 统计各音素lattice depth分布&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[28,29]},&quot;v&quot;:&quot;lattice_oracle_align.sh&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[29,30]},&quot;v&quot;:&quot;lattice-oracle：获取和转写文本编辑距离最小的解码路径&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[30,31]},&quot;v&quot;:&quot;get_ctm&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[31,32]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-align-words-lexicon 对齐词边界&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[32,33]},&quot;v&quot;:&quot;&lt;font style=background:green&gt;lattice-1best 获取最优路径（消歧符可能导致上述lattice有多条路径）&lt;/font&gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[33,34]},&quot;v&quot;:&quot;nbest-to-ctm 打印utt_id, channel, start, dur, word_id&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[34,35]},&quot;v&quot;:&quot;align-text&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[35,36]},&quot;v&quot;:&quot;wer_per_utt_details.pl：打印utt-id、编辑距离、转写词数、解码结果、转写&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[36,37]},&quot;v&quot;:&quot;wer_per_spk_details.pl&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[37,38]},&quot;v&quot;:&quot;wer_ops_details.pl：打印各个词识别为正确、插入、删除、替换的频次&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[38,39]},&quot;v&quot;:&quot;get_ctm_edits.py：打印utt_id, channel, start, dur, 识别的单词, 置信度(始终为1），转写的单词, 编辑类型 &lt;br/&gt;（其中，sil表示无对应的转写单词且识别为&amp;lt;eps&amp;gt; sil；若转写单词不在词典中且识别为&amp;lt;unk&amp;gt;则编辑类型为cor）&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[40,41]},&quot;v&quot;:&quot;modify_ctm_edits.py：若识别为non-scored words（!sil, &amp;lt;eps&amp;gt;, &amp;lt;spoken_noise&amp;gt;, &amp;lt;unk&amp;gt;）间的替换、插入、删除，&lt;br/&gt;或者识别为连续的重复（如转写文本为a，识别为a a；或转写文本为a b，识别为a b a b），&lt;br/&gt;将转写文本中的词替换为识别文本中的，前者编辑类型改为fix，后者编辑类型改为cor&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[42,43]},&quot;v&quot;:&quot;taint_ctm_edits.py：识别错误前后连续相邻的sil、fix、OOV识别为&amp;lt;unk&amp;gt; 标记为tainted；若该识别错误为删除且前/后有tainted，删除该行&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[44,45]},&quot;v&quot;:&quot;segment_ctm_edits.py&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[45,46]},&quot;v&quot;:&quot;默认参数&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[46,47]},&quot;v&quot;:&quot;片段最短时长0.5s、新片段最短时长1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[47,48]},&quot;v&quot;:&quot;tainted词最大时长0.05s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[48,49]},&quot;v&quot;:&quot;片段首尾静音最大时长0.5s（若导致不满足片段最短时长或新片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[49,50]},&quot;v&quot;:&quot;片段首尾non-scored word最大时长0.5s（若导致不满足片段最短时长，放宽该条件）&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[50,51]},&quot;v&quot;:&quot;片段内单个静音段最大时长2s、片段内单个non-scored word&lt;font style=background:green&gt;（除OOV，实现未考虑）&lt;/font&gt;最大时长2s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[51,52]},&quot;v&quot;:&quot;如果片段首尾靠近识别错误，填充0.05s &amp;lt;unk&amp;gt;&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[52,53]},&quot;v&quot;:&quot;tainted词+填充的unk占片段时长的最大比例0.1&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[53,54]},&quot;v&quot;:&quot;根据上一条规则分段时，分割点静音段或non-scored word的最短时长0.1s&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[54,55]},&quot;v&quot;:&quot;合并重叠或相邻的片段时，若片段间删除的转写词数&amp;lt;=1，保留&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[55,56]},&quot;v&quot;:&quot;GetSegmentsForUtterance&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[56,57]},&quot;v&quot;:&quot;ComputeSegmentCores：挑选仅含cor、fix、sil的片段，至少有一个词识别为cor且不为OOV，不包含tainted&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[57,58]},&quot;v&quot;:&quot;PossiblyAddTaintedLines：若边界识别为cor并且单词非non-scored word，前后相邻的1个词为tainted，扩充该词&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[58,59]},&quot;v&quot;:&quot;PossiblySplitSegment：根据片段内静音最大时长、片段内non-scored word最大时长分段，将识别为sil或转写文本为non-scored word的词均分&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[59,60]},&quot;v&quot;:&quot;PossiblyTruncateBoundaries：根据片段首尾静音最大时长、片段首尾non-scored word最大时长 截断片段首尾sil或non-scored word&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[60,61]},&quot;v&quot;:&quot;RelaxBoundaryTruncation：片段首尾非tainted截断：若满足片段时长&amp;gt;=片段最短时长、新片段最短时长，不撤销；若全撤销后仍不满足，全撤销；否则放宽截断比例至正好满足条件。&lt;br/&gt;令b=1-a，则length_with_truncation + (length_with_relaxed_boundaries - length_with_truncation) * b = length_cutoff &lt;br/&gt; start_keep_proportion = orig_start_keep_proportion + (1-orig_start_keep_proportion) * b&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[61,62]},&quot;v&quot;:&quot;PossiblyAddUnkPadding：若片段首尾识别为cor并且非non-scored word，填充unk_padding：不超过音频起止时刻；若填充时长&amp;lt; 0.5*unk_padding，不填充&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[62,63]},&quot;v&quot;:&quot;删除不满足新片段最短时长、片段最短时长的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[63,64]},&quot;v&quot;:&quot;PossiblyTruncateStartForJunkProportion：若片段起始 (unk_padding+tainted词时长)/(时长&amp;gt;min_split_point_duration，第一个识别为sil或识别为cor的non_scored_word前的时长) &amp;gt;= max_junk_proportion，删除该词前的片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[64,65]},&quot;v&quot;:&quot;PossiblyTruncateEndForJunkProportion：同上，处理片段末尾&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[65,66]},&quot;v&quot;:&quot;ContainsAtLeastOneScoredNonOovWord：片段包含至少一个识别为cor、非OOV的scored_word，否则删除&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[66,67]},&quot;v&quot;:&quot;若片段首尾(unk_padding+tainted词)时长占比&amp;gt;max_junk_proportion，删除该片段&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:7,&quot;p&quot;:{&quot;lines&quot;:[67,68]},&quot;v&quot;:&quot;合并重叠或相连的片段：若重叠片段包含的识别为del的词数&amp;gt;合并时保留的最大删除词数，则text不包含这些词&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[68,69]},&quot;v&quot;:&quot;AccWordStatsForUtterance：统计转写文本中各个词的词频、不被包含在分段中的比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[69,70]},&quot;v&quot;:&quot;WriteSegmentsForUtterance：写分段转写文本text和segment文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[70,71]},&quot;v&quot;:&quot;PrintDebugInfoForUtterance：写ctm文件&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[71,72]},&quot;v&quot;:&quot;PrintSegmentStats：打印音频总数，被完全丢弃的音频数，总时长，每一步处理后segment数目、相对于原始数据的时长比例&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:5,&quot;p&quot;:{&quot;lines&quot;:[72,73]},&quot;v&quot;:&quot;PrintWordStats&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[74,75]},&quot;v&quot;:&quot;创建数据文件夹：统计padding=训练集（音频时长-特征时长）最高频的值，segment结束时刻+=padding &lt;br/&gt; 主要处理feats.scp、vad.scp，cmvn.scp需要重新生成&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:3,&quot;p&quot;:{&quot;lines&quot;:[76,77]},&quot;v&quot;:&quot;重新计算CMVN&quot;}]},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[78,79]},&quot;v&quot;:&quot;对齐清洗后数据&quot;},{&quot;t&quot;:&quot;heading&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[80,81]},&quot;v&quot;:&quot;重训GMM模型&quot;}]}"/></div><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 工具包 </category>
          
          <category> Kaldi </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>评分标准</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ping-fen-biao-zhun/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ping-fen-biao-zhun/</url>
      
        <content type="html"><![CDATA[<h1 id="评分标准">1. 评分标准</h1><h2 id="考纲">1.1. 考纲</h2><ul><li><a href="http://www.neea.edu.cn/res/Home/1901/d15ec0514666ac280810099f9595b557.pdf">普通高等学校招生全国统一考试大纲-2019</a><ul><li>语音项目<ul><li>基本读音：辅音连缀、成音节、单词重音</li><li>连读、失去爆破、弱读、同化</li><li>意群与停顿、语调、句子重音、节奏</li></ul></li></ul></li><li>英语口语等级考试三级考试大纲-2018（湖北省教育考试院）<a href="http://www.hbea.edu.cn/html/2018-11/12162.html">^湖北SETS</a><ul><li>短文朗读：语音语调（权重0.6）、流利程度（权重0.4）</li><li>情景提问：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>情景应答：语音语调（权重0.2）、语法词汇（权重0.45）、流利程度与交际能力（权重0.35）</li><li>连续表达：语音语调（权重0.2）、语法词汇（权重0.3）、流利程度（0.2）、交际能力（权重0.3）</li><li>分档：4档，百分制 &gt;&#x3D;85、&gt;&#x3D;75、&gt;&#x3D;60、&lt;60</li></ul></li><li>广东高考-2011：模仿朗读、情景问答、故事复述<a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></li><li>广西高考-2021 <a href="%5B%E5%B9%BF%E8%A5%BF%E9%AB%98%E8%80%83-2021%EF%BC%88%E6%8B%9B%E7%94%9F%E8%80%83%E8%AF%95%E9%99%A2%EF%BC%89%5D(https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf)">^广西高考</a><ul><li>模仿朗读：语音语调（权重0.5）、流利度（权重0.3）、完整度（权重0.2）</li><li>口头表达：内容（权重0.5）、语法（0.27）、语音语调+流利度（0.23）</li></ul></li><li>2019年江苏省初中英语听力口语自动化考试纲要：朗读短文（4档）、情景问答（2档）、话题简述（4档）<a href="https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf">^江苏中考</a></li><li>宁波市2022年初中学业水平考试英语听力口语自动化考试说明：朗读短文（4档）、情景问答（3档）、话题简述（4档）<a href="http://nbeea.nbedu.net.cn/ckfile/files/%E5%AE%81%E6%B3%A2%E5%B8%822022%E5%B9%B4%E5%88%9D%E4%B8%AD%E5%AD%A6%E4%B8%9A%E6%B0%B4%E5%B9%B3%E8%80%83%E8%AF%95%E8%8B%B1%E8%AF%AD%E5%90%AC%E5%8A%9B%E5%8F%A3%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%8C%96%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E.pdf">^宁波中考</a></li><li><a href="http://jyj.panjin.gov.cn/2019_09/24_00/content-65069.html">盘锦中考-2019</a>：朗读（4档）、情景问答（3档）</li><li><a href="http://edu.wenzhou.gov.cn/art/2022/3/3/art_1341152_59021518.html">温州中考-2022</a>：篇章朗读、情景问答、说话 （无评分标准介绍）</li></ul><h2 id="维度分考察点">1.2. 维度分考察点</h2><table><thead><tr><th>维度分</th><th>考察点</th><th>备注</th><th>分档描述</th></tr></thead><tbody><tr><td>发音准确度</td><td>语音、声调&#x2F;单词重音</td><td>与完整度无关，自由表述题中与正确答案无关[^先声]</td><td>5档：<br>语音、语调清晰、准确；<br>有错误，但不影响理解；<br>有错误，且有时影响理解；<br>有多处错误，且影响理解；<br>表现出较严重发音困难，且严重影响理解[^广西高考]</td></tr><tr><td>流畅度</td><td>语速、停顿次数、重复[^讯飞]</td><td>与朗读的内容无关[^先声] <font style="background: green">（回读）</font></td><td>5档：<br>朗读自然流利，语速适中，有节奏感[^宁波中考]无语流中断，停顿和反复现象很少[^湖北SETS]；<br>基本流畅；<br>部分话语不够流畅；<br> 话语大部分不流畅; <br> 不流畅[^广西高考]</td></tr><tr><td>标准度&#x2F;韵律</td><td>无中式口音，能灵活地运用连读、重读、失去爆破等发音技巧，节奏良好，感情充沛[^讯飞]<br>意群停顿、升降调、句子重音[^先声]</td><td></td><td></td></tr><tr><td>完整度</td><td>朗读题：已读内容占提示文本的比例<br>自由表述题：要点覆盖率</td><td></td><td>5档：<br>内容丰富，完整、连贯；<br>内容基本完整、偶尔不够连贯；<br>有部分陈述不够完整，有时不连贯；<br>大部分陈述不完整，或不连贯；严重缺乏完整性和连贯性[^广西高考]</td></tr><tr><td>语法</td><td>人称、单复数、时态、语态、动词的及物性；<br>词汇、短语、语法结构使用[^广西高考]</td><td></td><td>5档：<br>能用合适的词汇、短语、语法结构组织话语；<br>个别地方出现错误；<br>少量错误；<br>大部分不正确；<br>不能正确使用[^广西高考]</td></tr></tbody></table><h2 id="音标朗读">1.3. 音标朗读</h2><h2 id="单词朗读">1.4. 单词朗读</h2><h2 id="短文朗读">1.5. 短文朗读</h2><h3 id="总分与维度分">1.5.1. 总分与维度分</h3><blockquote><p>成人句子：total_score &#x3D; (0.6*accuracy_score + fluency_score*0.3 + standard_score*0.1)* integrity_score&#x2F;100</p><p>成人篇章：total_score &#x3D; (0.5*accuracy_score + fluency_score*0.3 +standard_score*0.2)* integrity_score&#x2F;100 <a href="https://www.xfyun.cn/doc/Ise/IseAPI.html">^讯飞</a></p></blockquote><h2 id="单项选择">1.6. 单项选择</h2><p>用户只能按事先设定的固定答案作答；只有读正确答案并且发音正确、完整，才有得分；用户回答多个选项，以后面的回答为准。<a href="https://open.singsound.com/doc/engine?type=engine-en-en.sent.score">^先声</a></p><h2 id="情景问答">1.7. 情景问答</h2><h3 id="题型说明">1.7.1. 题型说明</h3><p>先描述一段场景，然后从描述的场景中提出一个问题，让回答者根据听到的场景回答问题<a href="https://open.singsound.com/doc/engine?type=engine-en-en.sent.score">^先声</a></p><h3 id="示例">1.7.2. 示例</h3><p><a href="https://www.chivox.com/opendoc/#/ChineseDoc/coreEn">^先声</a></p><pre class="line-numbers language-none"><code class="language-none">&quot;para&quot;（描述情景的文本）: &quot;It&#39;s unbelievable. He looks stupid, but in fact, he is such a great and humorous actor. What&#39;s going on? You know what? Mr. Bean graduate from Oxford University. Exactly, I am also very crazy about Mr. Bean. He is really a funny guy and he does have a great sense of humor. In my eyes, he is a genius. I really admire him. I couldn&#39;t agree more, and it&#39;s his giftedness and hard works that make him succeed. After seeing his interesting films, I feel cheerful and excited, he brings happiness to us. Yes, I hope we can bring laughter to people too, just like Mr. Bean. I can&#39;t wait to see more his films after class. But first thing first, let&#39;s get our homework done.&quot;,&quot;quest_ans&quot;（提问问题的文本）: &quot;What makes Mr. Bean so successful?&quot;,&quot;lm&quot;（可能的正确回答；每个text表示一种正确的回答）: [    &#123;&quot;text&quot;: &quot;It&#39;s his giftedness and hard works that make him succeed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;is talent and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard works makes Mr. Bean so successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His talent and hard work makes him successful.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His body language is so funny, he makes people laugh, feel happy and relaxed.&quot;&#125;,    &#123;&quot;text&quot;: &quot;Hard work and giftedness.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His giftedness and hard work.&quot;&#125;,    &#123;&quot;text&quot;: &quot;his talents and he is very hard working.&quot;&#125;,    &#123;&quot;text&quot;: &quot;His gift and hard works.&quot;&#125;],&quot;key&quot;（关键点可能的表述方式；关键点对打分的影响很大）:     [[&quot;giftedness&quot;, &quot;gift&quot;, &quot;talent&quot;], &quot;hard work&quot;],&quot;unkey&quot;（错误答案，用户发音命中其中任一错误答案，对得分影响很大，得分会较低）:[&quot;no&quot;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="评分标准-1">1.7.3. 评分标准</h3><h4 id="广东高考">1.7.3.1. 广东高考</h4><p><a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></p><table><thead><tr><th></th><th>权重</th><th>分档描述</th></tr></thead><tbody><tr><td>信息</td><td>75%</td><td>1.5分：按照要求传递了信息；<br>1分：基本按照要求传递信息（漏了一、两点次要信息、或添加了无关信息）；<br>0分：不能按照要求传递信息</td></tr><tr><td>语言</td><td>25%</td><td>0.5分：不影响理解所表达的信息；<br>0分：导致不能理解所表达的信息</td></tr></tbody></table><h4 id="宁波中考">1.7.3.2. 宁波中考</h4><p><a href="http://nbeea.nbedu.net.cn/ckfile/files/%E5%AE%81%E6%B3%A2%E5%B8%822022%E5%B9%B4%E5%88%9D%E4%B8%AD%E5%AD%A6%E4%B8%9A%E6%B0%B4%E5%B9%B3%E8%80%83%E8%AF%95%E8%8B%B1%E8%AF%AD%E5%90%AC%E5%8A%9B%E5%8F%A3%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%8C%96%E8%80%83%E8%AF%95%E8%AF%B4%E6%98%8E.pdf">^宁波中考</a></p><table><thead><tr><th>分值</th><th>评分标准</th></tr></thead><tbody><tr><td>1</td><td>意思明白，表达清楚，语音、语调正确，词语、语法合乎规范。</td></tr><tr><td>0.5</td><td>意思基本明白，表达基本清楚，语音、语调基本正确，词语、语法有错误。</td></tr><tr><td>0</td><td>答非所问，或错误很多，不能达意。</td></tr></tbody></table><h4 id="其它版本">1.7.3.3. 其它版本</h4><ul><li>江苏中考（2档）<a href="https://jys.jsies.cn/htmledit/uploadfile/20190111153949047.pdf">^江苏中考</a>、深圳中考、衢州中考</li></ul><h4 id="维度分">1.7.3.4. 维度分</h4><p>驰声：内容、语法、发音、流利度</p><p>先声：完整度、发音、流利度，只建议展示总分</p><h2 id="故事复述">1.8. 故事复述</h2><p><a href="https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E9%AB%98%E8%80%83%E8%8B%B1%E8%AF%AD%E5%90%AC%E8%AF%B4%E8%80%83%E8%AF%95/2678957">^广东高考</a></p><h3 id="题型说明-1">1.8.1. 题型说明</h3><p>Retelling(故事复述)，要求考生先听一段大约两分钟的独白，录音播放两遍。考生准备一分钟之后开始复述所听的内容。要求考生尽可能使用自己的语言复述，而且复述内容应涵盖尽可能多的原文信息点。选取的独白其体裁主要以记述文和议论文为主。</p><h3 id="示例-1">1.8.2. 示例</h3><p><font style="background: yellow">（300词左右）</font></p><p><strong>A Young Man’s Present</strong></p><p>A young man who lived in London was in love with a beautiful girl. Soon she became his girlfriend. The man was very poor while the girl was rich. The young man wanted to give her a present on her birthday. He wanted to buy something beautiful for her, but he had no idea how to do it, as he had very little money. The next morning he went to a shop. There were many fine things: rings, gold watches, diamonds — but all these things were too expensive. There was one thing he could not take his eyes off. It was a beautiful vase. That was a suitable present for his girlfriend. He had been looking at the vase for half an hour when the manager of the shop noticed him. The young man looked so pale, sad and unhappy that the manager asked what had happened to him.</p><p>The young man told him everything, The manager felt sorry for him and decided to help him. He came up with a good idea. The manager pointed to the corner of the shop. To his great surprise the young man saw a vase broken into many pieces. The manager said: “I can help you. I shall order my worker to pack it and take it to your girlfriend. When he enters the room, he will drop it.”</p><p>On the birthday of his girlfriend the young man was very excited.</p><p>Everything happened as had been planned. The worker brought in the vase, and as he entered the room, he dropped it. There was horror on everybody’s face. When the vase was unpacked the guests saw that each piece was packed separately.</p><h3 id="评分标准-2">1.8.3. 评分标准</h3><table><thead><tr><th></th><th>内容</th><th>语言</th><th>流利</th><th>语音</th></tr></thead><tbody><tr><td>权重</td><td>50%</td><td>16.7%</td><td>20.8%</td><td>12.5%</td></tr><tr><td>考察点</td><td>原文信息点被覆盖的比例</td><td>语法</td><td></td><td>语音语调</td></tr></tbody></table><p>考生不按话题规定内容表述或套背内容毫不相干的范文：0分<a href="%5B%E5%B9%BF%E8%A5%BF%E9%AB%98%E8%80%83-2021%EF%BC%88%E6%8B%9B%E7%94%9F%E8%80%83%E8%AF%95%E9%99%A2%EF%BC%89%5D(https://www.gxeea.cn/gallary/upload_images/1173_26561_1606206201913.pdf)">^广西高考</a></p><h2 id="话题简述">1.9. 话题简述</h2><h2 id="信度、效度">1.10. 信度、效度</h2><ul><li>信度：多位专家打分，分数是否一致<ul><li>人工评分平均相关度、平均误差：计算评测员1与其它评测员的平均分的相关度、平均误差，作为评测员1的评分性能；以此类推；取多名评测员的平均作为人工评分性能</li></ul></li><li>效度：分数能否真实反映学生水平</li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>英文口语评测功能</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/ying-wen-kou-yu-ping-ce-gong-neng/</url>
      
        <content type="html"><![CDATA[<h1 id="英文口语评测功能">1. 英文口语评测功能</h1><h2 id="技术文档">1.1. 技术文档</h2><ul><li>科大讯飞：<a href="https://www.xfyun.cn/doc/Ise/IseAPI.html#%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E">https://www.xfyun.cn/doc/Ise/IseAPI.html#接口说明</a></li><li>驰声：<a href="https://www.chivox.com/opendoc/#/ChineseDoc/coreEn/">https://www.chivox.com/opendoc/#/ChineseDoc/coreEn/</a></li><li>先声：<a href="https://open.singsound.com/doc/engine?type=engine-en-en.word.score">https://open.singsound.com/doc/engine?type=engine-en-en.word.score</a></li></ul><h2 id="题型">1.2. 题型</h2><table><thead><tr><th>题型</th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音标朗读</td><td></td><td>√ 自定义文本音标</td><td>√</td></tr><tr><td>单词朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>单词纠音（音素识别）</td><td></td><td>√</td><td>√</td></tr><tr><td>句子朗读</td><td>√</td><td>√</td><td>√ 支持音频对比，返回音量、语调、语速相关度得分</td></tr><tr><td>句子纠音</td><td></td><td>√</td><td></td></tr><tr><td>篇章朗读</td><td>√</td><td>√</td><td>√</td></tr><tr><td>背诵</td><td></td><td>√ 篇章朗读题型背诵模式，要求严格按顺序朗读</td><td>√</td></tr><tr><td>句子选读</td><td></td><td></td><td>√ 返回实际朗读的是第几个句子</td></tr><tr><td>自然拼读</td><td></td><td></td><td>√ <font style="background: green">（文本形式不明确）</font></td></tr><tr><td>选择</td><td>√</td><td>√ 支持单选、多选</td><td>√ <br>支持扩展选择题：用户可以在事先设定的固定答案基础上扩展表述；引擎检测到读的更像哪个答案，就会有对应的得分。支持设置错误关键词。<br>支持设置解码网络：常规、精简（解码速度快，但效果下降；当参考文本较多时，可以设置）</td></tr><tr><td>问答</td><td>√</td><td>√</td><td>√ 支持设置错误关键词</td></tr><tr><td>复述、口头翻译、看图说话、口头作文</td><td>√</td><td>√</td><td>√</td></tr><tr><td>自由识别</td><td></td><td>√</td><td></td></tr></tbody></table><h2 id="功能配置">1.3. 功能配置</h2><table><thead><tr><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>区分英美式发音</td><td></td><td>支持K12词汇</td><td>√</td></tr><tr><td>自定义音标</td><td>√ 支持指定数字的读法</td><td>√</td><td>√</td></tr><tr><td>集外词</td><td></td><td>√</td><td>√</td></tr><tr><td>人群定制</td><td>传入group（成人、中学、小学）、grade（年级，junior、middle、senior）</td><td>自适应少儿、成人群体</td><td>儿童单词、句子为单独的题型；看图说话、复述支持设置，影响打分松紧度</td></tr><tr><td>松紧调节</td><td>仅中文评测支持，3档</td><td>线性调节</td><td>朗读题 5档；问答题 0.8-1.5线性调节</td></tr><tr><td>实时评测</td><td></td><td>√</td><td>√</td></tr></tbody></table><h2 id="评测结果">1.4. 评测结果</h2><table><thead><tr><th></th><th></th><th>科大讯飞</th><th>驰声</th><th>先声</th></tr></thead><tbody><tr><td>音频级</td><td>总分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>发音准确度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>流畅度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>标准度&#x2F;韵律分</td><td>√</td><td>意群（sense）、重读、升降调</td><td>√ 意群、重读、升降调占比分别为50%、25%、25%</td></tr><tr><td></td><td>完整度分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>语法分</td><td></td><td>√</td><td></td></tr><tr><td></td><td>识别文本</td><td>自由表述题</td><td>句子纠音：若错读、增读的单词在参考文本内，正常识别，否则标记为unk。<br>自由识别：识别文本带标点符号，支持逗号、句号、问号、感叹号</td><td></td></tr><tr><td></td><td>关键词&#x2F;要点命中</td><td></td><td>√</td><td>√</td></tr><tr><td>句子级</td><td>句末升降调检错</td><td></td><td>√</td><td>√</td></tr><tr><td>词级</td><td>评分</td><td>√</td><td>√</td><td>√</td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td>√ 不区分回读</td><td>正确、漏读、回读</td></tr><tr><td></td><td>浊化</td><td></td><td>√</td><td></td></tr><tr><td></td><td>连读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>失去爆破</td><td></td><td>√</td><td></td></tr><tr><td></td><td>重读</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>意群停顿</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>字母-音素对应</td><td></td><td>√</td><td>√</td></tr><tr><td>音节级</td><td>评分</td><td>√</td><td></td><td>√</td></tr><tr><td></td><td>发音检错</td><td>√</td><td></td><td></td></tr><tr><td></td><td>重音检错</td><td>√ 检测重读音节是否重读</td><td>√ 检测音节是否重读</td><td>√</td></tr><tr><td>音素级</td><td>评分</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音检错</td><td></td><td>√</td><td>√</td></tr><tr><td></td><td>发音诊断</td><td></td><td>√ 音素识别</td><td></td></tr><tr><td></td><td>正确、替换、漏读、增读、回读</td><td>√</td><td></td><td></td></tr></tbody></table><ul><li>音标、单词朗读仅发音准确度维度</li><li>标准度&#x2F;韵律分：科大讯飞：文本单词数&gt;&#x3D;5时才有</li><li>语法分：仅自由表述题有</li><li>驰声<ul><li>选择题、AITalk：返回置信度得分，可由应用层根据题目难易设置阈值（通常为75）判断结果是否正确</li></ul></li><li>先声<ul><li>句子朗读：统计各音素出现的次数、平均发音得分</li></ul></li></ul><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>小妇人</title>
      <link href="/blog/books/xiao-fu-ren/"/>
      <url>/blog/books/xiao-fu-ren/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 书籍 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>和平战士</title>
      <link href="/blog/videos/he-ping-zhan-shi/"/>
      <url>/blog/videos/he-ping-zhan-shi/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 视频 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>定风波</title>
      <link href="/blog/words/ding-feng-bo/"/>
      <url>/blog/words/ding-feng-bo/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 文字 </category>
          
          <category> 诗词 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch</title>
      <link href="/blog/ji-qi-xue-xi/kuang-jia/pytorch/"/>
      <url>/blog/ji-qi-xue-xi/kuang-jia/pytorch/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 框架 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CTC</title>
      <link href="/blog/ji-qi-xue-xi/loss/ctc/"/>
      <url>/blog/ji-qi-xue-xi/loss/ctc/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> loss </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>音标</title>
      <link href="/blog/yu-yin/yu-yin-xue/yin-biao/"/>
      <url>/blog/yu-yin/yu-yin-xue/yin-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="音标">1. 音标</h1><h2 id="符号表">1.1. 符号表</h2><h3 id="辅音">1.1.1. 辅音</h3><table><thead><tr><th>辅音</th><th>单词</th><th>音标</th></tr></thead><tbody><tr><td>p</td><td>pen</td><td>&#x2F;pen&#x2F;</td></tr><tr><td>b</td><td>bad</td><td>&#x2F;bæd&#x2F;</td></tr><tr><td>t</td><td>tea</td><td>&#x2F;tiː&#x2F;</td></tr><tr><td>d</td><td>did</td><td>&#x2F;dɪd&#x2F;</td></tr><tr><td>k</td><td>cat</td><td>&#x2F;kæt&#x2F;</td></tr><tr><td>ɡ</td><td>get</td><td>&#x2F;ɡet&#x2F;</td></tr><tr><td>tʃ</td><td>chain</td><td>&#x2F;tʃeɪn&#x2F;</td></tr><tr><td>dʒ</td><td>jam</td><td>&#x2F;dʒæm&#x2F;</td></tr><tr><td>f</td><td>fall</td><td>&#x2F;fɔːl&#x2F;</td></tr><tr><td>v</td><td>van</td><td>&#x2F;væn&#x2F;</td></tr><tr><td>θ</td><td>thin</td><td>&#x2F;θɪn&#x2F;</td></tr><tr><td>ð</td><td>this</td><td>&#x2F;ðɪs&#x2F;</td></tr><tr><td>s</td><td>see</td><td>&#x2F;siː&#x2F;</td></tr><tr><td>z</td><td>zoo</td><td>&#x2F;zuː&#x2F;</td></tr><tr><td>ʃ</td><td>shoe</td><td>&#x2F;ʃuː&#x2F;</td></tr><tr><td>ʒ</td><td>vision</td><td>&#x2F;ˈvɪʒn&#x2F;</td></tr><tr><td>h</td><td>hat</td><td>&#x2F;hæt&#x2F;</td></tr><tr><td>m</td><td>man</td><td>&#x2F;mæn&#x2F;</td></tr><tr><td>n</td><td>now</td><td>&#x2F;naʊ&#x2F;</td></tr><tr><td>ŋ</td><td>sing</td><td>&#x2F;sɪŋ&#x2F;</td></tr><tr><td>l</td><td>leg</td><td>&#x2F;leɡ&#x2F;</td></tr><tr><td>r</td><td>red</td><td>&#x2F;red&#x2F;</td></tr><tr><td>j</td><td>yes</td><td>&#x2F;jes&#x2F;</td></tr><tr><td>w</td><td>wet</td><td>&#x2F;wet&#x2F;</td></tr></tbody></table><h3 id="元音">1.1.2. 元音</h3><table><thead><tr><th>牛津</th><th>单词</th><th>音标</th><th>备注</th><th>朗文</th><th>-</th><th>-</th><th>-</th><th>cambridge</th><th>-</th><th>-</th><th>-</th><th>-</th></tr></thead><tbody><tr><td>ʌ</td><td>cup</td><td>&#x2F;kʌp&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɑː</td><td>father</td><td>&#x2F;ˈfɑːðə(r)&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɒ</td><td>got</td><td>&#x2F;ɡɒt&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔː</td><td>saw</td><td>&#x2F;sɔː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ə</td><td>about</td><td>&#x2F;əˈbaʊt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɜː</td><td>fur</td><td>&#x2F;fɜː(r)&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪ</td><td>sit</td><td>&#x2F;sɪt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>i</td><td>happy</td><td>&#x2F;ˈhæpi&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>iː</td><td>see</td><td>&#x2F;siː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊ</td><td>put</td><td>&#x2F;pʊt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>u</td><td>actual</td><td>&#x2F;ˈæktʃuəl&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>uː</td><td>too</td><td>&#x2F;tuː&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>e</td><td>bed</td><td>&#x2F;bed&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>æ</td><td>cat</td><td>&#x2F;kæt&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aɪ</td><td>my</td><td>&#x2F;maɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eɪ</td><td>say</td><td>&#x2F;seɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɔɪ</td><td>boy</td><td>&#x2F;bɔɪ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>aʊ</td><td>now</td><td>&#x2F;naʊ&#x2F;</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>əʊ</td><td>go</td><td>&#x2F;ɡəʊ&#x2F;</td><td></td><td>British English</td><td>oʊ</td><td>note</td><td>American English</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ɪə</td><td>near</td><td>&#x2F;nɪə(r)&#x2F;</td><td>British English</td><td><del>British English</del></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>eə</td><td>hair</td><td>&#x2F;heə(r)&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ʊə</td><td>pure</td><td>&#x2F;pjʊə(r)&#x2F;</td><td>British English</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>ɒː</td><td>dog</td><td>American English</td><td>ɚ</td><td>mother</td><td></td><td>American English，轻音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>iə</td><td>peculiar</td><td></td><td>ɝ</td><td>worm</td><td></td><td>American English，重读音节</td><td>ər</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>t̬</td><td>butter</td><td>&#x2F;ˈbʌt̬.ɚ&#x2F;</td><td>American English</td><td>[ɾ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>l̩</td><td>little</td><td>&#x2F;ˈlɪt.l̩&#x2F;</td><td></td><td>[ɫ]</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>ə</sup>l,<sup>ə</sup>m,<sup>ə</sup>n</td><td></td><td>&#x2F;leɪb.<sup>ə</sup>l&#x2F;</td><td></td><td>(ə)l …</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td><sup>r</sup></td><td>four apples</td><td>fɔː<sup>r</sup> + ˈæp.l ̩z &#x3D; fɔːˈræp.l ̩z</td><td>British English，在元音前时才发音</td><td>(r)</td></tr></tbody></table><h3 id="备注">1.1.3. 备注</h3><ul><li>&#x2F;i&#x2F;可以发成&#x2F;iː&#x2F;或&#x2F;ɪ&#x2F;或两者之间折中的音；&#x2F;u&#x2F;代表&#x2F;uː&#x2F;和&#x2F;ʊ&#x2F;之间的弱元音</li><li>&#x2F;ɒ&#x2F;只出现在英式英语中，美式英语的发音通常是&#x2F;ɔː&#x2F;或&#x2F;ɑː&#x2F;</li><li>(r)：只有紧跟的是下一个单词开头的元音时，英式发音才会出现&#x2F;r&#x2F;，如far away；否则省略&#x2F;r&#x2F;。对于美式英语来说，所有的&#x2F;r&#x2F;都应该发音。</li><li>&#x2F;l̩&#x2F; 、&#x2F;n̩&#x2F;、(&#x2F;m̩)&#x2F;：成音节，如final &#x2F;ˈfaɪnl&#x2F;，发音为[<sup>ə</sup>l] or [<sup>ə</sup>n] </li><li>音位变体<ul><li><p>&#x2F;t&#x2F;音素还包含闪音[ɾ]和glottal stop [ʔ]。 </p><p>[ɾ] 发音像快速的&#x2F;d&#x2F;，美式发音，在很多拼写为-t-或-tt-的单词中，在元音或&#x2F;r&#x2F;之后、非重读元音或音节&#x2F;l&#x2F;之前，如city &#x2F;ˈsɪt̮ɪ &#x2F;; parting &#x2F;ˈpɑrt̮ɪŋ &#x2F;; little &#x2F;ˈlɪt̮l &#x2F;； </p><p>英美式发音有时会用glottal stop [ʔ] (声带短暂的闭合)来表达&#x2F;t&#x2F;，比如football &#x2F;ˈfʊtbɔːl&#x2F;和button &#x2F;ˈbʌtn&#x2F;。</p></li><li><p>&#x2F;l&#x2F;在元音之前或中间时（如 like）与在其他位置时（如 full [ɫ] ）发音不同</p></li><li><p>&#x2F;r&#x2F; [ɹ]如red</p></li></ul></li><li>&#x2F;x&#x2F;：摩擦音，如苏格兰的loch、爱尔兰的lough &#x2F;lɒx&#x2F;</li><li>˜：鼻元音，可能在某些源自法语的单词中保留，如penchant &#x2F;ˈpɒ̃ʃɒ̃&#x2F;</li><li>有的发音标注了强、弱形式，给出的第一个发音通常代表最常用的；但是当一个单词被强调时，应该采用strong form；当单词位于句子末尾时，也通常使用strong form。如can &#x2F;kən&#x2F;, strong form &#x2F;kæn&#x2F;</li><li>重读音节相对loud、持续时间长、发音清晰，并且可以通过音调被注意到。重读音节通常不包含弱元音&#x2F;ə&#x2F;、&#x2F;i&#x2F;或&#x2F;u&#x2F;。 </li><li>英语音素 48 vs 44<blockquote><p>传统语音学认为：英语有48个音素。一个音素对应一个音标，所以共有48个国际音标。这也是我们国内更加熟知的一种音标体系。</p><p>现代语音学认为：英语有44个音素。因为现代语音学认为 &#x2F;tr&#x2F; &#x2F;dr&#x2F; &#x2F;ts&#x2F; &#x2F;dz&#x2F; 这四个不是独立的音素，而是辅音连缀。</p><p><a href="https://zhuanlan.zhihu.com/p/31484071">https://zhuanlan.zhihu.com/p/31484071</a></p></blockquote><blockquote><p>&#x2F;ts&#x2F;、&#x2F;dz&#x2F;、&#x2F;tr&#x2F;、&#x2F;dr&#x2F; 如果作为辅音连缀简单地连读，影响正确发音</p><p><a href="https://www.hjenglish.com/yinbiao/p776611/">https://www.hjenglish.com/yinbiao/p776611/</a></p></blockquote></li></ul><h3 id="参考">1.1.4. 参考</h3><ul><li><a href="https://www.oxfordlearnersdictionaries.com/about/english/pronunciation_english">https://www.oxfordlearnersdictionaries.com/about/english/pronunciation_english</a></li><li><a href="https://www.oxfordlearnersdictionaries.com/about/pronunciation/_american_english">https://www.oxfordlearnersdictionaries.com/about/pronunciation\_american_english</a></li><li><a href="https://www.ldoceonline.com/howtouse.html">https://www.ldoceonline.com/howtouse.html</a></li><li><a href="https://dictionary.cambridge.org/help/phonetics.html">https://dictionary.cambridge.org/help/phonetics.html</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/%E8%8B%B1%E8%AA%9E%E5%9C%8B%E9%9A%9B%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/英語國際音標</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/DJ%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/DJ音標</a></li><li><a href="https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/KK%E9%9F%B3%E6%A8%99">https://go-xyz.xyz/extdomains/zh.wikipedia.org/wiki/KK音標</a></li><li>音素、音标、国际音标（IPA）、DJ音标、KK音标、宽式标音等 <a href="https://en-yinbiao.xiao84.com/study">https://en-yinbiao.xiao84.com/study</a></li><li>音标特殊字符unicode编码：<a href="http://www.fmddlmyy.cn/text65.html">http://www.fmddlmyy.cn/text65.html</a> （注意ɡ、ː、ˈ、ˌ）</li></ul><h2 id="词典">1.2. 词典</h2><ul><li><p>主流英语词典：朗文、牛津、剑桥、柯林斯、韦式 <a href="https://www.jianshu.com/p/26d12a32f048">https://www.jianshu.com/p/26d12a32f048</a></p></li><li><p>朗文交际9000词 <a href="https://github.com/MuhammadYaseenKhan/Longman-Communication">https://github.com/MuhammadYaseenKhan/Longman-Communication</a></p></li><li><p>韦式词典</p><p><a href="https://www.merriam-webster.com/assets/mw/static/pdf/help/guide-to-pronunciation.pdf">https://www.merriam-webster.com/assets/mw/static/pdf/help/guide-to-pronunciation.pdf</a></p><p><a href="https://mdx.mdict.org/%E5%85%AD%E5%A4%A7%E7%9F%A5%E5%90%8D%E8%AF%8D%E5%85%B8/%E9%9F%A6%E6%B0%8F_Merriam-Webster/Merriam-Webster/Merriam-Webster/">https://mdx.mdict.org/六大知名词典/韦氏_Merriam-Webster/Merriam-Webster/Merriam-Webster/</a></p></li><li><p>数据堂英语发音词典：<a href="https://m.datatang.com/news/info/aboutus/451">https://m.datatang.com/news/info/aboutus/451</a></p></li></ul><h2 id="ARPAbet音素集">1.3. ARPAbet音素集</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3fda55097eb7eb054863211dc53ea73d?method=download&shareKey=fecd6d9867afa9c0ab9874d271e9ce46" alt="wiki"></p><p>元音区分是否重读，重音标记：0表示非重音，1表示主重音，2表示次重音</p><h3 id="arpabet-to-ipa">1.3.1. arpabet-to-ipa</h3><p><a href="https://github.com/wwesantos/arpabet-to-ipa/blob/master/src/App.php">https://github.com/wwesantos/arpabet-to-ipa/blob/master/src/App.php</a></p><h3 id="与48个音素的区别">1.3.2. 与48个音素的区别</h3><ul><li>无短元音&#x2F;ɒ&#x2F;，美式发音中多为&#x2F;ɑː&#x2F;或&#x2F;ɔː&#x2F;，如lot、long</li><li>无双元音&#x2F;ɪə&#x2F;、&#x2F;eə&#x2F;、&#x2F;ʊə&#x2F;，用两个单元音表示</li><li>无4个辅音连缀，用两个音标表示</li><li>&#x2F;ʌ&#x2F;、&#x2F;ə&#x2F;共用符号AH，美式英语中常为多发音，如but</li></ul><h3 id="CMU-Carnegie-Mellon-University-词典">1.3.3. CMU(Carnegie Mellon University)词典</h3><p><a href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict/">http://www.speech.cs.cmu.edu/cgi-bin/cmudict/</a></p><p>CMU词典用于北美英语，采用ARPAbet音素集，元音区分是否重读</p><p>39个音素：</p><p>元音(15)：AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW</p><p>辅音(24)：B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, P, R, S, SH, T, TH, V, W, Y, Z, ZH</p><h2 id="发音学习视频">1.4. 发音学习视频</h2><ul><li>《BBC音标教程》Alex <a href="https://www.bilibili.com/video/BV127411n7nj">https://www.bilibili.com/video/BV127411n7nj</a></li><li>《英语语音》屠蓓 <a href="https://www.bilibili.com/video/BV1EP4y1p7p3">https://www.bilibili.com/video/BV1EP4y1p7p3</a></li><li>《美语从头学：美语音标》赖世雄  <a href="https://www.bilibili.com/video/BV1qo4y1S7tY/">https://www.bilibili.com/video/BV1qo4y1S7tY/</a></li><li>英语兔 <a href="https://space.bilibili.com/483162496/channel/series">https://space.bilibili.com/483162496/channel/series</a></li></ul><h2 id="频谱">1.5. 频谱</h2><p><a href="https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html">https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/spectrogram-sounds.html</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GOP</title>
      <link href="/blog/yu-yin/yu-yin-ping-ce/gop/"/>
      <url>/blog/yu-yin/yu-yin-ping-ce/gop/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音评测 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>序列区分性训练</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/xu-lie-qu-fen-xing-xun-lian/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>解码器</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/jie-ma-qi/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/jie-ma-qi/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>语言模型</title>
      <link href="/blog/yu-yin/yu-yin-shi-bie/yu-yan-mo-xing/"/>
      <url>/blog/yu-yin/yu-yin-shi-bie/yu-yan-mo-xing/</url>
      
        <content type="html"><![CDATA[<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> 语音 </category>
          
          <category> 语音识别 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
